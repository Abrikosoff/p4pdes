
We start with the Poisson problem on a square---a clich\'e, of course---because it is a straightforward PDE problem on which to learn key parts of \PETSc.  We will build a structured grid using a \pDMDA, then assemble a \pMat in parallel based on this grid, and then solve in parallel using a \pKSP object.  Discretizing the PDE generates a linear system which is slightly more interesting than the tridiagonal system we solved in Chapter \ref{chap:ls}.

\section{Poisson problem on a square domain}

Let $\mathcal{S}$ be the open unit square $(0,1)\times(0,1)$ with boundary $\partial\mathcal{S}$.  The following is our \emph{Poisson problem} for this Chapter;  see Figure \ref{fig:unitsquare}:
\begin{align}
- \grad^2 u &= f \quad \text{ on } \mathcal{S}, \label{poissonsquare} \\
u &= 0 \quad \text{ on } \partial \mathcal{S}. \label{poissonsquarebcs}
\end{align}
The next several paragraphs are standard reminders of the context for equations \eqref{poissonsquare} and \eqref{poissonsquarebcs}.

\begin{marginfigure}
\input{tikz/unitsquare.tex}
\caption{The Poisson equation on the unit square $\mathcal{S}$, with homogeneous Dirichlet boundary conditions.}
\label{fig:unitsquare}
\end{marginfigure}

In \eqref{poissonsquare} the \emph{Laplacian} of $u(x,y)$,
\begin{equation}
  \grad^2 u = \Div(\grad u) = \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2},  \label{eq:st:lapdefn}
\end{equation}
almost always appears in mathematical models through conservation of $u$, in some sense, along with an assumption that the flux of $u$ is proportional to its gradient \citep{Ockendonetal2003}.  The divergence ``$\Div$'' is from the connection between a flux integral over a closed curve or surface and an integral over the interior of that curve or surface, namely the divergence or Gauss-Green theorem \citep[Appendix C]{Evans2010}.

The \emph{Poisson equation} \eqref{poissonsquare} is here subject to \emph{homogeneous Dirichlet} boundary conditions \eqref{poissonsquarebcs}.  Historically speaking, \eqref{poissonsquare} and \eqref{poissonsquarebcs} form the \emph{Dirichlet problem} if $f=0$ and if the boundary conditions were instead given by some $g$ along $\partial S$.  Nonetheless, we call all instances of the same basic problem ``Poisson.''  We will consider various boundary conditions later (Chapter \ref{chap:un}), always allowing non-zero right-hand sides $f$.  The problem must have appropriate boundary conditions, either Dirichlet ($u$ known) or Neumann (derivative of $u$ known), or a combination thereof, if it is to determine a unique solution.

The Poisson problem may model the electrostatic potential, the equilibrium distribution from certain random walks, or various other other physical phenomena.  For example, in the context of heat conduction in solids Fourier's law says that the heat flux is $\bq = -k \grad u$, where $k$ is the conductivity.  Conservation of heat energy says $c\rho\, \partial u/\partial t = - \Div\bq + f$ if $f$ describes a heat source within the domain.  The coefficient ``$c\rho$'' parameterizes the ability of the material to hold heat by a gain in temperature \citep{Ockendonetal2003}.  If $k$ is constant then in steady state these facts combine to give Poisson's equation $0 = k \grad^2 u + f$.  Holding the temperature fixed at zero along the boundary completes our Poisson problem.

With Dirichlet boundary conditions \eqref{poissonsquarebcs}, the solution to the Poisson problem is unique if it exists.\sidenote{See Theorem 5 in section 2.2 in \citep{Evans2010} or subsection 5.2.1 of \citep{Ockendonetal2003}.}  On the other hand, holding the heat flux, i.e.~$\grad u$, fixed along the boundary, \emph{Neumann conditions}, is just as common.  With only Neumann boundary conditions the Poisson equation $-\grad^2 u = f$ is not a well-posed problem because if $u$ is a solution then $v=u+c$ is also a solution for any constant $c$.  Indeed, without any boundary conditions, there is an infinite-dimensional space of solutions to the Laplace equation $-\grad^2 w = 0$ on $\mathcal{S}$---i.e.~\emph{harmonic functions}---so the Poisson equation is far from well-posed.

In this Chapter we will require that $f(x,y)$ be continuous and bounded on $\mathcal{S}$, so that we can compute its pointwise values.  With homogeneous Dirichlet boundary conditions and this assumption on $f$, standard theory says that $u(x,y)$ exists and is continuous on the closed square $\bar{\mathcal{S}}$ \citep[Theorem 6 in section 5.6]{Evans2010}, so there is no ambiguity in the boundary condition ``$u=0$ on $\partial \mathcal{S}$.''  Also we can evaluate pointwise errors if we have an exact solution. 

Because \eqref{poissonsquare} and \eqref{poissonsquarebcs} form a linear problem, finite-dimensional approximations of it are simply linear systems, as solved in Chapter \ref{chap:ls}. In this Chapter and the next we apply a \emph{finite difference} (FD) method, as a way to generate the linear system.  In Chapter \ref{chap:of} we apply a finite element approach instead.


\section{Building a structured grid}

Our FD method is based on a \emph{structured grid} of $m_x m_y$ points on the unit square, as in Figure \ref{fig:unitsquaregrid}, with spacing $h_x=1/(m_x-1)$ and $h_y=1/(m_y-1)$ in the two directions.  The grid coordinates are $x_i = i\, h_x$ for $i = 0,1,\dots,m_x-1$ and $y_j = j\, h_y$ for $j=0,1,\dots,m_y-1$.  The construction of such a 2D grid, and distributing it across MPI processes, uses a \PETSc object not covered in Chapters \ref{chap:gs} and \ref{chap:ls}.

\begin{marginfigure}
\input{tikz/unitsquaregrid.tex}
\caption{A grid on the unit square $\mathcal{S}$, with $m_x=5$ and $m_y=7$.}
\label{fig:unitsquaregrid}
\end{marginfigure}

Consider the lines of code in Code \ref{code:dmdacreatetwod}, an extract from \texttt{poisson.c} shown later (Codes \ref{code:poissoncreate} and \ref{code:poissonsolve}).  They create a \PETSc \pDM object, an abstract type for describing the topology (connectedness) and geometry of a grid, the way it is distributed across \MPI processes, and the way each process can access data from its neighboring processes.  This specific case creates a ``\pDMDA'', which is the structured-grid subclass of \pDMs, for grids like Figure \ref{fig:unitsquaregrid}.  Historically-speaking, ``\pDM'' might stand for ``distributed mesh'' and ``\texttt{DA}'' for ``distributed array,'' but in practice ``\pDMDA'' means ``structured grid'' from now on.

\cinputraw{misc/dmdacreate2d.frag}{extract from ch3/poisson.c}{An example of creating a 2D \pDMDA.}{}{//START}{//STOP}{code:dmdacreatetwod}

\begin{marginfigure}
\input{tikz/unitsquaregridparallel.tex}
\caption{The same grid as in Figure \ref{fig:unitsquaregrid}, distributed across four \MPI processes, with \texttt{rank} $\in \{0,1,2,3\}$ in gray, by \texttt{DMDACreate2d()}.}
\label{fig:unitsquaregridparallel}
\end{marginfigure}

If we do
\begin{cline}
$ cd c/ch3/
$ make poisson
$ ./poisson -da_grid_x 5 -da_grid_y 7
\end{cline}
%$
then the particular structured grid shown in Figure \ref{fig:unitsquaregrid} is created.  In this case all points of the grid are ``owned'' by a single MPI process.  However, if we run with multiple \MPI processes by
\begin{cline}
$ mpiexec -n K ./poisson -da_grid_x MX -da_grid_y MY
\end{cline}
%$
then \PETSc does the best it can to balance the load of \texttt{MX} \texttt{MY} grid points among \texttt{K} processes, with the restriction that each \MPI process owns a rectangular subgrid.  For example,
\begin{cline}
$ mpiexec -n 4 ./poisson -da_grid_x 5 -da_grid_y 7
\end{cline}
%$
distributes the grid as in Figure \ref{fig:unitsquaregridparallel}.  Neither $m_x=5$ nor $m_y=7$ is divisible by two, but \PETSc distributes the four ranks across the $35$ grid points relatively uniformly; the rank $0$ process owns 12 points and rank $3$ owns 6, while the other ranks are in between.  In this case the load is only balanced to within a factor of two, but on larger grids this ``\texttt{PETSC\_DECIDE}'' method can do better.

\begin{marginfigure}
\input{tikz/unitsquaregridprime.tex}
\caption{Process-owned domains are far from square if the number of \MPI processes is prime.}
\label{fig:unitsquaregridprime}
\end{marginfigure}

The observant reader has already noted that if the total number of processes \texttt{K}, in ``\texttt{mpiexec -n K},'' is prime then we get not-at-all-square process-owned domains.  For instance, the ``bad'' result from running
\begin{cline}
$ mpiexec -n 5 ./poisson -da_grid_x 5 -da_grid_y 7
\end{cline}
%$
is shown in Figure \ref{fig:unitsquaregridprime}.  Each process' portion of the grid has large perimeter-to-area ratio so that interprocess communication will be large.

\begin{marginfigure}
\input{tikz/unitsquaregrideight.tex}
\caption{A balanced distribution of a $10\times 10$ grid across four \MPI processes, with \texttt{rank} in gray, as created by \texttt{DMDACreate2d()}.}
\label{fig:unitsquaregrideight}
\end{marginfigure}

Though these defaults can be overridden by runtime options \texttt{-da\_grid\_x} and \texttt{-da\_grid\_y}, the fifth and sixth arguments ``\texttt{-9}'' of \texttt{DMDACreate2d()} in Code \ref{code:dmdacreatetwod} are used to set default dimensions $m_x=m_y=9$.  Thus
\begin{cline}
$ mpiexec -n 4 ./poisson
\end{cline}
%$
distributes the default $9\times 9$ grid in a reasonably well-balanced manner among four processes.  Figure \ref{fig:unitsquaregrideight} shows the perfectly-balanced result of
\begin{cline}
$ mpiexec -n 4 ./poisson -da_grid_x 10 -da_grid_y 10
\end{cline}
%$
with each rank owning a square subgrid of 25 nodes.

To explain the other options to \texttt{DMDACreate2d()} we quote the \PETSc manual pages description:

\begin{code}
DMDACreate2d(MPI_Comm comm, DMBoundaryType bx, DMBoundaryType by,
  DMDAStencilType stype, PetscInt M, PetscInt N, PetscInt m, PetscInt n,
  PetscInt dof, PetscInt s, const PetscInt lx[], const PetscInt ly[],
  DM *da)
\end{code}
where
\small
\begin{itemize}[align=left]
\item[\texttt{comm}]   MPI communicator \\
\item[\texttt{bx,by}]  type of ghost nodes the array have; use one of \texttt{DM\_BOUNDARY\_NONE, DM\_BOUNDARY\_GHOSTED, DM\_BOUNDARY\_PERIODIC} \\
\item[\texttt{stype}] stencil type; use either \texttt{DMDA\_STENCIL\_BOX} or \texttt{DMDA\_STENCIL\_STAR} \\
\item[\texttt{M,N}]	   global dimension in each direction of the array; use \texttt{-M} and or \texttt{-N} to indicate that it may be set to a different value from the command line with \texttt{-da\_grid\_x <M> -da\_grid\_y <N>} \\
\item[\texttt{m,n}]   corresponding number of processes in each dimension (or \texttt{PETSC\_DECIDE} to have calculated) \\
\item[\texttt{dof}]     number of degrees of freedom per node \\
\item[\texttt{s}]       stencil width \\
\item[\texttt{lx,ly}]  arrays containing the number of nodes in each cell along the x and y coordinates, or \texttt{NULL}; if non-null, these must be of length as m and n, and the corresponding m and n cannot be \texttt{PETSC\_DECIDE}; the sum of the \texttt{lx[]} entries must be M, and the sum of the \texttt{ly[]} entries must be N \\
\item[\texttt{da}]      output: the resulting distributed array object 
\end{itemize}
\normalsize

In Code \ref{code:dmdacreatetwod}, the second and third arguments are \texttt{DM\_BOUNDARY\_NONE} because our Dirichlet boundary conditions do not need communication to the next process' domain, nor periodic wrapping.  In the fourth argument we use \texttt{DMDA\_STENCIL\_STAR} because only cardinal neighbors of a grid point will be used when forming the matrix.\sidenote{Figure \ref{fig:unitsquaregridstencil} below shows the ``stencil'' of our FD method.}  The two \texttt{PETSC\_DECIDE} arguments which follow tell \PETSc to distribute the grid over processes according to the size of (number of processes in) the \MPI communicator using \PETSc internal logic as illustrated above.  The next two arguments, in the ninth and tenth positions, say that our PDE is scalar (\texttt{dof}$=1$) and that the FD method will only need one neighbor in each direction (\texttt{s}$=1$).  The next two arguments after that are \texttt{NULL} because we are \emph{not} telling \PETSc any details about how to distribute processes over the grid; it \texttt{DECIDE}s for itself.  Finally, the \pDMDA object is created via an output argument.

The call to \texttt{DMDASetUniformCoordinates()} in Code \ref{code:dmdacreatetwod} sets the domain to be $[0,1]\times[0,1]$ in the sense that the \pDM object knows the spacing and locations of the grid points.  The last two arguments are ignored in this case; they would set limits on the third dimension if \texttt{da} were created with \texttt{DMDACreate3d()}.

\medskip
\begin{figure}
\includegraphics[width=\textwidth]{figs/petscghostvalues}
\caption{\PETSc's parallel decomposition of structured (left) and unstructured (right) grids, showing owned (``local'') and accessible (``ghost'') nodes for one process.}
\label{fig:petscghostvalues}
\end{figure}

The standard \PETSc view of what \pDM s ``look like'' is in Figure \ref{fig:petscghostvalues}.  The code in Code \ref{code:dmdacreatetwod} generates something like the left figure, namely a structured-grid \pDM, except that the one shown in Figure \ref{fig:petscghostvalues} has \texttt{DMDA\_STENCIL\_BOX} stencil type, unlike ours.  On the right is an unstructured grid, of the type created in Chapter \ref{chap:un} for the finite element method.  In both cases the Figure shows the nodes owned by a given process (``local'' nodes) and those other nodes that are accessible by the local process (``ghost'' nodes).


\section{Finite difference method}

We were trying to approximate PDE problem \eqref{poissonsquare} and \eqref{poissonsquarebcs}, not just build a grid.  The following FD method leads to the next step, creating and assembling a \pMat and \pVecs, for the linear system corresponding to the PDE.

By a well-known Taylor's theorem argument \citep{MortonMayers2005}, if $F(x)$ is sufficiently smooth then
\begin{equation}
   F''(x) = \frac{F(x+h) - 2 F(x) + F(x-h)}{h^2} + O(h^2)  \label{secondderivativeFD}
\end{equation}
as $h$ goes to zero.  This formula, applied to partial derivatives, will approximate the Laplacian in equation \eqref{poissonsquare}.  In fact, if $u_{i,j}$ is the gridded approximation to the value $u(x_i,y_j)$ of the exact solution $u(x,y)$ at a grid point,\sidenote{This is an important phrase!  We compute values $u_{i,j}$ from the finite difference equations.  We generally do not know the values $u(x_i,y_j)$.} and if $f_{i,j} = f(x_i,y_j)$, then from \eqref{secondderivativeFD} we have this FD approximation to equation \eqref{poissonsquare}:
\begin{equation}
- \frac{u_{i+1,j} - 2 u_{i,j} + u_{i-1,j}}{h_x^2} - \frac{u_{i,j+1} - 2 u_{i,j} + u_{i,j-1}}{h_y^2} = f_{i,j}. \label{poissonsquareFDearly}
\end{equation}
Equation \eqref{poissonsquareFDearly} applies at all interior points, where $1 \le i \le m_x-2$ and $1 \le j \le m_y-2$.  The boundary conditions \eqref{poissonsquarebcs} become
\begin{equation}
u_{0,j} = 0, \quad u_{m_x-1,j} = 0, \quad u_{i,0} = 0, \quad u_{i,m_y-1} = 0, \label{poissonsquareFDbcs}
\end{equation}
for all $i,j$.

At grid location $(x_i,y_j)$, equation \eqref{poissonsquareFDearly} relates the unknown $u_{i,j}$ to its four cardinal neighbors $u_{i+1,j}$, $u_{i-1,j}$, $u_{i,j+1}$, and $u_{i,j-1}$.  This pattern (Figure \ref{fig:unitsquaregridstencil}) is a \emph{stencil}, in particular a ``star'' stencil.  By contrast, a ``box'' stencil would additionally involve the four diagonal neighbors.  In 2D, a star stencil relates five unknowns, while a box stencil relates nine.

\begin{marginfigure}
\input{tikz/unitsquaregridstencil.tex}
\caption{This ``star'' stencil simply illustrates FD scheme \eqref{poissonsquareFDearly}.}
\label{fig:unitsquaregridstencil}
\end{marginfigure}

We will treat all values $u_{i,j}$ as unknowns, whether on the boundary or in the interior, so we have $L=m_x m_y$ unknowns.  Equations \eqref{poissonsquareFDearly} and \eqref{poissonsquareFDbcs} form a linear system of $L$ equations,
\begin{equation}
A \bu = \bb, \label{poissonlinearsystem}
\end{equation}
where $A$ is a $L\times L$ matrix and $\bu,\bb$ are $L\times 1$ column vectors.

However, to show entries of $A$ and $\bb$ in linear system \eqref{poissonlinearsystem} we must globally-order the unknowns.  Such an ordering is implemented inside a \PETSc \pDMDA, and indeed our code (\texttt{poisson.c} below) will use only the grid-wise coordinates $(i,j)$.\sidenote{The ability to assemble \pMats and \pVecs with $(i,j)$-type indexing is one reason structured-grid codes using \pDMDA can be quite short.}  Here we expose the ordering for the purpose of displaying the system in matrix-vector form.

The ordering used in a one-process (serial) run by a 2D \pDMDA is shown in Figure \ref{fig:unitsquaregridordering}.  On an $m_x$ by $m_y$ grid one could write it as
\begin{equation}
    U_k = u_{i,j} \quad \text{ where } \quad k = j\,m_x + i \label{orderingfd}
\end{equation}
for $i=0,1,\dots,m_x-1$ and $j=0,1,\dots,m_y-1$, so $k=0,1,\dots,m_x m_y-1$.  \PETSc does such index transformations inside the \pDMDA implementation.

\begin{marginfigure}
\input{tikz/unitsquaregridordering.tex}
\caption{Ordering of unknowns \eqref{orderingfd} on a $m_x=4$ and $m_y=3$ grid.  Index $k$ from \eqref{orderingfd} is shown in \textbf{bold}.}
\label{fig:unitsquaregridordering}
\end{marginfigure}

\medskip\noindent\hrulefill
\begin{example} In the $m_x=4$ and $m_y=3$ case (Figure \ref{fig:unitsquaregridordering}) we have grid spacing $h_x=1/3$ and $h_y=1/2$.  Only the $k=5$ and $k=6$ equations are not boundary conditions \eqref{poissonsquareFDbcs}.  The linear system \eqref{poissonlinearsystem} is
\setcounter{MaxMatrixCols}{20}
\begin{equation*}
\begin{bmatrix}
1 &  &  &  &  &  &  &  &  &  &  &  \\
  & 1&  &  &  &  &  &  &  &  &  &  \\
  &  & 1&  &  &  &  &  &  &  &  &  \\
  &  &  & 1&  &  &  &  &  &  &  &  \\
  &  &  &  & 1&  &  &  &  &  &  &  \\
  & c&  &  & b& a& b&  &  & c&  &  \\
  &  & c&  &  & b& a& b&  &  & c&  \\
  &  &  &  &  &  &  & 1&  &  &  &  \\
  &  &  &  &  &  &  &  & 1&  &  &  \\
  &  &  &  &  &  &  &  &  & 1&  &  \\
  &  &  &  &  &  &  &  &  &  & 1&  \\
  &  &  &  &  &  &  &  &  &  &  & 1
\end{bmatrix}
\begin{bmatrix}
u_{0,0} \\
u_{1,0} \\
u_{2,0} \\
u_{3,0} \\
u_{0,1} \\
u_{1,1} \\
u_{2,1} \\
u_{3,1} \\
u_{0,2} \\
u_{1,2} \\
u_{2,2} \\
u_{3,2}
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
0 \\
f_{1,1} \\
f_{2,1} \\
0 \\
0 \\
0 \\
0 \\
0
\end{bmatrix}
\end{equation*}
where $a = 2/h_x^2 + 2/h_y^2 = 26$, $b = - 1/h_x^2 = -9$ and $c = - 1/h_y^2 = -4$.

The matrix $A$ is not symmetric.  Furthermore it is not well-scaled, for such a small example, because the 2-norm condition number is $\kappa(A) = \|A\|_2 \|A^{-1}\|_2 = 43.16$.

\noindent\hrulefill
\end{example}

\medskip
Before assembling system \eqref{poissonsquareFDearly} and \eqref{poissonsquareFDbcs} by writing \PETSc code, there are two nontrivial observations about it.  These observations lead to an equivalent linear system that is easier to solve both in the sense that we have more options for solving the system,\sidenote{More \pKSP and \pPC choices, and more that converge.} and in the sense that the condition number is smaller.

First, equations \eqref{poissonsquareFDearly} have very different ``scaling'' from equations \eqref{poissonsquareFDbcs}.  For example, if $m_x=m_y=1001$, so that $h_x=h_y=0.001$, then the coefficient of $u_{i,j}$ in \eqref{poissonsquareFDearly} is $4/(.001)^2 = 4 \times 10^6$, while the coefficients from \eqref{poissonsquareFDbcs} are equal to 1.  To make the equations better scaled, we multiply \eqref{poissonsquareFDearly} by the grid cell area $h_x h_y$ to get
\begin{equation}
2 (a + b) u_{i,j} - a \left(u_{i+1,j} + u_{i-1,j}\right) - b \left(u_{i,j+1} + u_{i,j-1}\right) = h_x h_y f_{i,j} \label{poissonsquareFD}
\end{equation}
where $a=h_y/h_x$ and $b=h_x/h_y$.  Using \eqref{poissonsquareFD}, all the equations in the system will have coefficients of comparable size, unless the cell aspect ratio $h_y/h_x$ is very large or small.  If $h_x=h_y$ then diagonal entries are $4$ and off-diagonal entries are $-1$.

Second, the FD equations can be re-interpreted to give a \emph{symmetric} matrix $A$.  For example, at a grid point adjacent to the left-hand boundary of the square, the $i=1$ case of \eqref{poissonsquareFD}, the boundary location value $u_{0,j}$ appears in the equation.  The matrix in the linear system will be symmetric if we systematically ``move'' such values to the right-hand side vector $\bb$, which we can do because the value $u_{0,j}$ is known.  This forces off-diagonal entries $A$ to be zero in columns corresponding to known boundary values.  By converting the earlier system to a symmetric matrix $A$ we open up a larger range of linear algebra methods for solving the system efficiently, specifically including conjugate gradients as the \pKSP type and Cholesky preconditioners (\texttt{-pc\_type cholesky} or \texttt{-pc\_type icc}) as the \pPC type.

With these two modifications we can redo the last example.

\medskip\noindent\hrulefill
\begin{example} \label{exampleredo} For the same $m_x=4$ and $m_y=3$ case shown in Figure \ref{fig:unitsquaregridordering}, equations \eqref{poissonsquareFDbcs} and \eqref{poissonsquareFD} yield the linear system
\begin{equation*}
\begin{bmatrix}
1 &  &  &  &  &  &  &  &  &  &  &  \\
  & 1&  &  &  &  &  &  &  &  &  &  \\
  &  & 1&  &  &  &  &  &  &  &  &  \\
  &  &  & 1&  &  &  &  &  &  &  &  \\
  &  &  &  & 1&  &  &  &  &  &  &  \\
  &  &  &  &  & \alpha& \beta&  &  &  &  &  \\
  &  &  &  &  & \beta& \alpha&  &  &  &  &  \\
  &  &  &  &  &  &  & 1&  &  &  &  \\
  &  &  &  &  &  &  &  & 1&  &  &  \\
  &  &  &  &  &  &  &  &  & 1&  &  \\
  &  &  &  &  &  &  &  &  &  & 1&  \\
  &  &  &  &  &  &  &  &  &  &  & 1
\end{bmatrix}
\begin{bmatrix}
u_{0,0} \\
u_{1,0} \\
u_{2,0} \\
u_{3,0} \\
u_{0,1} \\
u_{1,1} \\
u_{2,1} \\
u_{3,1} \\
u_{0,2} \\
u_{1,2} \\
u_{2,2} \\
u_{3,2}
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
0 \\
(1/6) f_{1,1} \\
(1/6) f_{2,1} \\
0 \\
0 \\
0 \\
0 \\
0
\end{bmatrix}
\end{equation*}
where $\alpha = 2 (h_y/h_x) + 2 (h_x/h_y) = 13/3$ and $\beta = - h_y/h_x = - 3/2$.  The matrix is symmetric, positive definite, and better-scaled than before, with $\kappa(A)=5.83$.

\noindent\hrulefill
\end{example}


\section{Code for matrix assembly}

Code \ref{code:poissonMat} assembles ``$A$'' in linear system \eqref{poissonlinearsystem} from equations \eqref{poissonsquareFD}.  Arguments to \texttt{formMatrix()} in Code \ref{code:poissonMat} are the \pDM, the value to put on the diagonal for Dirichlet boundary conditions,\sidenote{The value on the diagonal is different in a time-dependent problem which reuses the code.  See Chapter \ref{chap:sc}.} and the returned (i.e.~modified) \pMat \texttt{A}.

\cinputpart{poisson.c}{\CODELOC}{Fill matrix entries from formulas \eqref{poissonsquareFDbcs} and \eqref{poissonsquareFD} using \texttt{MatSetValuesStencil()}.}{I}{//STARTMATRIX}{//ENDMATRIX}{code:poissonMat}

The ``\texttt{DMDALocalInfo info}'' object needs special description.  It is a \PETSc-defined C structure using six integers to describe both the global grid size and the extent of the locally-owned subgrid, as shown in Figure \ref{fig:localpartofgrid}.  The global size is in members \texttt{info.mx,info.my}.  The local process owns a \texttt{info.xm} by \texttt{info.ym} rectangular subgrid, with a range of indices
\begin{align*}
&\text{\texttt{info.xs}} \le i \le \text{\texttt{info.xs}} +\text{\texttt{info.xm}}-1, \\
&\text{\texttt{info.ys}} \le j \le \text{\texttt{info.ys}} +\text{\texttt{info.ym}}-1
\end{align*}
(in two dimensions).  For example, in Figure \ref{fig:unitsquaregridparallel} we have \texttt{info.mx} $=5$, the rank $0$ and $2$ processes have \texttt{info.xs} $=0$ and \texttt{info.xm} $=3$, and the rank $1$ and $3$ processes have \texttt{info.xs} $=3$ and \texttt{info.xm} $=2$.  There are similar $y$ ranges.

\begin{figure}
\input{tikz/localpartofgrid}
\caption{A \texttt{DMDALocalInfo} struct describes the indices for a local process' part of a 2D grid, plus the global grid size.}
\label{fig:localpartofgrid}
\end{figure}

The local-ownership index ranges from \texttt{info} are used in the \texttt{for} loops which re-appear every time we do operations on a structured 2D grid:
\begin{code}
for (j=info.ys; j<info.ys+info.ym; j++) {
  for (i=info.xs; i<info.xs+info.xm; i++) {
    DO SOMETHING AT GRID POINT (i,j)
  }
}
\end{code}

Still considering the code in Code \ref{code:poissonMat}, the \pMat object $A$ assembled by \texttt{formMatrix()}  has ranges of rows owned by each process, the standard parallel layout \texttt{MATMPIAIJ} of \pMat objects in \PETSc (Chapter \ref{chap:gs}).  However, because we work with the locally-owned subgrid using $(i,j)$ indices, we can forget the actual layout of a \pMat and only focus on the part of the grid owned by the process, instead of worrying about the matrix itself.

In particular, local indices $(i,j)$ can be used when inserting entries into \pMat \texttt{A}, which is really a dynamical data structure for matrix assembly.  Thus in Code \ref{code:poissonMat} we see one use of \texttt{MatSetValuesStencil()} for each locally-owned grid point.  For a generic interior point this command inserts five coefficients into the matrix.  The key data structure is of type \texttt{MatStencil}, an apparently-trivial struct
\begin{code}
typedef struct {
  PetscInt k,j,i,c;
} MatStencil;
\end{code}
In our 2D case, with a single degree of freedom at each node,\sidenote{The Poisson equation \eqref{poissonsquare} is a scalar PDE so the unknown at each grid point is the scalar $u_{i,j}$.  A system of equations like Navier-Stokes would have \texttt{dof}$>1$ when we call \texttt{DMDACreateXd()}, and the ``\texttt{c}'' member of \texttt{MatStencil} would get used.} we only use the \texttt{i} and \texttt{j} members of \texttt{MatStencil}.  From \eqref{poissonsquareFD}, the actual matrix entries are $a_{i,i} = 2\left(h_y/h_x + h_x/h_y\right)$ on the diagonal, and $a_{i,j} = -h_y/h_x$ or $a_{i,j} = -h_x/h_y$ for off-diagonals.


\section{A particular problem}

At this point we need a specific Poisson problem so our example code can solve it.  To do this we \emph{choose}\sidenote{For the code \texttt{tri.c} in Chapter \ref{chap:ls} we did something similar, i.e.~choosing exact solution $\bu$ before computing $\bb=A\bu$ by matrix multiplication.} an exact solution, taking care that it satisfies homogeneous Dirichlet boundary conditions ($u=0$ along $\partial \mathcal{S}$):
\begin{equation}
u(x,y) = (x^2 - x^4) (y^4 - y^2). \label{eq:st:exactsolution}
\end{equation}
Then we merely differentiate to get $f = -\grad^2 u$.  Thus \eqref{eq:st:exactsolution} solves \eqref{poissonsquare} with right side
\begin{equation}
f(x,y) = 2 (1 - 6 x^2) y^2 (1 - y^2) + 2 (1 - 6 y^2) x^2 (1 - x^2).\label{manufacturedf}
\end{equation}
From now on we will refer to $u$ in \eqref{eq:st:exactsolution} as ``$u_{ex}$'', the exact solution.  This same problem and solution appears in Chapter 4 of \citep{Briggsetal2000}, so these formulas are not original. % page 64

Observe that the truncation error term $O(h^2)$ in equation \eqref{secondderivativeFD} has a coefficient proportional to fourth derivatives \citep{MortonMayers2005} so our FD method will not be exact on this problem.  That is, $u_{ex}$ has nonzero fourth derivatives.  This is \emph{good} for testing, and we would not want to use the simpler form $u(x,y)=(x-x^2)(y^2-y)$, for example, to test convergence rate of the code, because we want the decay of numerical error with refining grids to be generic.

Code \ref{code:poissonExactRHS} shows how \eqref{eq:st:exactsolution} is implemented as \texttt{formExact()}, and it shows how \eqref{manufacturedf} is implemented as \texttt{formRHS()}.  The computations in these Codes use only local grid coordinates $(i,j)$.  \PETSc pointer arithmetic (i.e.~tricks) allows us to index the C arrays we get from \texttt{DMDAVecGetArray()} using $(i,j)$.  When we are done with computing \pVecs we restore the C arrays by calling \texttt{DMDAVecRestoreArray()}.

\cinputpart{poisson.c}{\CODELOC}{Assemble the exact solution \pVec for formula \eqref{eq:st:exactsolution}, and the \pVec for the right-hand-side of equation \eqref{poissonsquareFD} from formula \eqref{manufacturedf}.}{II}{//STARTEXACT}{//ENDRHS}{code:poissonExactRHS}


\section{Solving the PDE}

Code \ref{code:poissoncreate} shows how \texttt{poisson.c} creates the various objects needed to solve the Poisson problem, namely one \pDM, one \pMat, and three \pVecs.  A \pDM object can compute matrix and vector sizes from the grid dimensions, so we call \texttt{DMCreateMatrix()} and \texttt{DMCreateGlobalVector()} to create \pMat and \pVec objects, respectively.  Then we call the functions shown in Codes \ref{code:poissonMat} and \ref{code:poissonExactRHS} above to assemble the matrix and vectors.

\cinputpart{poisson.c}{\CODELOC}{Set up \pDM, \pMat, and \pVec objects, and assemble the linear system.}{III}{//STARTCREATE}{//ENDCREATE}{code:poissoncreate}

As in Chapter \ref{chap:ls}, the linear system itself is solved by a \pKSP Krylov space method solver object.  In Code \ref{code:poissonsolve} we create it and tell it about \pMat \texttt{A} through a call to \texttt{KSPSetOperators()}.  Recall there are two ways \texttt{A} is used, namely as the system matrix and as the ``material'' from which the preconditioner is built; this explains why \texttt{A} appears as two arguments of \texttt{KSPSetOperators()}.  We also call \texttt{KSPSetFromOptions()} so that we can change the \pKSP type at runtime (illustrated below).

The system is solved by calling \texttt{KSPSolve()}.  Then we report on the solution by computing the numerical error by the norm $\|u-u_{ex}\|_\infty$.  Finally we wrap up by destroying objects and calling \texttt{PetscFinalize()}.

\cinputpart{poisson.c}{\CODELOC}{Solve using \pKSP, and report on solution.}{IV}{//STARTSOLVE}{//ENDSOLVE}{code:poissonsolve}


\section{Runtime controls and viewers}

In a first run we want to see the residuals decrease:
\begin{cline}
$ cd c/ch3/
$ make poisson
$ ./poisson -ksp_monitor
  0 KSP Residual norm 1.020952970432e-01 
  1 KSP Residual norm 2.656923348626e-02 
  2 KSP Residual norm 8.679141000397e-03 
  3 KSP Residual norm 1.557150861763e-03 
  4 KSP Residual norm 2.239919982542e-04 
  5 KSP Residual norm 2.519822315367e-05 
  6 KSP Residual norm 2.152764600588e-06 
  7 KSP Residual norm 2.650467236964e-07 
on 9 x 9 grid:  error |u-uexact|_inf = 0.000763959
\end{cline}
%$
Recall that a default $9\times 9$ grid was chosen in calling \texttt{DMDACreate2d()}.  We see seven iterations of the KSP method, a small final residual norm, and an apparently small numerical error.  It is reasonable to think that we have solved the problem, but further inspection is definitely in order.

\begin{marginfigure}
\bigskip
\includegraphics[width=0.8\textwidth]{figs/dmM5N7}
\caption{\PETSc can show the structured-grid \pDMDA at runtime, here for a single-process run.}
\label{fig:dmM5N7}
\end{marginfigure}

Options \texttt{-da\_grid\_x MX -da\_grid\_y MY} set the grid at runtime, and we can examine both the grid (i.e.~the \pDM object) and our assembled \pMat graphically to check these objects.  If X11 or other windowing is correctly linked in your \PETSc installation then
\begin{cline}
$ ./poisson -da_grid_x 5 -da_grid_y 7 -dm_view draw -draw_pause 5
\end{cline}
%$
gives Figure \ref{fig:dmM5N7}, same as the $m_x=5$, $m_y=7$ grid in Figure \ref{fig:unitsquaregrid}, including the global node ordering by formula \eqref{orderingfd}.  Options
\begin{cline}
$ ./poisson -da_grid_x 5 -da_grid_y 7 -a_mat_view draw -draw_pause 5
\end{cline}
%$
show a graphic similar to Figure \ref{fig:matM5N7}.  These views suggest that we have the right kind of matrix structure for the Poisson problem, namely a symmetric sparse matrix with tridiagonal blocks along the diagonal and a banded structure.

\begin{marginfigure}
\bigskip
\includegraphics[width=0.95\textwidth]{figs/matM5N7}
\caption{\PETSc can show the matrix structure too.  The actual graphic is in color, but here the two dark shades show positive and negative entries while the light shade shows allocated locations which are zero.}
\label{fig:matM5N7}
\end{marginfigure}

As a specific check on our matrix assembly, this sparse matrix view is easily checked to be identical to the matrix in the linear system on page \pageref{exampleredo}:
\begin{cline}
$ ./poisson -da_grid_x 4 -da_grid_y 3 -a_mat_view
Mat Object:(a_) 1 MPI processes
  type: seqaij
row 0: (0, 1)  (1, 0)  (4, 0) 
row 1: (0, 0)  (1, 1)  (2, 0)  (5, 0) 
row 2: (1, 0)  (2, 1)  (3, 0)  (6, 0) 
row 3: (2, 0)  (3, 1)  (7, 0) 
row 4: (0, 0)  (4, 1)  (5, 0)  (8, 0) 
row 5: (1, 0)  (4, 0)  (5, 4.33333)  (6, -1.5)  (9, 0) 
row 6: (2, 0)  (5, -1.5)  (6, 4.33333)  (7, 0)  (10, 0) 
row 7: (3, 0)  (6, 0)  (7, 1)  (11, 0) 
row 8: (4, 0)  (8, 1)  (9, 0) 
row 9: (5, 0)  (8, 0)  (9, 1)  (10, 0) 
row 10: (6, 0)  (9, 0)  (10, 1)  (11, 0) 
row 11: (7, 0)  (10, 0)  (11, 1) 
on 4 x 3 grid:  error |u-uexact|_inf = 0.0085927
\end{cline}
%$

Additionally, a ``movie'' of the \pKSP iterates $\{\bu_k\}$ comes from running
\begin{cline}
$ ./poisson -da_grid_x 100 -da_grid_y 100 -ksp_monitor_solution draw -draw_pause 0.1
\end{cline}
%$
(not shown).  Alternatively, as in Chapter \ref{chap:ls}, a line graph of the (preconditioned) residual norm is from \texttt{-ksp\_monitor\_lg\_residualnorm} (also not shown).  These various viewing options should be used on under-development PDE-solving code on a structured grid.

There are two ways to specify grid refinement.  One is to specify the grid dimensions directly as above, but the other is to have the \pDM refine the grid by factors of two.  More precisely, the number of \emph{subintervals} is increased by a factor of two.  For example, this option replaces our default grid of $8$ subintervals (i.e.~$9 \times 9$ grid \emph{points}) by $16$ subintervals (i.e.~$17\times 17$ points) in each direction:
\begin{cline}
$ ./poisson -da_refine 1
on 17 x 17 grid:  error |u-uexact|_inf = 0.000196764
\end{cline}
%$

Choices of solver and parameters can be made at runtime, once we have adequately-described the problem in terms of \PETSc objects.  We have delayed questions of convergence and efficiency until runtime, but here is what we might want to know now:\begin{itemize}
\item is our numerical method correctly implemented? (\emph{convergence})
\item what is going on inside \PETSc? (\emph{exposure})
\item how to get high performance? (\emph{efficiency})
\end{itemize}
In the next three sections we address these in turn, with only a superficial stab at efficiency.  Attaining efficiency requires a better understanding of this goal, and better choices for preconditioners, than we have now.


\section{Convergence}

We want to see the numerical errors decrease as we refine the grid.  Such decrease is expected because the finite differences become better approximations of the corresponding derivatives, but we want to \emph{see} it, so we know our actual implementation is correct.  The rate at which the error decreases should match what we expect from theory.

Recall that \texttt{poisson.c} already generates the max norm error (Code \ref{code:poissonsolve}), and we know how to refine the grid by factors of two, so here is a bash loop refinement study:
\label{poissonconvdata}
\begin{cline}
$ for K in 0 1 2 3 4 5 6; do ./poisson -da_refine $K; done
on 9 x 9 grid:  error |u-uexact|_inf = 0.000763959
on 17 x 17 grid:  error |u-uexact|_inf = 0.000196764
on 33 x 33 grid:  error |u-uexact|_inf = 4.91557e-05
on 65 x 65 grid:  error |u-uexact|_inf = 1.29719e-05
on 129 x 129 grid:  error |u-uexact|_inf = 3.76924e-06
on 257 x 257 grid:  error |u-uexact|_inf = 1.73086e-06
on 513 x 513 grid:  error |u-uexact|_inf = 1.23567e-06
\end{cline}
%$
The data is shown in Figure \ref{fig:poisson-conv} as stars.  For the four coarsest grids refinement by a factor of two gives a reduction in error by a factor of about four, as expected because our FD method is $O(h^2)$ \citep{MortonMayers2005}.  Unfortunately the error seems to have stopped falling for the three finer grids, at a numerical error level around $\|u-u_{ex}\|_\infty \approx 10^{-6}$.

\begin{figure}
\bigskip
\includegraphics[width=0.75\textwidth]{figs/poisson-conv}
\caption{To show convergence we refine the \pDM grid by factors of two so that $h\to 0$.  With a stronger linear solver tolerance (\texttt{-ksp\_rtol 1.0e-12}) the errors (dots) fall at the expected rate (dashed), unlike with the default tolerance ($10^{-5}$; stars).}
\label{fig:poisson-conv}
\end{figure}

This stagnation of convergence is not a sign of an implementation error.  It comes from the default tolerance for the \pKSP object, and we simply need to ask for more accuracy using option \texttt{-ksp\_rtol}.  On a particular grid we can see the effect of a smaller tolerance by using option \texttt{-ksp\_monitor} to watch the residuals:
\begin{cline}
$ ./poisson -ksp_monitor
  0 KSP Residual norm 1.020952970432e-01 
  1 KSP Residual norm 2.656923348626e-02 
...
  6 KSP Residual norm 2.152764600588e-06 
  7 KSP Residual norm 2.650467236964e-07 
on 9 x 9 grid:  error |u-uexact|_inf = 0.000763959
$ ./poisson -ksp_monitor -ksp_rtol 1.0e-12
  0 KSP Residual norm 1.020952970432e-01 
  1 KSP Residual norm 2.656923348626e-02 
...
 12 KSP Residual norm 3.524000607280e-13 
 13 KSP Residual norm 2.674839310683e-14 
on 9 x 9 grid:  error |u-uexact|_inf = 0.000763883
\end{cline}
On this coarse grid the numerical error is nearly the same, but on finer grids the fact that the linear system is solved more exactly also brings the numerical solution significantly closer to the exact one.  In fact, rerunning the above bash loop with the stronger $10^{-12}$ tolerance yields excellent evidence of convergence (dots in Figure \ref{fig:poisson-conv}).  The linear-fit rate of this logarithmic error data is $O(h^{1.9999})$.  Our implementation is correct.


\section{A first look at efficiency}

On the other hand, the finer grid calculations above are slow.  How about if we re-run in parallel?  Are we using the right Krylov method and preconditioner combinations?

On \WORKSTATION we get these results for default choices on a refined grid:
\begin{cline}
$ timer ./poisson -da_refine 6 
on 513 x 513 grid:  error |u-uexact|_inf = 1.23567e-06
real 17.92
$ timer mpiexec -n 4 ./poisson -da_refine 6 
on 513 x 513 grid:  error |u-uexact|_inf = 1.29328e-06
real 7.76
\end{cline}
It is nice to see a speedup, though only by a factor of $17.92/7.76 = 2.3$ on this fixed-size problem, which is disappointing given the four-times increase in floating-point processing power.\sidenote{Running the \PETSc-recommended \texttt{streams} benchmark on this machine, which measures sustainable memory bandwidth, shows only a 1.82 times speedup from 1 to 4 cores, so such disappointment is to be expected.  Multicore architectures tend to have a problem with memory bandwidth, relative to multiple single-core nodes under MPI \citep{Williamsetal2009}.}

These first-try performance results raise more questions than they answer.  In particular, are we effectively using the available \PETSc options at runtime?  As noted in Chapter \ref{chap:gs}, a first step is to find allowed options by piping the output from \texttt{-help} into a pager like \texttt{less}, or \texttt{grep}ing for the option prefix we want:
\begin{cline}
$ ./poisson -help | less
$ ./poisson -help | grep ksp_
\end{cline}
This shows the default values for parameters; for example the default for \texttt{-ksp\_rtol} is \texttt{1e-05}, which explains why convergence ``leveled out'' on fine grids in the last section on convergence.  Note that if the result from \texttt{grep} is too long, just pipe it again:
\begin{cline}
$ ./poisson -help | grep ksp_ | less
\end{cline}
%$

Recall we used \texttt{-ksp\_view} in Chapter \ref{chap:ls} to expose the linear solver and preconditioner.  The serial \pKSP and \pPC defaults are GMRES and ILU($0$) as the preconditioner, while in parallel the defaults are GMRES and block Jacobi as a preconditioner, but where each diagonal block---there are four in the above parallel run---is preconditioned with ILU($0$).  In Chapter \ref{chap:ls} we pointed out that this parallel preconditioner, like most of them, gives process-count dependent results.

The serial \PETSc defaults for \pKSP and \pPC give a certain timing and a certain number of iterations on a given grid:
\begin{cline}
$ timer ./poisson -da_refine 5 -ksp_converged_reason
Linear solve converged due to CONVERGED_RTOL iterations 506
on 257 x 257 grid:  error |u-uexact|_inf = 1.73086e-06
real 1.39
\end{cline}
%$
That is a lot of iterations, and it is not clear if this timing is fast or slow.  Without thinking too hard, experimentation should show us better options.  Table \ref{tab:poissontiming} below was generated by running
\begin{cline}
$ timer ./poisson -da_refine 5 -ksp_converged_reason -ksp_type KSP -pc_type PC
\end{cline}
%$
The iteration count is also worth noting when it can help explain the timing, though at this stage we are identifying the un-defined term ``efficiency'' as just ``execution time.''

\renewcommand{\intime}[1]{\input{timing/poisson/#1}}
\begin{table}
\texttt{
\begin{tabular}{llll}
\underline{KSP}\hspace{0.5in} & \underline{PC}\hspace{1.3in} & \underline{\textrm{time (s)}}\hspace{0.3in} & \underline{\textrm{iterations}} \\
gmres      & none        & \intime{gmres.none} \\
           & ilu         & \intime{gmres.ilu} \\
           & ilu + restart=$200$ & \intime{gmres.ilu.restart} \\
cg         & none        & \intime{cg.none} \\
           & jacobi      & \intime{cg.jacobi} \\
           & icc         & \intime{cg.icc} \\
           & icc + rtol=$10^{-14}$ & \intime{cg.icc.tight} \\
preonly    & cholesky    & \intime{preonly.cholesky}  \\
minres     & none        & \intime{minres.none} \\
\end{tabular}
}
\caption{Times and number of \pKSP iterations for serial runs of \texttt{poisson.c} on $257\times 257$ grids.  The assembled matrix is \emph{symmetric}, \emph{diagonally-dominant}, and \emph{positive definite}.  All runs were on \WORKSTATION (see page \pageref{defineworkstation}).} \label{tab:poissontiming}
\end{table}

From Table \ref{tab:poissontiming} we see that for GMRES, having a preconditioner is essential.  That is, ILU($0$) substantially reduces both iteration count and time compared to no preconditioner.

The number of iterations suggests that GMRES went through several restarts, which it does by default every 30 iterations.  For the current problem memory overflow is no issue, so we avoid restart using option \texttt{-ksp\_gmres\_restart 200}.  This reduces iteration count but not execution time, and we do not recommend such a procedure for bigger problems.

The system matrix $A$ is symmetric and positive definite, so we recall from Chapter \ref{chap:ls} that the CG (conjugate gradients) Krylov method should apply, and that Cholesky and incomplete Cholesky (IC($0$)) are available as preconditioners.  In Table \ref{tab:poissontiming} we compare these methods also.  Because \texttt{-ksp\_preonly -pc\_type cholesky} is a direct solver, fair comparison to iterative methods suggests we should solve the equations accurately, so we add the tighter tolerance \texttt{-ksp\_rtol 1.0e-10} to all iterative runs in Table \ref{tab:poissontiming}, except for the run where this was further tightened to \texttt{-ksp\_rtol 1.0e-10}.

We see that Jacobi preconditioning has little benefit over \emph{un}-preconditioned CG, because the diagonal is effectively constant,\sidenote{See Exercise \ref{chap:st}.\ref{exer:st:jacobivsnone}.} but that IC($0$) for CG is the best so far.  The direct Cholesky solver is slower, presumably because of fill-in of the band seen in Figure \ref{fig:matM5N7}.

We have so far not mentioned the minimum residual method \citep[MINRES]{Greenbaum1997} which applies to indefinite symmetric matrices, but it is implemented in \PETSc too.  We see that it seems to do no better than CG, at least in their un-preconditioned form.  \citet[][p.~88]{Elmanetal2005}, however, states that ``when solving discrete Poisson problems the convergence of MINRES is almost identical to that of CG.''

If we were to stop now we might conclude that we have found a good \pKSP and \pPC combination for the Poisson equation, namely CG + IC($0$), and proceed to harder problems.  This conclusion would be premature.


\section{The scaling flaw in CG iterations}

Unfortunately, even with the preconditioners tried above, CG has a distinct flaw.\sidenote{At least from a 21st-century point of view.  Table \ref{tab:poissontiming} might have satisfied in 1970.}  Namely, the iteration count grows with refined grids, so that \citep[p.~76]{Elmanetal2005}
\begin{quote}
for uniformly refined grids, the number of CG iterations required to meet a fixed tolerance will approximately double with each grid refinement.
\end{quote}
This is exactly what we see from the following bash loop:
\begin{cline}
$ for NN in 1 2 3 4 5; do
> ./poisson -da_refine $NN -ksp_type cg -pc_type none -ksp_converged_reason; done
Linear solve converged due to CONVERGED_RTOL iterations 36
on 17 x 17 grid:  error |u-uexact|_inf = 0.000196729
Linear solve converged due to CONVERGED_RTOL iterations 73
on 33 x 33 grid:  error |u-uexact|_inf = 4.91819e-05
Linear solve converged due to CONVERGED_RTOL iterations 148
on 65 x 65 grid:  error |u-uexact|_inf = 1.22921e-05
Linear solve converged due to CONVERGED_RTOL iterations 299
on 129 x 129 grid:  error |u-uexact|_inf = 3.07512e-06
Linear solve converged due to CONVERGED_RTOL iterations 606
on 257 x 257 grid:  error |u-uexact|_inf = 7.69971e-07
\end{cline}

Of course, this is un-preconditioned CG.  While it is not surprising that \texttt{-pc\_type jacobi} does no better (Exercise \ref{chap:st}.\ref{exer:st:jacobivsnone}), it is easy to check that our favorite method so far has the same poor scaling.  Figure \ref{fig:poisson-cg-scale} shows that though IC($0$) gives somewhat faster solves and lower iteration counts, the \emph{scaling} of those counts is the same as \texttt{-pc\_type none}: the number of iterations doubles with each factor-of-two grid refinement.

\begin{figure}
\bigskip
\includegraphics[width=0.75\textwidth]{figs/poisson-cg-scale}
\caption{As the grid is refined ($h\to 0$), un-preconditioned CG scales poorly on our Poisson problem, and IC($0$) preconditioning does not improve the scaling.}
\label{fig:poisson-cg-scale}
\end{figure}

A theoretical bound on the number of CG iterations would help to understand these results, but stating such a theorem requires preparation.  Suppose that $A$ is an $N\times N$, symmetric, and positive-definite matrix.  We can define a new norm, the \emph{$A$-norm}, on $\bv\in\RR^N$,
\begin{equation}
\|\bv\|_A = \ip{\bv}{A\bv}^{1/2} = (\bv^\top A\bv)^{1/2}.  \label{normAdefn}
\end{equation}
Recall that the eigenvalues $\lambda_j(A)$ of such a matrix $A$ are positive and identical to the singular values, so that the 2-norm condition number of $A$ is the ratio of the largest and smallest eigenvalues,
	$$\kappa_2(A) = \frac{\lambda_{\max}(A)}{\lambda_{\min}(A)} \ge 1.$$
Finally, recall that $\be_k = \bu_k - \bu$ denotes the error of $\bu_k$ as a solution to $A\bu=\bb$.  In these terms we can state the following conceptually-useful theorem giving the sense in which CG is optimal\sidenote{In  fact this sense was already stated in Table \ref{tab:krylovcompare}.} and supplying an error bound.

\renewcommand{\labelenumi}{(\roman{enumi})}
\begin{theorem}  \label{thm:cgiterations}
Suppose $\bu_j$ are the iterates from un-preconditioned CG.  Then:\begin{enumerate}
\item Let $\mathcal{P}_j^1$ be the space of all real polynomials $p(x)$ of degree at most $j$ such that $p(0)=1$.  Then
    $$\|\be_j\|_A = \min_{p \in \mathcal{P}_j^1} \|p(A) \be_0\|_A.$$
\item The relative $A$-norm of the error at the $j$th iteration is bounded by an amount which is computable from the 2-norm condition number $\kappa=\kappa_2(A)$, namely
	$$\frac{\|\be_j\|_A}{\|\be_0\|_A} \le 2 \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^j.$$
\item The relative 2-norm of the residual is likewise bounded:
	$$\frac{\|\br_j\|_2}{\|\br_0\|_2} \le 2 \sqrt{\kappa} \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^j.$$
\end{enumerate}
\end{theorem}

Part (\emph{i}) of the Theorem is proved by \citet[p.~50]{Greenbaum1997}.  Part (\emph{ii}) follows from (\emph{i}) by using Chebyshev polynomials to construct a polynomial which is small on the interval $[\lambda_{\min}(A),\lambda_{\max}(A)]$ \citep[p.~51]{Greenbaum1997}.  Part (\emph{iii}) follows from (\emph{ii}) by understanding the norm $\|\cdot\|_A$ in terms of the 2-norm and the matrix $\sqrt{A}$; see Exercise \ref{chap:st}.\ref{exer:st:Anorm}.  We state part (\emph{iii}) for the simple reason that $\|\br_k\|_2$ is a quantity computed by \PETSc while $\|\be_k\|_A$ is not.  Parts (\emph{ii}) and (\emph{iii}) are not optimal bounds, though such can be found \citep[p.~51]{Greenbaum1997}.

Consider the number of iterations $j$ needed to reduce $\|\be_j\|_A$ by a factor of $\eps<0$.  Note $(x-1)/(x+1) = 1 - 2/(x+1)$, and recall that $\ln(1-x) \approx -x$ for small $x$.  If $\sqrt{\kappa}$ is large then part (\emph{ii}) of the Theorem says that $\|\be_j\|_A$ is reduced from its initial value $\|\be_0\|_A$ by a factor of $\eps$ if
    $$j \le \frac{\ln(\eps/2)}{\ln\left(1 - \frac{2}{\sqrt{\kappa}+1}\right)} \approx -\frac{\ln(\eps/2)}{2} (\sqrt{\kappa}+1).$$
In summary,
\begin{equation}
\frac{\|\be_j\|_A}{\|\be_0\|_A} \le \eps \quad \text{if} \quad j = O\left(\sqrt{\kappa_2(A)}\right).  \label{cgkappaiter}
\end{equation}
The same conclusion applies to reducing $\|\br_j\|_2$ by a factor of $\eps$, though the analysis is not so simple; see Exercise \ref{chap:st}.\ref{exer:st:Anorm}.

Though the theorem states a key connection between condition number and iteration count, the precise condition number of a discretized-PDE matrix $A_h$, coming from a mesh with spacing $h$, is not typically available.  On the other hand we can ask \PETSc to approximate eigenvalues of $A_h$ as the Krylov iteration proceeds.\sidenote{We need the iteration to use the \emph{un}-preconditioned operator when computing eigenvalues, so use \texttt{-pc\_type none} or right-side-preconditioning \texttt{-ksp\_pc\_side RIGHT}.}  Doing this will largely explain the poor scaling of CG iteration count for our symmetric-matrix, structured-grid, Poisson problem case:
\begin{cline}
$ ./poisson -ksp_type cg -pc_type none -ksp_compute_eigenvalues
Iteratively computed eigenvalues
0.304482 + 0i
...
7.69543 + 0i
on 9 x 9 grid:  error |u-uexact|_inf = 0.00076388
\end{cline}
%$
In fact $17$ eigenvalues were printed, because there were $17$ \pKSP iterations and the method computes one additional eigenvalue per iteration.  We suppressed all but the first (smallest) and last (largest), as they suffice for approximating the condition number.  Thus, on the above grid with spacing $h=1/8$:
    $$\kappa_2(A_{h=1/8}) \approx \frac{7.69543}{0.304482} = 25.274.$$
Re-running with option \texttt{-da\_refine N} for \texttt{N}$=1,2$ we get:
\begin{align*}
    \kappa_2(A_{h=1/16}) &\approx \frac{7.92311}{0.0768589} = 103.09, \\
    \kappa_2(A_{h=1/32}) &\approx \frac{7.98073}{0.0192611} = 414.34.
\end{align*}
We now see the issue clearly.  Each time the grid is refined by a factor of two (i.e.~$h\to h/2$), the condition number $\kappa_2(A_h)$ increases by a factor of four.  Then \eqref{cgkappaiter} says we should expect an increase in iteration count by a factor of two.

In fact, as an \emph{a priori} statement, the $2$-norm condition number of the discrete Poisson matrix $A_h$ on a uniform rectangular grid with spacing $h$ is known to be
\begin{equation}
    \kappa_2(A_h) = O(h^{-2}), \label{kappaofh}
\end{equation}
exactly as seen experimentally above.  This can be shown by exact analysis using the eigenvectors of the discrete Poisson problem in the uniform rectangular case \citep{Briggsetal2000}.  A more robust and general FEM error analysis which applies to unstructured grids having a mesh-uniformity bound \citep{Elmanetal2005} also applies.  In either case, combining \eqref{cgkappaiter} and \eqref{kappaofh} shows that we should expect to need
\begin{equation}
    j = O(h^{-1}), \label{kofh}
\end{equation}
iterations to solve our discrete Poisson problem to a given relative tolerance using un-preconditioned CG.

We have also seen that IC($0$) preconditioning reduces the time and iteration count, but that it does not improve on scaling \eqref{kofh}; recall Figure \ref{fig:poisson-cg-scale}.  Detailed analysis of the effects of incomplete Cholesky precondition must be difficult, and it will certainly not be pursued here, but we do now have the correct context to understand this claim about the result \citet[][p.~82]{Elmanetal2005}:
\begin{quote}
One known result [\emph{for CG for the Poisson equation}] is that the asymptotic behavior of the condition number using IC($0$) preconditioning is unchanged: $\kappa(M^{-1} A) = O(h^{-2})$.
\end{quote}
So IC($0$) is not as promising a preconditioner as we had first hoped.


\section{Krylov is not enough \dots \emph{good} preconditioning is needed}

When solving the sparse linear systems from discretized PDEs, Krylov iterations like CG and preconditioning methods like IC($0$) are useful tools because they can improve upon naively-applied direct linear algebra methods.  However, besides the bad news above that CG iterations scale badly with refining grids, there is more ``bad news'' for our Krylov approach.  Namely, that smartly-applied direct linear algebra techniques can also beat CG+IC($0$), at least in the two-dimensional case of this Chapter.

Specifically, suppose we run \texttt{poisson} with option \texttt{-da\_refine 7}, to give a $1025 \times 1025$ grid and about $10^6$ degrees of freedom.  Suppose we compare IC($0$)-preconditioned CG iterations with two other methods:
\begin{cline}
$ ./poisson -da_refine 7 -ksp_type cg -pc_type icc
$ ./poisson -da_refine 7 -ksp_type preonly -pc_type cholesky -pc_factor_mat_ordering_type nd
$ ./poisson -da_refine 7 -ksp_type preonly -pc_type lu -pc_factor_mat_ordering_type nd
\end{cline}
%$
(To be precise, we added options \texttt{-ksp\_converged\_reason} and \texttt{-ksp\_rtol 1.0e-10} to the CG run.)  We get the first three rows of Table \ref{tab:cgdirectmg}, the first of which requires no further explanation because it is the best-so-far way of using CG.  The second and third runs, which are faster, are direct methods.
%timer ./poisson -da_refine 7 -ksp_type cg -pc_type icc -ksp_converged_reason -ksp_rtol 1.0e-10
%timer ./poisson -da_refine 7 -ksp_type preonly -pc_type cholesky -pc_factor_mat_ordering_type nd
%timer ./poisson -da_refine 7 -ksp_type preonly -pc_type lu -pc_factor_mat_ordering_type nd
%timer ./fish2 -da_refine 7 -ksp_type cg -pc_type mg -ksp_converged_reason -ksp_rtol 1.0e-10

\begin{table}
\texttt{
\begin{tabular}{lllll}
\underline{\textrm{Code}}\hspace{0.3in} & \underline{KSP}\hspace{0.4in} & \underline{PC}\hspace{0.5in} & \underline{\textrm{time (s)}}\hspace{0.2in} & \underline{\textrm{iterations}} \\
poisson & cg         & icc           & 27.10 & 1056 \\
        & preonly    & cholesky + nd & 24.96 &    1 \\
        &            & lu + nd       & 16.33 &    1 \\
fish2   & cg         & mg            &  2.60 &    5 \\
\end{tabular}
}
\caption{Time and iteration count for \texttt{-da\_refine 7} runs, a $1025 \times 1025$ grid on a single MPI process on \WORKSTATION.  The \texttt{fish2} code appears in Chapter \ref{chap:pr}.} \label{tab:cgdirectmg}
\end{table}

Option \texttt{-pc\_factor\_mat\_ordering\_type nd} asks that the direct method be applied using the \emph{nested dissection} ordering \citep{George1973}.  While we give no details, in outline the unknowns and equations are re-ordered to reduce the cost of the factorization.  For LU factorization the nested dissection ordering is actually the \PETSc default for single-process runs.\sidenote{And for this reason, the result of the \texttt{-ksp\_type preonly -pc\_type lu} was not in Table \ref{tab:poissontiming}.}  In any case, the two direct solvers in Table \ref{tab:cgdirectmg} are clearly competitive with IC($0$)-preconditioned CG.

At this point the impulsive reader might give up on Krylov methods, but that also would be premature.  We do need to find a better preconditioner if we are going to have scalable solutions of this Poisson PDE problem.  We will eventually show such methods (Chapter \ref{chap:pr}), which give both efficient and parallel-scalable (Chapter \ref{chap:sc}) solutions of large nonlinear and variable-coefficient elliptic PDE problems in 2D and 3D, something direct methods of linear algebra do not achieve.

In particular, we are headed toward geometric multigrid methods (Chapter \ref{chap:pr}).  These methods can act as preconditioners on Krylov iterations, a combination which provides optimal scaling for the structured, rectangular-grid Poisson problem of the current Chapter.  We add one more run to the results in Table \ref{tab:cgdirectmg}, namely
\begin{cline}
./fish2 -da_refine 7 -ksp_type cg -pc_type mg
\end{cline}
where \texttt{fish2.c} is a Chapter \ref{chap:pr} program for solving the same Poisson problem.  The result shows significant further improvement over the nested-dissection direct methods.

However, to exploit the power of geometric multigrid we also need to do a better job using \PETSc's tools for assembling the finite linear system.  Specifically, we need to allow the \pDMDA object to ``tell us'' the grid on which we are assembling the linear system, something which is not done in \texttt{poisson.c}.


\section{Exercises}

\renewcommand{\labelenumi}{\arabic{chapter}.\arabic{enumi}\quad}

\begin{enumerate}

\item Use \texttt{DMDACreate1d()}, and other appropriate modifications of \texttt{poisson.c}, to write a code \texttt{poisson1D.c} for the 1D Poisson problem
    $$-u'' = f, \quad u(0)=u(1)=0.$$
For simplicity, choose exact solution $u(x)=x^2(1-x^2)$ to determine $f(x)$.  Note you can either use \texttt{MatSetValues()} or \texttt{MatSetValuesStencil()} because indexing is easy in 1D.  Show that your results converge on a sequence of refining grids, as done in Figure \ref{fig:poisson-conv}; the errors should go to zero as $O(h^2)$.  Find a grid refinement level, and use \pKSP options as appropriate, to get 11 digit accuracy at every grid point.  \emph{It will never again be this good.}
%  FD is:  - u_{i-1} + 2 U_i - u_{i+1} = h^2 f_i
%  for exact u:  u'   = 2x(1-x^2) + x^2(-2x) = 2x - 4x^3
%                -u'' = -(2 - 12 x^2) = 12x^2 - 2

\item Modify \texttt{poisson.c} to allow non-homogeneous Dirichlet boundary data $u|_{\partial \mathcal{S}}=g$.  Test the correctness (i.e.~convergence at the expected rate) of your code by solving \eqref{poissonsquare} with both $f$ and $g$ derived from an exact solution $u(x,y)=3x + \sin(20xy)$.  Again examine  numerical errors under grid refinement.

\item \label{exer:st:jacobivsnone} Confirm the nearly-identical performance of un-preconditioned and Jacobi-preconditioned CG, by doing
\begin{cline}
$ timer ./poisson -da_refine N -ksp_converged_reason -ksp_type cg -pc_type X
\end{cline}
%$
for \texttt{N} $\in\{1,\dots,6\}$ and \texttt{X} $\in\{$\texttt{none},\texttt{jacobi}$\}$.  Explain.
% ANSWER: because all non-boundary condition rows, i.e. all rows with coupling to other equations, which could be split out as A = I \oplus B for B with same condition number as A, have the same diagonal entry of "4" so the only effect of -pc_type jacobi is to scale the spectrum, but the fundamental CG convergence result says CG depends on condition number, i.e. in invariant to scaling of spectrum
Now confirm that IC($0$) preconditioning gives lower iteration counts but the same bad scaling.

\item Add \texttt{-log\_summary} to an un-preconditioned CG run, e.g.
\begin{cline}
$ ./poisson -ksp_converged_reason -ksp_type cg -pc_type none -log_summary
\end{cline}
%$
By looking at the ``Count'' column, and noting that the iteration count comes from \texttt{-ksp\_converged\_reason} output, confirm that the computational work of one CG iteration consists of two inner products (\texttt{VecTDot}), three vector updates (\texttt{VecAXPY} and \texttt{VecAYPX}), and one matrix-vector product (\texttt{MatMult}).  Find a pseudo-code for CG in some textbook, and confirm this work pattern.

\item \label{exer:st:Anorm} Let $A$ be a symmetric positive-definite $N\times N$ matrix.  Note $A$ is diagonalizable and has positive eigenvalues.
  \renewcommand{\labelenumii}{(\emph{\roman{enumii}})}
  \begin{enumerate}
  \item Show that \eqref{normAdefn} defines a norm on $\RR^N$.
  \item Define the matrix $\sqrt{A}$ as the unique symmetric and positive-definite matrix such that $(\sqrt{A})^2 = A$.  Show that $\|v\|_A = \|\sqrt{A}\,v\|_2$ and that $\kappa_2\left(\sqrt{A}\right) = \sqrt{\kappa_2(A)}$.
  \item Recalling that $A\be=-\br$, show that
      $$\frac{1}{\|\sqrt{A}\|_2} \|\br\|_2 \le \|\be\|_A \le \|\sqrt{A}\|_2 \|\br\|_2.$$
  \item Prove part (\emph{iii}) of the Theorem on page \pageref{thm:cgiterations}.
  \end{enumerate}

\item If you have difficulty reproducing timings like those in Table \ref{tab:cgdirectmg}, note that they come from a \PETSc configuration with ``optimized'' configuration option\sidenote{\emph{Not} runtime option!} \texttt{--with-debugging=0}.  If you have not already done so, see the \PETSc installation page
\begin{quote}
\href{http://www.mcs.anl.gov/petsc/documentation/installation.html}{\texttt{www.mcs.anl.gov/petsc/documentation/installation.html}}
\end{quote}
and generate a new configuration, with a new \texttt{PETSC\_ARCH} value, with this ``optimized'' configuration option.  Generate your own version of Table \ref{tab:cgdirectmg}.

\item Use \texttt{DMDACreate3d()} and etc.~in a code \texttt{poisson3D.c} which solves a 3D Poisson problem on the unit cube $\mathcal{C}=[0,1]^3$ using the same \pDMDA and \pKSP methods as in \texttt{poisson.c}.  Looking forward, how is the approach taken in Chapter \ref{chap:ad}, to a closely-related linear PDE problem in 3D, different from your code?

\end{enumerate}


Solving ordinary differential equations (ODEs) with \PETSc ought to be easy.  Because we have practice with \PETSc objects, it is.  This Chapter ``restarts'' on an easy problem, and a small example, but the goal is to transition to numerical methods for time-dependent, parabolic PDEs like the heat equation.  This transition, too, is straightforward.

In this Chapter we look at three examples:
\begin{itemize}
\item a linear system of two ODEs,
\item an arbitrarily-large system of ODEs arising from spatial finite-difference approximation of the 2D time-dependent heat PDE,
\item and another large ODE system arising from two coupled nonlinear PDEs in 2D, forming a two-species reaction/diffusion model.
\end{itemize}

These problems are solved using a \PETSc \pTS time-stepping object.  The particular integration method used by the \pTS can be chosen at run-time.  Because many possible methods exist, a review of numerical time-stepping is appropriate.  We will recall the basics of both \emph{explicit} and \emph{implicit} methods.  Because implicit time-stepping methods need the full stack of \pSNES/\pKSP/\pPC solvers for the (generally) non-linear equations at each time step, our experience with \pSNES examples from previous Chapters will come in handy.

User code will semi-discretize the above two PDE examples in space.  This ``method of lines'' approach, a common way to solve PDEs using \PETSc \pTS, avoids any need to write code specific to a temporal discretization.  Though the last of the above examples is a \emph{system} of PDEs, the additional \PETSc constructs needed for this kind of system are quite easy to write.


\section{Systems of ODEs}

Consider an ODE system in form
\begin{equation}
\by' = \bg(t,\by)  \label{eq:ts:ode}
\end{equation}
where $\by(t) \in \RR^N$ and $\by'=d\by/dt$.  Suppose an \emph{initial value} is given:
\begin{equation}
\by(t_0) = \by_0.  \label{eq:ts:ode:iv}
\end{equation}
Under reasonable assumptions on the behavior of the \emph{right-hand-side function} $\bg$, the continuum problem \eqref{eq:ts:ode}, \eqref{eq:ts:ode:iv} is a well-posed problem, at least for short times.\sidenote{This claim is well-known and will be stated with precision momentarily.}  Thus one can make predictions from such a ODE \emph{initial value problem} (IVP), forward or backward from the initial time.

Let us be specific about sufficient regularity of $\bg$ to imply well-posedness.  We will assume for simplicity that $\bg(t,\by)$ is continuous on a cylinder around the initial point $(t_0,\by_0)$,
   $$\mathcal{D} = \{(t,\by) \,:\, |t-t_0| \le \delta, \|\by - \by_0\| \le \omega\}$$
where $\delta > 0$ and $\omega > 0$.  We further assume that $\bg$ is \emph{Lipschitz} in its second argument, so that output differences are bounded proportionally to input differences: there is $L\ge 0$ so that
\begin{equation}
\|\bg(t,\by_1) - \bg(t,\by_0)\| \le L \|\by_1-\by_0\|  \label{eq:ts:glipschitz}
\end{equation}
for $(t,\by_i) \in \mathcal{D}$.  Then the problem \eqref{eq:ts:ode}, \eqref{eq:ts:ode:iv} has a unique continuous, and continuously-differentiable, solution $\by(t)$ on a generally-shorter interval, namely $|t-t_0|<\eps$ for some $0 < \eps \le \delta$ \citep[section 17.5]{HirschSmaleDevaney2004}.  Furthermore the solution depends continuously on both the initial value $\by_0$ and the right-hand-side $\bg$.

Our brief attention to well-posedness is motivated by the following practical observation:  When we run a numerical differential equation code it will produce numbers.\sidenote{It \emph{always} produces numbers!}  These numbers are (essentially) never the exact solution of the differential equation, but they are ``correct'' in a numerical-analysis sense if we can demonstrate convergence to the solution of a well-posed continuum problem.  Thus we accept ``wrong'' numbers as correct in cases where demonstrably convergent numerical analysis is applied to a well-posed problem.

Much more egregiously wrong, however, are numbers which represent no continuum solution at all.  Benign-looking scalar nonlinear ODEs can put us in such peril.  In fact, Exercise \ref{chap:ts}.\ref{exer:ts:tan} gives a simple, and well-known, example where the solution ceases to exist after a certain finite interval of time.  The approximating code sails right by the end of this interval of time, producing numbers that are infinitely-erroneous.  The examples in this book include several nonlinear PDEs, where this concern is yet greater.  Thus caution is appropriate.  When possible we consider well-posedness, and perhaps even approximate-ability (regularity), prior to implementation.

For linear systems, solutions exist for all time.  The following example ODE system is thus a safe starting point for introducing the numerical tools.

\noindent\hrulefill
\begin{example}  \label{ex:ts:odeeasy} Consider the initial value problem in two dimensions
\begin{equation}
   \by' = \begin{bmatrix} y_1 \\ - y_0 + t\end{bmatrix}, \qquad \by(0) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \label{eq:ts:example}
\end{equation}
from initial time $t_0=0$.  This is a linear system with
    $$\bg(t,\by) = A \by + \bbf \,\text{ where } A = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \text{ and } \bbf(t) = \begin{bmatrix} 0 \\ t\end{bmatrix}.$$
The exact solution, shown in Figure \ref{fig:ts:ode}, is
\begin{equation}
    \by(t) = \begin{bmatrix} t - \sin t \\ 1 - \cos t \end{bmatrix}.\label{eq:ts:examplesoln}
\end{equation}

\vspace{-3mm}
\begin{figure}
\includegraphics[width=0.7\textwidth]{figs/ode}
\caption{The solution to \eqref{eq:ts:example} is given by \eqref{eq:ts:examplesoln}, with $y_0(t)$ solid and $y_1(t)$ dashed.}
\label{fig:ts:ode}
\end{figure}
\end{example}
\vspace{-5mm}
\noindent\hrulefill


\section{Methods, and their accuracy, for ODE initial value problems}

The above example ODE system can be approximated by almost any time-stepping numerical method.  They all start from the initial values and use the right-hand-side function $g(t,\by)$ to update an approximate solution, generating sequences $\{\bY_\ell\}$ which approximate the solution at discrete times $t_\ell$.  The goal, of course, is that $\bY_\ell \approx \by(t_\ell)$.  Because these ideas are easy to find in the literature\sidenote{A good reference is \citep{AscherPetzold1998}, for example.}, and are likely to be in the reader's background in some form, we only supply a gloss here.

Let $t_0<t_1<t_2<\dots<t_L$ be a finite sequence of times, with steps $\Delta t_\ell = t_\ell - t_{\ell-1}$ for $\ell=1,2,\dots,L$.  Let $\bY_0 = \by_0$ be the known starting value  \eqref{eq:ts:ode:iv}.  \emph{Euler's method}, also known as the forward Euler method, generates $\bY_1,\bY_2,\dots,\bY_L$ from
\begin{equation}
\frac{\bY_\ell - \bY_{\ell-1}}{\Delta t_\ell} = \bg(t_{\ell-1},\bY_{\ell-1}). \label{eq:ts:forwardeuler}
\end{equation}
One can also write this as a formula for the updated value $\bY_\ell$,
\begin{equation}
\bY_\ell = \bY_{\ell-1} + \Delta t_\ell \, \bg(t_{\ell-1},\bY_{\ell-1}). \label{eq:ts:forwardeulerupdate}
\end{equation}

The \emph{backward Euler method} is just as easy to state,
\begin{equation}
\frac{\bY_\ell - \bY_{\ell-1}}{\Delta t_\ell} = \bg(t_{\ell},\bY_{\ell}),  \label{eq:ts:backwardeuler}
\end{equation}
but much more work is required to use it in practice.  Indeed, just writing the analog of \eqref{eq:ts:forwardeulerupdate}, namely a formula by which to compute $\bY_\ell$, would require doing algebra on the function $\bg$.  Backward Euler is an \emph{implicit} method.  At each time step \eqref{eq:ts:backwardeuler} generates the kind of (generally) nonlinear system, with unknown values $\bY_\ell$, which we solved in previous Chapters by using Newton's method and \pSNES.

Both methods \eqref{eq:ts:forwardeuler} and \eqref{eq:ts:backwardeuler}, which are the simplest approximations of \eqref{eq:ts:ode}, compute a first-order finite difference approximation of the derivative of $\by(t)$ by evaluating the right-hand side function $\bg(t,\by)$ at a particular point.  They differ only in the evaluation point.  They are ``first-order accurate'' in the simple sense that, for any twice-continuously-differentiable function $f(t)$, by Taylor's theorem
    $$\frac{f(t+\Delta t) - f(t)}{\Delta t} = f'(t) + O(\Delta t)$$
as $\Delta t \to 0$.  One says that the Euler methods have first-order \emph{local truncation error}.

Under sufficient assumptions of smoothness, another meaning of ``first-order accuracy'' also applies, namely as a rate of convergence as $\Delta t\to 0$.  Indeed, one would hope that the approximations $\bY_\ell$ converge to the values $\by(t_\ell)$ as $\Delta t\to 0$, but this requires something more than small local truncation error.  One also needs to bound the rate at which errors build up over many steps.

As a first attempt to prove convergence, we start by assuming that the solution $\by(t)$ exists, that a Lipschitz bound \eqref{eq:ts:glipschitz} applies to $\bg(t,\by)$, and that the solution has bounded second derivative ($|\by(t)''|\le M$).  These statements all apply on some interval $[t_0,t_f]$.  One can then show that the Euler methods satisfy $|\bY_\ell-\by(t_\ell)| = O(\Delta t^1)$.  In slightly more detail, one can show \citep{AscherPetzold1998}
\begin{equation}
|\bY_\ell-\by(t_\ell)| \le \frac{\Delta t\,M}{2L} (e^{L(t_\ell-t_0)}-1),  \label{eq:ts:eulerbound}
\end{equation}
but the exponential rate on the right is often a severely-pessimistic bound.

The \emph{numerical error} $E_\ell = |\bY_\ell-\by(t_\ell)|$, also called the \emph{global truncation error}, is therefore first-order in $\Delta t$ for the Euler methods: $E_\ell = O(\Delta t^1)$.  This statement is the intended meaning of ``first-order accuracy,'' even though bound \eqref{eq:ts:eulerbound} itself is rarely used quantitatively.\sidenote{Not least because we apply higher-order methods, as below.}

The \emph{order} of a method, stated as $E_\ell = O(\Delta t^p)$ for some $p>0$, tells us what to expect when we shorten the time step, namely by how much a reduction of the time step will reduce the numerical error.  Halving the time step in a first-order method should reduce the numerical error by half, for a second-order method by a fourth, and so on.

\newcommand{\RKtwoa}{RK$2$a\xspace}
\newcommand{\RKthreebs}{RK$3$bs\xspace}
\newcommand{\RKfour}{RK$4$\xspace}

It should be no surprise that there are methods with improved accuracy compared to the Euler methods.  Among these are the \emph{one-step multi-stage} methods, with the \emph{Runge-Kutta} family best known.  By definition, one-step methods use $\bY_{\ell-1}$ only, and not previous values, to build the next approximation $\bY_\ell$, but generally by multiple evaluations of the right-hand side $\bg$.  For example, denoting the time-step simply as $h=\Delta t_\ell$, the \emph{explicit trapezoidal} method \citep{AscherPetzold1998} is a second-order rule which takes a forward Euler step but then ``goes back'' and recomputes the step by the average of values of $\bg$:
\begin{align}
\hat\bY &= \bY_{\ell-1} + h\, \bg(t_{\ell-1},\bY_{\ell-1}) \label{eq:ts:rk2a} \\
\bY_\ell &= \bY_{\ell-1} + \frac{h}{2} \bg(t_{\ell-1},\bY_{\ell-1}) + \frac{h}{2} \bg(t_\ell,\hat\bY). \notag
\end{align}
Though two evaluations of $\bg$ (i.e.~the two \emph{stages}) occur at each time step, only the final result $\bY_\ell$ is used to inform the next time step (i.e.~the one which computes $\bY_{\ell+1}$).  In this sense the method is both ``one-step'' and ``multi-stage.''  (Method \eqref{eq:ts:rk2a} is also called ``\RKtwoa'' in \PETSc; it is one of several possible second-order Runge-Kutta methods.)

The well-known fourth-order Runge-Kutta method ``\RKfour'' has four stages.  Instead of stating this particular scheme using formulas like those in \eqref{eq:ts:rk2a} above, we define tabular notation for any one-step, $s$-stage method.  The $s+1$ formulas
\begin{align}
\hat\bY_i &= \bY_{\ell-1} + h \sum_{j=1}^s a_{ij}\, \bg(t_{\ell-1} + c_j h, \hat\bY_j), \qquad 1 \le i \le s \label{eq:ts:rkgeneral} \\
\bY_\ell  &= \bY_{\ell-1} + h \sum_{i=1}^s b_i\, \bg(t_{\ell-1} + c_i h, \hat\bY_i) \notag
\end{align}
correspond to the \emph{tableau} \citep{Butcher2008}
\begin{center}
\begin{tabular}{c|cccc}
$c_1$    & $a_{11}$ & $a_{12}$ & $\cdots$ & $a_{1s}$ \\
$c_2$    & $a_{21}$ & $a_{22}$ & $\cdots$ & $a_{2s}$ \\
$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
$c_s$    & $a_{s1}$ & $a_{s2}$ & $\cdots$ & $a_{ss}$ \\ \hline
         & $b_1$    & $b_2$    & $\cdots$ & $b_s$
\end{tabular}
\end{center}

For example, the $s=2$ method \RKtwoa is the left tableau in Table \ref{tab:ts:exampletableau}, and the $s=4$ method \RKfour is the middle one.  Spaces in the tableau correspond to zeros.  In all three methods in Table \ref{tab:ts:exampletableau}, $\hat\bY_1=\bY_{\ell-1}$ so the first rows are trivial.  Also, because these schemes are explicit they have nonzero $a_{ij}$ only strictly below the diagonal; we return to this idea below.  In summary, a one-step, multi-stage method, whether described by \eqref{eq:ts:rkgeneral} or a tableau, computes a new value $\bY_\ell$ via intermediate results $\hat\bY_i$.

\begin{table}
\begin{tabular}{c|cc}
$0$   \\
$1$ & $1$ \\ \hline
    & $\frac{1}{2}$ & $\frac{1}{2}$
\end{tabular}
\hfill
\begin{tabular}{c|cccc}
$0$   \\
$\frac{1}{2}$ & $\frac{1}{2}$ \\
$\frac{1}{2}$ & $0$           & $\frac{1}{2}$ \\
$1$           & $0$           & $0$           & $1$  \\ \hline
              & $\frac{1}{6}$ & $\frac{1}{3}$ & $\frac{1}{3}$ & $\frac{1}{6}$
\end{tabular}
\hfill
\begin{tabular}{c|cccc}
$0$   \\
$\frac{1}{2}$ & $\frac{1}{2}$ \\
$\frac{3}{4}$ & $0$           & $\frac{3}{4}$ \\
$1$           & $\frac{2}{9}$ & $\frac{1}{3}$ & $\frac{4}{9}$ \\ \hline
              & $\frac{2}{9}$ & $\frac{1}{3}$ & $\frac{4}{9}$ & $0$ \\
              & $\frac{7}{24}$ & $\frac{1}{4}$ & $\frac{1}{3}$ & $\frac{1}{8}$
\end{tabular}
\medskip
\caption{Tableau for the explicit trapezoidal rule (\RKtwoa; left), the classical fourth-order Runge-Kutta method (\RKfour; middle), and an embedded four-stage, third-order scheme (\RKthreebs, the \PETSc default; right).} \label{tab:ts:exampletableau}
\end{table}

We now describe the default Runge-Kutta scheme in \PETSc.  It is \emph{adaptive} which means that it adjusts the time step by a procedure which uses information from the already-computed values.  For example, an adaptive scheme will take shorter steps when the values from $\bg$ are rapidly-changing or irregular, and longer steps when they are smoother and more predictable.  Such strategies require error estimation and control \citep{AscherPetzold1998}.  It is done internally by the \PETSc time-steppers (below), and not visible in our user code.

One strategy to build an adaptive Runge-Kutta method is to make it \emph{embedded}.  Such a scheme is in fact two methods which have the same $c_j$ and $a_{ij}$ coefficients in their tableau, but different $b_j$ \emph{and} different orders of accuracy.  The Bogacki-Shampine scheme \RKthreebs is such an embedded pair shown at right in Table \ref{tab:ts:exampletableau}.\sidenote{It is the \texttt{ode23} method in \Matlab.}  Each is four-stage, but one is of third-order and one of second-order.  In the tableau the $b_j$ coefficients are listed on different rows below the horizontal line.  The different results from these two schemes is used to decide if the time step was short enough to satisfy the error tolerance/goal \citep{Butcher2008}.  In \RKthreebs specifically, one scheme (first $b_j$ row) makes $O(\Delta t^3)$ local truncation errors, while the other (second $b_j$ row) makes $O(\Delta t^2)$ errors, and the difference estimates the accuracy of the lower order scheme.  The work of computing two approximations is nearly the same cost as just one, because they share the same evaluations of $\bg$; this is the central meaning of ``embedded.''  Further details of the \RKthreebs scheme, including its stability and efficiency properties, are beyond our scope, but see \citep{BogackiShampine1989}.

There are alternative families other than one-step schemes.  For example, the \emph{second-order backwards differentiation formula}
\begin{equation}
\bY_\ell = \frac{4}{3} \bY_{\ell-1} - \frac{1}{3} \bY_{\ell-2} + \frac{2}{3} h \bg(t_\ell,\bY_{\ell}) \label{eq:ts:bdf2}
\end{equation}
is a \emph{multi-step} method.  It uses \emph{two} previous values $\bY_{\ell-2},\bY_{\ell-1}$.  This works in a straightforward way only if the step-size $h=\Delta t$ is independent of $\ell$.  Other schemes such as the \emph{general linear methods} \citep{Butcher2008} combine the multi-stage and multi-step ideas.  However, in our quick treatment of time-stepping, we will say little more about multi-step and general linear methods.

A general theme when comparing methods is that the cost is proportional to the number of (distinct) evaluations of the right-hand side $\bg$.  At least for explicit schemes, the additional arithmetic in a particular scheme, represented for example by the number of coefficients in the tableau, is often less important.  Implicit schemes, however, also require a solver for the algebraic system that arises at each step.  Solving this system, including but not limited to additional evaluations of $\bg$ that this causes (e.g.~as needed in the Newton iteration), dominates the cost of an implicit scheme.

A large number of possible methods are available from the \PETSc command line.   Choosing the ``best'' one for a given ODE system is often not a reasonable goal.  Our goal in this Chapter is only to suggest some reasonable choices for broad classes of problems.  In any case we might invoke an essential \PETSc mantra here: a rich set of command-line, run-time options is \emph{not a bad thing}.


\section{\PETSc \pTS: a first example}

The example system on page \pageref{ex:ts:odeeasy} is a good starting point for our first ODE-solving program because it is of small and fixed dimension\sidenote{Our last fixed-dimension programs were at the start of Chapter \ref{chap:nl}.  Later codes \texttt{heat.c} and \texttt{pattern.c} in this Chapter us a \pDMDA grid and have run-time-controlled dimension.} and because it has an exact solution from which we may evaluate numerical error.

\cinputpart{ode.c}{\CODELOC}{The \texttt{main()} part of \texttt{ode.c} creates and configures a \pTS object, plus the \pVecs for approximate and exact solutions.}{I}{//MAIN}{//ENDMAIN}{code:ts:ode:main}

The program \texttt{c/\CODELOC ode.c} is shown in Codes \ref{code:ts:ode:main} and \ref{code:ts:ode:callbacks}.  The first thing the code does is to create space for the approximate and exact solutions in \pVecs of fixed size $N=2$.  Then we have these commands which initialize the \pTS object:
\begin{code}
  TSCreate(PETSC_COMM_WORLD,&ts);
  TSSetProblemType(ts,TS_NONLINEAR);
  TSSetRHSFunction(ts,NULL,FormRHSFunction,NULL);
\end{code}
By setting the problem type to \texttt{TS\_NONLINEAR} we are saying that the problem is in form \eqref{eq:ts:ode} with a potentially-nonlinear function $\bg(t,\by)$; we ignore the fact that our particular system is linear.  Calling \texttt{TSSetRHSFunction()} sets a call-back to our function \texttt{FormRHSFunction()} which evaluates $\bg(t,\by)$; see Code \ref{code:ts:ode:callbacks}.

Next we set the particular \pTS solver to Runge-Kutta:
\begin{code}
  TSSetType(ts,TSRK);
\end{code}
This is only a default choice as it can be overridden by run-time option \texttt{-ts\_type}.

Then some additional commands configure the time-axis of the \pTS, but again these choices can be overridden by command-line options:
\begin{code}
  TSSetInitialTimeStep(ts,t0,dt);
  TSSetDuration(ts,100*(int)((tf-t0)/dt),tf-t0);
  TSSetExactFinalTime(ts,TS_EXACTFINALTIME_MATCHSTEP);
  TSSetFromOptions(ts);
\end{code}
Note that \texttt{TSSetInitialTimeStep()} sets both the initial time \texttt{t0} and the step \texttt{dt}.  If the numerical ODE method chosen at run-time is adaptive then \texttt{dt} is \emph{only} the initial time step because the method generally modifies the step after the first one.  \texttt{TSSetDuration()} sets both the duration (\texttt{tf-t0}) and the maximum number of steps that the solver is allowed to take; we set the latter to 100 times the intended number of steps of length \texttt{dt}.

The values for the initial time, duration, and initial time step can all have changed after the \texttt{TSSetFromOptions()} call because of run-time options.  Thus, before we ask for the \pTS to solve the problem by time stepping, we get the current time (\texttt{TSGetTime()}) and we set the initial values from that.  The function that evaluates the exact solution, including the initial values, is shown in Code \ref{code:ts:ode:callbacks}.  After a call to \texttt{TSSolve()} comes a bit more standard code which computes the norm of the numerical error and then destroys the objects we created.

Code \ref{code:ts:ode:callbacks} shows function \texttt{FormRHSFunction()} which evaluates $\bg(t,\by)$ for the example on page \pageref{ex:ts:odeeasy}.  The input \pVec \texttt{y} is read the latter into a \texttt{const} array with \texttt{VecGetArrayRead()}, while the output \pVec \texttt{g}, which our code modifies, is accessed through \texttt{VecGetArray()}.  Note that \texttt{FormRHSFunction()} is similar in signature to \texttt{FormFunction()} in Chapter \ref{chap:nl} examples.

\cinputpart{ode.c}{\CODELOC}{\texttt{SetFromExact()} computes the exact solution, while \texttt{FormRHSFunction()} computes the right-hand side of \eqref{eq:ts:ode}, namely $\bg(t,\by)$.}{II}{//CALLBACKS}{//ENDCALLBACKS}{code:ts:ode:callbacks}

Let us try it out.  At the start it makes sense to \texttt{monitor} and \texttt{view} the \pTS.  First the result of \texttt{-ts\_monitor}:
\begin{cline}
$ cd c/ch5/
$ make ode
$ ./ode -ts_monitor
0 TS dt 0.1 time 0.
1 TS dt 0.170141 time 0.1
2 TS dt 0.169917 time 0.270141
3 TS dt 0.171145 time 0.440058
...
86 TS dt 0.206773 time 19.6777
87 TS dt 0.11548 time 19.8845
88 TS dt 0.205616 time 20.
error at tf = 20.000 with 88 steps:  |y-y_exact|_inf = 0.00930352
\end{cline}
%$
It is clear that the time-stepping method is adaptive and, given the small numerical error, that it has essentially succeeded.  Comparing Figure \ref{fig:ts:ode}, an error of magnitude $9\times 10^{-3}$ at time $t=20$ is indeed small.

What solution method was used?  Do \texttt{-ts\_view}:
\begin{cline}
$ ./ode -ts_view
TS Object: 1 MPI processes
  type: rk
  maximum steps=20000
  maximum time=20.
  total number of nonlinear solver iterations=0
  total number of nonlinear solve failures=0
  total number of linear solver iterations=0
  total number of rejected steps=12
    RK 3bs
    Abscissa     c =  0.000000  0.500000  0.750000  1.000000 
  FSAL: yes
  TSAdapt Object:   1 MPI processes
...
\end{cline}
%$
As asserted earlier, the default for type \texttt{TSRK} is \RKthreebs.  Interestingly, the adaptive method tried and then rejected $12$ steps in addition to the $88$ which were accepted.


\section{Controlling \pTS}

As with other major \PETSc solver types, there are many control options.  Let's recall how to get help:
\begin{cline}
$ ./ode -help |grep ts_
\end{cline}
%$

To see numerical values of the solution at each time-step, do
\begin{cline}
$ ./ode -ts_monitor_solution
\end{cline}
%$
For a run-time graphical (line-graph) view of the solution, do
\begin{cline}
$ ./ode -ts_monitor -ts_monitor_lg_solution -draw_pause 0.1
\end{cline}
%$
The result is a X windows graphic of the evolving solution, similar to Figure \ref{fig:ts:ode}.  

Figure \ref{fig:ts:ode} was actually produced by a Python script called \texttt{c/\CODELOC plotTS.py}.  It takes as input a binary output from \pTS monitoring, which is generated like this:
\begin{cline}
$ ./ode -ts_monitor binary:t.dat -ts_monitor_solution binary:y.dat
$ ./plotTS.py -o figure.png t.dat y.dat
\end{cline}
(Running \texttt{plotTS.py} requires copies of, or sym-links to, Python scripts \texttt{PetscBinaryIO.py} and \texttt{petsc\_conf.py} from the \texttt{bin/} directory of your \texttt{\$PETSC\_DIR}.)

We are free to adjust the start time, end time, and initial time step at the command line:
\begin{cline}
$ ./ode -ts_init_time 1.0 -ts_final_time 2.0 -ts_dt 0.001 -ts_monitor
\end{cline}
%$
We can also turn off adaptive time-stepping:
\begin{cline}
$ ./ode -ts_adapt_type none -ts_monitor
\end{cline}
%$
This generates $200$ steps using the requested initial step \texttt{dt}$=0.1$.

We can also change the relative and absolute tolerances used in adaptive time-stepping away from their default values of $10^{-4}$:
\begin{cline}
$ ./ode -ts_monitor -ts_rtol 1.0e-1
$ ./ode -ts_monitor -ts_rtol 1.0e-6
$ ./ode -ts_monitor -ts_rtol 1.0e-6 -ts_atol 1.0e-6
\end{cline}
%$
These runs give $19$, $117$, and $392$ steps, respectively, while the corresponding final numerical errors are $1.3 \times 10^0$, $3.6\times 10^{-3}$, and $1.2\times 10^{-4}$.  The reader should note, emphatically, that:
\begin{quote}
Supplying a particular value to \texttt{-ts\_rtol} and/or \texttt{-ts\_atol} \emph{does not} cause the final numerical error to be bounded by the same value.
\end{quote}
After all, the numerical error \emph{accumulates} with the steps, and can grow exponentially.
  
The time-stepping method itself is easy to set by using \texttt{-ts\_type}.  If Runge-Kutta is chosen then the flavor is assigned with \texttt{-ts\_rk\_type}:
\begin{cline}
$ ./ode -help |grep ts_type
  -ts_type <rk>: TS method (one of) euler beuler cn pseudo gl ssp theta alpha rk
                 arkimex rosw eimex mimex
$ ./ode -ts_type rk -help |grep ts_rk
  -ts_rk_type <3bs> (choose one of) 5dp 5f 4 3bs 3 2a 1fe
\end{cline}

Various \pTS methods can be compared on this first ODE example problem; see Exercises \ref{chap:ts}.\ref{exer:ts:odepossible} and \ref{chap:ts}.\ref{exer:ts:odeserial}.  However, we defer showing the results of a comparison until after introducing the concepts of stiffness and stability.  At that point we show results from a much larger ODE system, one generated from the time-dependent heat equation, a PDE.


\section{Implicitness, stiffness, and stability}

With the exception of the backward Euler method \eqref{eq:ts:backwardeuler}, the above methods are \emph{explicit}.  They compute $\bY_\ell$ from previous solution values via a fixed number of applications of the right-hand side $\bg$.  For example, the \RKtwoa (explicit trapezoid) rule \eqref{eq:ts:rk2a} can be written
\begin{equation}
\bY_\ell = \bY_{\ell-1} + \frac{h}{2} \bg(t_{\ell-1},\bY_{\ell-1}) + \frac{h}{2} \bg\big(t_\ell,\bY_{\ell-1} + h\, \bg(t_{\ell-1},\bY_{\ell-1})\big). \label{eq:ts:rk2aexplicit}
\end{equation}
In terms of their tableau, explicit Runge-Kutta schemes only have non-zero coefficients $a_{ij}$ below the diagonal (Table \ref{tab:ts:exampletableau}).

\emph{Implicit} methods, by contrast, require solving equations.  At each time step the implicit scheme can be written as the (generally) nonlinear system
\begin{equation}
    \bF(\bY_\ell)=0.  \label{eq:ts:implicitgenericsystem}
\end{equation}
for the $N$ real variables, the components of $\bY_\ell$.  For example, the backward Euler method \eqref{eq:ts:backwardeuler} corresponds to the function $\bF(\bX) = \bX - \bY_{\ell-1} - h \bg(t_\ell,\bX)$.

We will solve system \eqref{eq:ts:implicitgenericsystem} by Newton's method, for which we will need to differentiate $\bg$ either exactly or by finite-differences.  In contrast with an explicit scheme, we will also need to evaluate $\bg$ an unknown number of times during the iterative solution process.

A family of implicit methods are the \emph{theta ($\theta$) methods}, $0<\theta\le 1$,
\begin{equation}
\bY_\ell = \bY_{\ell-1} + (1-\theta) h \bg(t_{\ell-1},\bY_{\ell-1}) + \theta h \bg(t_\ell,\bY_{\ell}),  \label{eq:ts:theta}
\end{equation}
with tableau
\begin{center}
\begin{tabular}{c|cc}
$0$   \\
$1$ & $1-\theta$ & $\theta$ \\ \hline
    & $1-\theta$ & $\theta$
\end{tabular}
\end{center}
The $\theta=0$ case is the explicit forward Euler method \eqref{eq:ts:forwardeulerupdate}, while the $\theta=1$ case is the backward Euler method \eqref{eq:ts:backwardeuler}; these have first-order local truncation error.  The $\theta=1/2$ case is the \emph{trapezoid} method, also known as \emph{Crank-Nicolson} \citep{MortonMayers2005} in the context of the PDE methods which we will pursue below.  It has second-order $O(\Delta t^2)$ local truncation error.

Both the \RKtwoa rule \eqref{eq:ts:rk2aexplicit} and the $\theta=1/2$ case of \eqref{eq:ts:theta} are second-order.  Why would one make the effort to solve a system of algebraic equations at each time step so as to use the latter implicit scheme?  The answer is another ``numerical fact of life'',
\begin{quote}
\emph{stability} is obligatory in a numerical scheme, whereas higher-order error properties are merely desirable.
\end{quote}

Some informal discussion of the concepts of the ``stability'' of numerical schemes and the ``stiffness'' of ODE problems is appropriate even before giving precise definitions of these concepts.  Note there are multiple useful definitions of stability; we will focus below on ``absolute stability,'' plus some closely-related definitions.

Most explicit schemes are ``conditionally stable'' in the informal sense that they give good results when used with sufficiently-small time steps.  This computational burden may be avoidable, despite the obvious trade-off of many time-steps versus solving equations.  Some computations, with some accuracy goals, can succeed if done implicitly even though they are too costly by (conditionally-stable) explicit time-stepping.  This is characteristic of the discretized heat-equation-type PDE examples later in this Chapter.

The ``stiffness'' of certain ODE problems drives the investigation of the stability of different methods.  As a practical definition, an ODE system is \emph{stiff} if it has short time-scales which do not need to be resolved as accurately as some other time-scales, but which cause explicit schemes to take short time-steps.  An example is appropriate.

\noindent\hrulefill
\begin{example}  \label{ex:ts:odestiff}  Consider the linear ODE IVP, in $N=3$ dimensions,
\begin{equation}
   \by' = B \by, \qquad \by(0) = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \label{eq:ts:stiffexample}
\end{equation}
where
\begin{equation}
   B = \begin{bmatrix} 0 & 1 & 0 \\
                      -1 & 0 & 0.1 \\
                       0 & 0 & -101 \end{bmatrix}. \label{eq:ts:stiffexamplematrix}
\end{equation}
The exact solution at a particular time, say $t_f=10$ for concreteness, can be found by computing eigenvalues and thereby exponentiating a matrix (Exercise \ref{chap:ts}.\ref{exer:ts:stiffexample}).  To an absolute accuracy of $10^{-7}$ the solution is
\begin{equation}
    \by(t_f) = e^{B t_f} \by(0) = \begin{bmatrix} -1.383623 \\
                                                 -0.295886 \\
                                                  0 \end{bmatrix}.\label{eq:ts:stiffsoln}
\end{equation}

The eigenvalue $\lambda_{\min}=-101$ (Exercise \ref{chap:ts}.\ref{exer:ts:stiffexample}) of $B$, which is also an entry of $B$ in this case, corresponds to a very short timescale and rapid decay rate.  Precisely following this fast time scale, especially in a relative-magnitude sense, is not needed to produce accurate values for the first two components of $\by(t_f)$, in particular.

Considering the two $O(\Delta t^2$) methods introduced already, the explicit (\RKtwoa) and implicit (Crank-Nicolson) trapezoid rules, we write a modification of \texttt{ode.c} called \texttt{stiff.c} (Exercise \ref{chap:ts}.\ref{exer:ts:stiffcode}) to compare these methods on this problem.  Starting with the explicit \RKtwoa method, we take 200 steps from $t_0=0$ to $t_f=10$:
\begin{cline}
$ ./stiff -ts_type rk -ts_rk_type 2a -ts_adapt_type none -ts_dt 0.05
Vec Object: 1 MPI processes
  type: seq
8.08433e+182
-8.16517e+184
8.24763e+187
total steps = 200
\end{cline}
%$
This is clearly nonsense; an explosion.  However, using 1000 steps gives an answer which has (roughly) three-digit accuracy:
\begin{cline}
$ ./stiff -ts_type rk -ts_rk_type 2a -ts_adapt_type none -ts_dt 0.01
Vec Object: 1 MPI processes
  type: seq
-1.38367
-0.295656
1.03141e-301
total steps = 1000
\end{cline}
%$
By contrast, for 200 steps of Crank-Nicolson we get two-digit accuracy:
\begin{cline}
$ ./stiff -ts_type cn -ts_adapt_type none -ts_dt 0.05
Vec Object: 1 MPI processes
  type: seq
-1.383
-0.298767
1.6678e-73
total steps = 200
\end{cline}
%$
\end{example}
\noindent\hrulefill

Roughly-speaking, the problem with explicit schemes on an Example like this is that the schemes are sensitive to the magnitudes of the eigenvalues, one of which is large, even though the size of the solution component for that eigenvalue (i.e..~$e^{\lambda_{\min} t_t}$) is tiny.

As a practical matter one would use the explicit scheme with adaptive time-stepping.  However, for a stiff problem and an explicit scheme, a ``symptom'' still emerges, namely that the number of steps is insensitive to the accuracy goal (Exercise \ref{chap:ts}.\ref{exer:ts:stiffadaptive}).

Stiffness is supposed to be a property of the problem not the method.  However, there is no precise way to decide if a particular ODE system is stiff without introducing a parameter or other data; a problem may be stiff for one accuracy goal and not for others.  Indeed the word ``stiff'' is like ``sparse'' in the sense one must append ``\dots enough so that [something specific]'' if one needs to be quantitative.

In any case, comparisons of explicit and implicit methods on many stiff examples clearly shows that something other than local truncation error is important in determining the way the numerical error builds-up after many time steps.  So finally we should define our terms.


\section{Absolute stability}

Given $\lambda \in \CC$, let
\begin{equation}
y' = \lambda y, \qquad y(0)=1, \label{eq:ts:absstabtestproblem}
\end{equation}
a scalar ODE IVP, be the \emph{test equation}.\sidenote{More precisely, a \emph{parameterized family} of test \emph{problems}.}  We know that the solution is $y(t) = e^{\lambda t}$.

An ODE numerical method with step size $h$ is \emph{absolutely stable} for a given value of $\lambda$ if it generates iterates $\{Y_\ell\}$ for test equation \eqref{eq:ts:absstabtestproblem} with the property
\begin{equation}
|Y_\ell| \le |Y_{\ell-1}|. \label{eq:ts:absstab}
\end{equation}
However, the definition is not usually treated as dependent on two parameters because, for any of the one-step methods, application of the method to the test equation gives
    $$Y_\ell = f(h\lambda) Y_{\ell-1}$$
for some function $f(z)$.  Therefore we say that a numerical method is \emph{absolutely stable for a given} $z\in\CC$ if \eqref{eq:ts:absstab} applies when the scheme is applied to the test equation \eqref{eq:ts:absstabtestproblem} in a case where $h\lambda=z$ \citep{AscherPetzold1998}.

A brief analysis is appropriate.  Applying the methods so far to the test equation gives these formulas in the form $Y_\ell = f(z) Y_{\ell-1}$ (Exercise \ref{chap:ts}.\ref{exer:ts:absstabcases}):
\begin{align}
\text{Euler}              && Y_\ell &= \left(1+z\right) Y_{\ell-1}, \label{eq:ts:absstabcases} \\
\text{\RKtwoa}            && Y_\ell &= \left(1+z + \frac{1}{2} z^2\right) Y_{\ell-1}, \notag \\
\text{Crank-Nicolson}     && Y_\ell &= \left(\frac{1+\frac{1}{2} z}{1-\frac{1}{2}z}\right) Y_{\ell-1}, \notag \\
\text{backward Euler}     && Y_\ell &= \left(\frac{1}{1-z}\right) Y_{\ell-1}. \notag
\end{align}
To get absolute stability from these methods requires $|f(z)|\le 1$.  The corresponding \emph{regions of absolute stability} of the several methods are shown in Figure \ref{fig:ts:absstabregions}; the above methods are listed in order of increasing region size.

\begin{figure}
\includegraphics[width=0.8\textwidth]{figs/absstabregions}
\caption{Regions (shades of grey) of absolute stability $|f(z)|\le 1$ for the four methods in \eqref{eq:ts:absstabcases}, from Euler (darkest) to backward Euler (lightest).  Both implicit methods are absolutely stable on the left half-plane $\Im(z)\le 0$.}
\label{fig:ts:absstabregions}
\end{figure}

What does it look like when absolute stability fails to hold?  Consider the test problem with $\lambda=-1$.  With $h=0.5$ and $h=2.2$, the (explicit) Euler method gives results shown in Figure \ref{fig:ts:absstabfail}.  The exact solution is a decaying exponential, and a sufficiently-small stepsize ($h=0.5$) gives a qualitatively-correct result.  For a larger stepsize ($h=2.2$) both the Euler and the \RKtwoa rules generate a sequence of $\{Y_\ell\}$ which grows in magnitude; this is qualitatively wrong.  The fact that $z=h\lambda$ satisfies $|1+z|<1$ in one case and $|1+z|>1$ in the other illustrates the above analysis of the Euler method.

\begin{figure}
\includegraphics[width=1.0\textwidth]{figs/absstabfail}
\caption{Solutions to the test problem \eqref{eq:ts:absstabtestproblem} with $\lambda=-1$.  Large time steps $h$ cause a failure of absolute stability, and qualitatively-unreasonable results, for explicit methods like Euler and \RKtwoa.}
\label{fig:ts:absstabfail}
\end{figure}

It is natural to distinguish between the large stability regions of implicit schemes and the smaller ones of explicit schemes.  A numerical scheme is \emph{A-stable} if it is absolutely-stable for all $z$ in the left half-plane, that is, for all $z$ such that $\Im(z)\le 0$ \citep{AscherPetzold1998}.  The implicit schemes above are A-stable.\sidenote{This is not true of \emph{all} implicit Runge-Kutta methods, however.}

No explicit Runge-Kutta methods are A-stable.  In fact, when such a scheme is applied to the test equation we can write it as $Y_\ell = p(z) Y_{\ell-1}$ for a \emph{polynomial} function $p(z)$.  Because $|p(z)|\to\infty$ as $|z|\to \infty$ for any non-constant polynomial, the absolute-stability region $|p(z)|\le 1$ is bounded.

Finally, one can distinguish between the implicit methods above, namely backward Euler and trapezoid, by a stronger condition than absolute stability.  First, suppose $\gamma(t)$ is a bounded and measurable function and consider the modified test problem
\begin{equation}
y' = \lambda \left(y - \gamma(t)\right), \qquad y(0)=1. \label{eq:ts:stiffdecaytestproblem}
\end{equation}
The exact solution of this problem is asymptotic to $\gamma(t)$ as $\Re(\lambda) \to -\infty$ (Exercise \ref{chap:ts}.\ref{exer:ts:stiffdecayasymptotic}).  We say that a scheme has \emph{stiff decay} if, when it is applied to \eqref{eq:ts:stiffdecaytestproblem}, it yields approximations $\{Y_\ell\}$ with the property that for $t$ fixed, and $\ell h = t$ so that $Y_\ell \approx y(t)$,
\begin{equation}
|Y_\ell - \gamma(t)| \to 0 \qquad \text{as} \qquad h \Re(\lambda) = \Re z \to -\infty. \label{eq:ts:stiffdecaydefn}
\end{equation}
In short, a scheme has stiff decay if it can ``follow'' any nonhomogeneity $\gamma(t)$, that is, in the limit of sufficiently-negative eigenvalues (i.e.~$\Re \lambda < 0$) in the ODE.  The backward Euler has stiff decay, but the trapezoid rule does not \citep{AscherPetzold1998}.

A lower-order but stiff-decay scheme like backward Euler scheme may be chosen as a ``stable method of last resort'' in solving PDE (or PDE-like) problems which combine diffusive (heat-equation-like) character and ``rough'' data in the problem.  Diffusiveness implies that the high spatial frequencies are strongly damped, i.e.~corresponding to $\Re(\lambda)\ll 0$ above, while the roughness of the data corresponds to the need to ``follow'' a not-necessarily-smooth parameter like $\gamma(t)$ in \eqref{eq:ts:stiffdecaydefn}.  See Chapter \ref{chap:co} for an example.



\section{Jacobians for implicit methods}

As already stated, implicit methods require solving nonlinear equations \eqref{eq:ts:implicitgenericsystem} at each time step, namely
\begin{equation}
  \bF(\bY_\ell) = 0, \label{eq:ts:implicitgenericsystemAGAIN}
\end{equation}
and a proposal to use Newton's method to solve \eqref{eq:ts:implicitgenericsystemAGAIN} should be surprise; see the previous Chapter.  It is even obvious that the last value of the solution can be used as an initial iterate: $\bY_\ell^0 = \bY_{\ell-1}$.  However, Newton's method requires the Jacobian derivative of $\bF$.

For a representative example, the $\theta$ method \eqref{eq:ts:theta} corresponds to the residual function
\begin{equation}
  \bF(\bX) = \bX - \bY_{\ell-1} - (1-\theta) h \bg(t_{\ell-1},\bY_{\ell-1}) - \theta h \bg(t_\ell,\bX)  \label{eq:ts:thetasystem}
\end{equation}
wherein $\bY_{\ell-1}$ is known from the previous time step.  The Jacobian is
   $$J(\bX) = \frac{\partial \bF}{\partial \bX} = I - \theta h \frac{\partial \bg}{\partial \by}(t_\ell,\bX).$$

There is no need, however, to write Jacobian user code which is specific to the $\theta$ method (or any other ODE method).  First, the \pTS contains a \pSNES inside it.  Second, each \PETSc-implemented implicit time-stepping method in \pTS knows how to convert user-supplied code for the Jacobian derivative of the right-hand side of \eqref{eq:ts:ode}, that is, user-supplied code to compute
\begin{equation}
\frac{\partial \bg}{\partial \by} = \begin{bmatrix}\phantom{\bigg|} \frac{\partial g_i}{\partial y_j}(t,\by) \phantom{\bigg|}\end{bmatrix}, \label{eq:ts:rhsjacobian}
\end{equation}
into the necessary Jacobian of $\bF$ for Newton's method on \eqref{eq:ts:implicitgenericsystemAGAIN}.

An example \texttt{odejac.c} is shown in Codes \ref{code:ts:odejac:jacobian} and  \ref{code:ts:odejac:mat}; only the needed additions to \texttt{ode.c} are shown.  Note \texttt{odejac.c} is quite similar in structure to \texttt{ecjac.c} in Chapter \ref{chap:nl}, but with calls to \pSNES replaced by corresponding calls to \pTS.

\cinputpart{odejac.c}{\CODELOC}{To do implicit time-stepping we add a C function \texttt{FormRHSJacobian()}, which computes derivatives of the right-hand side of example ODE system \eqref{eq:ts:example}.}{I}{//JACOBIAN}{//ENDJACOBIAN}{code:ts:odejac:jacobian}

The new program calls \texttt{TSSetRHSJacobian()} to set a call-back to \texttt{FormRHSJacobian()}.  Our code sets the entries of the \pMat holding the derivative \eqref{eq:ts:rhsjacobian} of the right-hand side in Example ODE \eqref{eq:ts:example}.  An allocated \pMat, here called \texttt{J} for simplicity, is supplied to \texttt{TSSetRHSJacobian()}.  We assign \texttt{J} to both the Jacobian and preconditioner-material arguments of \texttt{TSSetRHSJacobian()}; compare usage of \texttt{SNESSetJacobian()} in Chapter \ref{chap:nl}.

\cinputpart{odejac.c}{\CODELOC}{A \pMat is created to hold $\partial \bg/\partial \by$.  The \pTS type is reset to the implicit Crank-Nicolson method.}{II}{//MATJ}{//ENDMATJ}{code:ts:odejac:mat}

Running this new program is straightforward.  We can compare the implicit (Crank-Nicolson), which is not adaptive, to the default explicit \RKthreebs method:
\begin{cline}
$ make odejac
$ ./odejac
error at tf = 20.000 with 200 steps:  |y-y_exact|_inf = 0.0151358
$ ./odejac -ts_type rk
error at tf = 20.000 with 88 steps:  |y-y_exact|_inf = 0.00930352
\end{cline}
%$
As expected, the former evaluated the Jacobian at every time step, while the latter did not:
\begin{cline}
$ ./odejac -log_view | grep SNESJacobianEval
SNESJacobianEval     200 1.0 ...
$ ./odejac -ts_type rk -log_view | grep SNESJacobianEval
$
\end{cline}
%$

If the reader has followed the above construction then it will also be clear that we could have used implicit time-stepping with the earlier, Jacobian-free program \texttt{ode.c}.  Though
\begin{cline}
$ ./ode -ts_type cn
\end{cline}
%$
gives an error message---can the reader predict it?---the addition of a finite-difference option for the underlying \pSNES does the job.  That is,
\begin{cline}
$ ./ode -ts_type cn -snes_fd
$ ./ode -ts_type cn -snes_mf
$ ./odejac
\end{cline}
%$
all give the same result,\sidenote{Very nearly; see Exercise \ref{chap:ts}.\ref{exer:ts:fdjacobian}.} and all call the \pSNES to solve a system of equations at each time step.

As a summary, Table \ref{tab:ts:odebasictypes} shows the \pTS types considered so far, including the order of the local truncation error.

\begin{table}
\small
\begin{tabular}{lllll}
\emph{method}                                & \texttt{-ts\_type} & \emph{order} & \emph{adaptive?} \\ \hline
forward Euler \eqref{eq:ts:forwardeuler}    & \texttt{euler}  & $1$ & no \\
Runge-Kutta \eqref{eq:ts:rkgeneral}         & \texttt{rk}     &     &    \\
\qquad \texttt{-ts\_rk\_type 2a}            &                 & $2$ & yes \\
\qquad \texttt{-ts\_rk\_type 3bs} \emph{[default]} &          & $3$ & yes \\
\qquad \texttt{-ts\_rk\_type 4}             &                 & $4$ & no \\
\qquad \texttt{-ts\_rk\_type 5f}            &                 & $5$ & yes \\
\qquad \texttt{-ts\_rk\_type 5dp}           &                 & $5$ & yes \\
 \hline
$\theta$ method \eqref{eq:ts:theta}         & \texttt{theta}  & $1$ or $2$ & optional \\
\qquad backward Euler \eqref{eq:ts:backwardeuler}: $\theta=1$  & \texttt{beuler} & $1$ & optional \\
\qquad Crank-Nicolson: $\theta=1/2$                     & \texttt{cn}     & $2$ & optional 
\end{tabular}
\caption{Basic ODE integration methods; \PETSc has many more.  Explicit schemes are above the line and implicit below.} \label{tab:ts:odebasictypes}
\end{table}

\medskip

As the Table suggests, the backward Euler and Crank-Nicolson methods are actually implemented as $\theta$-method special cases.  Knowing this, it is wise to ask for help:
\begin{cline}
$ ./odejac -ts_type theta -help |grep ts_theta
  -ts_theta_theta <0.5>: Location of stage (0<Theta<=1)
  -ts_theta_endpoint: <FALSE> Use the endpoint instead of midpoint form ...
  -ts_theta_adapt: <FALSE> Use time-step adaptivity with the Theta method
  -ts_theta_initial_guess_extrapolate: <FALSE> Extrapolate stage initial guess ...
\end{cline}
%$
In particular, the following are equivalent (backward Euler),
\begin{cline}
$ ./odejac -ts_type beuler
$ ./odejac -ts_type theta -ts_theta_theta 1.0
\end{cline}
and the following are equivalent (Crank-Nicolson),
\begin{cline}
$ ./odejac -ts_type cn
$ ./odejac -ts_type theta -ts_theta_theta 0.5 -ts_theta_endpoint
\end{cline}
These methods are optionally adaptive; for example:
\begin{cline}
$ ./odejac -ts_type cn -ts_theta_adapt
\end{cline}
%$
is the adaptive (though not embedded) form of Crank-Nicolson.

From now on for the rest of the book, when we consider numerical solutions to ODEs we will generally use tools suitable for the ``hard'' time-dependent case where the ODE \eqref{eq:ts:ode} is nonlinear \emph{and} where the ODE is stiff enough so that an implicit time-stepping method is automatically under consideration.  That is, we will solve problems of the form $\by' = \bg(t,\by)$ by using the \pSNES inside the \pTS object to do implicit time-stepping.

Later in this Chapter we will try a type not shown in the Table, \texttt{-ts\_type arkimex}, on a nontrivial example.  This method\sidenote{Class of methods, actually.} has properties combining the benefits of implicit and explicit methods.  Furthermore, the example will be based on ODE system form which is more general than \eqref{eq:ts:ode}.


\section{Time-dependent heat equation}

We now have the tools to return to solving \emph{partial} differential equations.  The well-known\sidenote{See \citep{HundsdorferVerwer2003,LeVeque2007,MortonMayers2005}.} theme in the remainder of this Chapter is that time-dependent PDEs generate arbitrarily-large system of ODEs when the space variables are discretized.

Consider the time-dependent heat equation in two space dimensions (2D)
\begin{equation}
\frac{\partial u}{\partial t} = D_0 \grad^2 u + f(x,y). \label{eq:ts:heatequation}
\end{equation}
We seek the temperature $u(t,x,y)$ at times $t \in [0,T]$ and points $(x,y)$ in the unit square $\mathcal{S} =(0,1)\times (0,1)$; see Figure \ref{fig:ts:heatsquare}.  For simplicity we set the \emph{diffusivity} $D_0>0$ to be constant and the \emph{heat source} $f(x,y)$ to be time-independent.

\begin{figure}
\input{tikz/heatsquare.tex}
\caption{A time-dependent heat equation initial/boundary-value problem on the unit square $\mathcal{S}$.}
\label{fig:ts:heatsquare}
\end{figure}

Before the numerics we need to specify a well-posed problem with boundary conditions, an initial condition, and a concrete source term.  As an example which is representative of real-world applications, we specify a problem \emph{without} a known\sidenote{The exact solution is \emph{knowable}, perhaps, but we do not bother.} exact solution, but in which a scalar quantity is conserved.

Somewhat arbitrarily---for the sake of variety---we have periodic boundary conditions along the top and bottom boundaries.  On the left ($x=0$) and right ($x=1$) we choose non-homogeneous and homogeneous Neumann conditions, respectively.  As a simple initial condition we choose zero temperature: $u(0,x,y)=0$.

For the source term we choose a particular formula which takes both positive and negative values,
    $$f(x,y) = 3 e^{-25(x-0.6)^2} \sin(2 \pi y).$$
For the boundary condition along the left side we set
\begin{equation}
-\frac{\partial u}{\partial x} = \gamma(y) \label{eq:ts:leftneumannbc}
\end{equation}
with $\gamma(y) = \sin(6\pi y)$.  It is easy to verify (Exercise \ref{chap:ts}.\ref{exer:ts:zerototalsource}) that the totals of these particular sources are zero:
\begin{equation}
\int_{\mathcal{S}} f(x,y)\,dx\,dy = 0, \qquad \int_0^1 \gamma(y)\,dy = 0. \label{eq:ts:zerototalsource}
\end{equation}

Physically-speaking, total thermal energy is proportional to the integral of $u$ over $\mathcal{S}$.  (For a material with density $\rho$ and heat capacity $c$ the internal energy is $\rho c$ times the integral.)  On the other hand, equation \eqref{eq:ts:heatequation} is derived from the principle of conservation of energy \citep{Ockendonetal2003}.  Given our particular choices, equation \eqref{eq:ts:heatequation} implies that the total energy is independent of time:
\begin{align}
\frac{d}{dt} \left(\int_{\mathcal{S}} u(t,x,y)\,dx\,dy\right) &= D_0 \int_0^1 \gamma(y)\,dy + \int_{\mathcal{S}} f(x,y)\,dx\,dy \label{eq:ts:conservedscalar} \\
   &= 0. \notag
\end{align}
(Again, see Exercise \ref{chap:ts}.\ref{exer:ts:zerototalsource}.)  We want our discretized equations \eqref{eq:ts:molheat} to conserve energy too, and we address this below.


\section{Method of lines}  Any time-dependent PDE like \eqref{eq:ts:heatequation} becomes a system of ODEs by \emph{semi-discretization in space}, also known as the \emph{method of lines} (MOL).  (This name imagines the continuous-time, discrete-space ODE domain as lines $(t,x_i,y_j)$ in the three-dimensional space-time.)

Suppose we discretize spatial derivatives the same way we did in Chapter \ref{chap:st}: we put a grid of $N = m_x m_y$ points $(x_i,y_j)$, with equal grid spacings $h_x,h_y$ in each direction, on $\mathcal{S}$.  (Compare Figure \ref{fig:unitsquaregrid} in Chapter \ref{chap:st} and Figure \ref{fig:ts:heatdefaultgrid} below.)  Then we replace the spatial derivatives in \eqref{eq:ts:heatequation} by centered finite differences while keeping the time derivatives.  That is, partial derivatives in $t$ become ordinary derivatives:
\begin{equation}
u_{i,j}' = D_0 \frac{u_{i+1,j} - 2 u_{i,j} + u_{i-1,j}}{h_x^2} + D_0 \frac{u_{i,j+1} - 2 u_{i,j} + u_{i,j-1}}{h_y^2} + f_{i,j}. \label{eq:ts:molheat}
\end{equation}
where $f_{i,j} = f(x_i,y_j)$ and
\begin{equation}
u_{i,j}(t) \approx u(t,x_i,y_j). \label{eq:ts:molumeaning}
\end{equation}

Equations \eqref{eq:ts:molheat} form a system of ODEs, i.e.~equation \eqref{eq:ts:ode}.  We can solve the system numerically, explicitly or implicitly, but there is no need to think about time-stepping methods now.  This deliberate ambiguity about time-stepping is central to the MOL.


\section{Implementation}

The program \texttt{c/ch5/heat.c} shown in Codes \ref{code:ts:heat:heatctx}--\ref{code:ts:heat:monitor} is easy to write based on our experiences with \pDMDA in Chapters \ref{chap:st} and \ref{chap:nl} and with \pTS in this Chapter.   The overall structure of \texttt{heat.c}, in terms of \PETSc components, is shown in Figure \ref{fig:ts:stack}.

\cinputpart{heat.c}{\CODELOC}{A context \texttt{struct} gives call-backs access to the \pDMDA and to the constant $D_0$ and functions $f$, $\gamma$.}{I}{//HEATCTX}{//ENDHEATCTX}{code:ts:heat:heatctx}

Code \ref{code:ts:heat:dmdasetup}, which is an extract of \texttt{main()}, sets up the \pDMDA object for the grid shown in Figure \ref{fig:ts:heatdefaultgrid}.  By specifying periodicity, grid points along the lower boundary $y=0$ are also (conceptually) located on the upper boundary $y=1$.  Also, ghost points are set up so that the $(x_i,y_{j\pm 1})$ neighbor of a grid point $(x_i,y_j)$ is always available.\sidenote{This applies both in serial and in parallel.}  The default grid of $m_x \times m_y = 5\times 4$ points, set by arguments to \texttt{DMDACreate2d()}, actually has square cells.  In fact, the spacings of the grid are $h_x=1/(m_x-1)$ and $h_y=1/m_y$ respectively---see the helper function \texttt{Spacings()} in \texttt{heat.c} (not shown).

\begin{figure}
% usage: \standardTSstack{scale}{Jacobian}{DMDA}
\standardTSstack{0.75}{}{}
\caption{The \PETSc ``stack'' used for time-dependent, structured-grid PDE examples \texttt{heat.c} and \texttt{pattern.c} in this Chapter.}
\label{fig:ts:stack}
\end{figure}

\cinputpart{heat.c}{\CODELOC}{Set up the \pDMDA grid object and needed \pVecs.}{II}{//DMDASETUP}{//ENDDMDASETUP}{code:ts:heat:dmdasetup}

Code \ref{code:ts:heat:dmdasetup} includes calls to \texttt{SetSource()} and \texttt{SetNeumanValues()} (not shown) to set $f,\gamma$ respectively.  A small idea here is that the non-homogeneous Neumann boundary value $\gamma(y)$ is allocated on a 2D grid even though it is only needed at points on the left boundary.  A less-wasteful design, though requiring more code, would create a separate 1D \pDMDA for this boundary (Exercise \ref{chap:ts}.\ref{exer:ts:heatneumanndmda}).

\begin{figure}
\input{tikz/heatdefaultgrid.tex}
\caption{Because of periodicity in $y$, the default $5\times 4$ point grid in \texttt{heat.c} corresponds to square cells.}
\label{fig:ts:heatdefaultgrid}
\end{figure}

\cinputpart{heat.c}{\CODELOC}{Create the \pTS.  Associate the \pDMDA, the RHS evaluation, the Jacobian-of-RHS evaluation, and the energy monitor code to it.}{III}{//TSSETUP}{//ENDTSSETUP}{code:ts:heat:tssetup}

The next part, another extract of \texttt{main()} shown in Code \ref{code:ts:heat:tssetup},\sidenote{A few lines before and after these extracts---these are not shown---read options and destroy objects, respectively.} starts by creating and configuring the \pTS.  The \pTS knows the size $N$ of the system of ODEs through an association to the \pDMDA grid object, set by calling \texttt{TSSetDM()}.  We also assign to the \pDMDA the ``local'' grid-aware functions \texttt{FormRHSFunctionLocal()} and \texttt{FormRHSJacobianLocal()}.  Also we set the default \pTS type to backward Euler, a stiff-decay method which can handle the stiff system \eqref{eq:ts:molheat}; we may and we will override this choice.  Then we set the zero initial condition and call \texttt{TSSolve()}.

\cinputpart{heat.c}{\CODELOC}{Evaluate the ODE RHS $\bg(t,\by)$ for ODE system \eqref{eq:ts:molheat}.}{II}{//RHSFUNCTION}{//ENDRHSFUNCTION}{code:ts:heat:rhsfunction}

\texttt{FormRHSFunctionLocal()} in Code \ref{code:ts:heat:rhsfunction} evaluates the RHS $\bg(t,\by)$ of ODE system \eqref{eq:ts:molheat}.  \texttt{FormRHSJacobianLocal()} in Code \ref{code:ts:heat:rhsjacobian} evaluates its derivatives (Jacobian).  These functions are where the finite difference formula \eqref{eq:ts:molheat} is applied.  Application at most points is straightforward; it is the same 5-point stencil shown in Figure \ref{fig:unitsquaregridstencil}.

A Neumann condition is needed at the left and right boundaries, however.  For example, at point $(x_0,y_j)=(0,y_j)$ we have \eqref{eq:ts:leftneumannbc}, which becomes
     $$- \frac{u_{+1,j} - u_{-1,j}}{2 h_x} = \gamma(y_j),$$
using a $O(h^2)$ centered finite difference approximation and a notional value ``$u_{-1,j}$'' \citep{MortonMayers2005}.  Writing this as a formula for $u_{-1,j}$, namely $u_{-1,j} = u_{+1,j} + 2 h_x \gamma(y_j)$, it replaces the notional value to give a meaningful version of \eqref{eq:ts:molheat} at the left boundary.  A simpler, homogeneous version of this technique applies at the right boundary.

Note that, in parallel, only certain MPI processes have access to the $i=0$ and $i=m_x-1$ parts of the grid.  This is why access to boundary values in array \texttt{au} must be protected by conditionals using information from the \texttt{DMDALocalInfo} object; recall Figure \ref{fig:localpartofgrid}.

\cinputpart{heat.c}{\CODELOC}{Evaluate the Jacobian of \eqref{eq:ts:molheat}.}{III}{//RHSJACOBIAN}{//ENDRHSJACOBIAN}{code:ts:heat:rhsjacobian}

A feature of this heat equation example is an energy ``monitor'' call-back which reports the total energy in the discretized system.  Code \ref{code:ts:heat:tssetup} shows that we (optionally) use \texttt{TSMonitorSet()} to set a call-back to our function \texttt{EnergyMonitor()}, itself shown in Code \ref{code:ts:heat:monitor}.  The trapezoid rule\sidenote{This quadrature can also be interpreted as the midpoint rule, in a grid-point-centered cell interpretation, though with modifications at the boundaries.} is used to do the integral $\int_{\mathcal{S}} u\,dx\,dy$.  Note that \texttt{EnergyMonitor()} uses a ``raw'' MPI call; we put each rank's contribution into a local variable \texttt{lenergy} and then use \texttt{MPI\_Allreduce()} to get \texttt{energy} which is the integral over the whole grid.  Also, a monitor set with \texttt{TSMonitorSet()} is \emph{additional} to the default monitor from option \texttt{-ts\_monitor}.

\cinputpart{heat.c}{\CODELOC}{Compute and then print the total thermal energy at the current time.}{V}{//MONITOR}{//ENDMONITOR}{code:ts:heat:monitor}


\section{Running \texttt{heat.c}, and visualization}

A helpful first run, which reminds us of the \pTS/\pSNES/\pKSP/\pPC/etc.~stack shown in Figure \ref{fig:ts:stack}, is
\begin{cline}
$ ./heat -ts_view
\end{cline}
%$
The full page of output is not shown.

A view of the time-stepping action, including convergence of the discrete equations \eqref{eq:ts:implicitgenericsystemAGAIN} at each time step, for the backward Euler method in this case, and the output of the energy monitor, comes from
\begin{cline}
$ ./heat -ts_monitor -heat_monitor -snes_converged_reason
solving on 5 x 4 grid with dx=0.25 x dy=0.25 cells, t0=0.,
and initial step dt=0.01 (so D0 dt / (dx dy) = 0.16) ...
  energy = 0.00e+00
0 TS dt 0.01 time 0.
  Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2
  energy = -6.43e-14
1 TS dt 0.01 time 0.01
  Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2
  energy = -1.02e-13
...
9 TS dt 0.01 time 0.09
  Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2
  energy = -1.62e-13
10 TS dt 0.01 time 0.1
\end{cline}
%$

Because the initial thermal energy is zero we should see ``\texttt{energy = 0.00e+00}'' at every time step.  That is what we \emph{would} see if there were no rounding error or implementation mistakes.  Noting the coarseness of the grid, so that errors of size comparable to truncation error would be many orders of magnitude larger than seen here, the above actual result shows energy is conserved to within rounding error.

We also see two Newton iterations per time step even though equations \eqref{eq:ts:implicitgenericsystemAGAIN} are linear.\sidenote{Because \eqref{eq:ts:heatequation} is a linear PDE, of course.}  This occurs because the \pKSP iteration halts at an insufficiently-tight tolerance, based on the \emph{preconditioned} residual of the linear system, relative to the desired \pSNES tolerance.  We can resolve this, if needed, by tightening the tolerance for the Krylov iteration or even asking for a direct solve:
\begin{cline}
$ ./heat -ts_monitor -heat_monitor -snes_converged_reason -ksp_rtol 1.0e-12
$ ./heat -ts_monitor -heat_monitor -snes_converged_reason -ksp_type preonly -pc_type lu
\end{cline}
Both techniques result in one \pSNES iteration per time step, but the only the former acts the same in serial and parallel.

Because we use \pDMDA, the structured grid on the domain $\mathcal{S}$ is decomposed in parallel.\sidenote{We do not consider ``parallel-in-time'' decompositions in this book.}  One can show the parallel decomposition at run-time either as text or graphically, for example:
\begin{cline}
$ mpiexec -n 4 ./heat -da_refine 1 -dm_view
$ mpiexec -n 4 ./heat -da_refine 1 -dm_view draw -draw_pause 1
\end{cline}
Furthermore it is good to remember, and to check using \texttt{-ts\_view}, that these runs use a nontrivial (default) stack of parallel, iterative tools at each time step: Newton's method with line-search (\texttt{-snes\_type newtonls}), the GMRES iteration (\texttt{-ksp\_type gmres}), block Jacobi as the parallel preconditioner (\texttt{-pc\_type bjacobi}), and ILU as the preconditioner on each rank's block (\texttt{-sub\_pc\_type ilu}).

A run-time graphical (X windows) view of the evolving solution, a movie, comes from:
\begin{cline}
$ ./heat -da_refine 5 -ts_final_time 0.02 -ts_dt 0.001 -ts_monitor \
    -ts_monitor_solution draw
\end{cline}
%$
Figure \ref{fig:ts:heatfinaltime} shows a grayscale version of the last frame of this movie.  The variation in temperature at the left side is from the non-homogeneous boundary condition $\gamma(y)$, while in the interior the variation is caused by the heat source $f(x,y)$.
  
\begin{figure}
\includegraphics[width=0.7\textwidth]{figs/heatfinaltime}
\caption{Temperature as grayscale in the final frame from a \texttt{heat.c} run.}
\label{fig:ts:heatfinaltime}
\end{figure}

For off-line viewing, a Python script \texttt{plotTS.py} in directory \texttt{c/ch5/} can be used to generate either a trajectory like Figure \ref{fig:ts:ode} or a sequence of frames like Figure \ref{fig:ts:heatfinaltime}.  Any format supported by the Python library Matplotlib (\href{http://matplotlib.org/}{\texttt{matplotlib.org}}) is allowed, but \texttt{.png} is the default.  One first generates \PETSc binary file output, from a run with a given resolution $m_x \times m_y$, and then the sequence of images from that:
\begin{cline}
$ ./heat -da_refine 5 -ts_monitor binary:t.dat -ts_monitor_solution binary:u.dat
$ ./plotTS.py -mx 129 -my 128 t.dat u.dat -oroot bar
\end{cline}
Files \texttt{bar000.png} \dots \texttt{bar010.png} are generated in this case.  Note that \texttt{plotTS.py} needs access to (e.g.~POSIX symbolic links to) \texttt{PetscBinaryIO.py} and \texttt{petsc\_conf.py} from \texttt{\$PETSC\_DIR/bin/}.  The documentation \texttt{MOVIES.md}, in the same directory, also describes how to generate a movie in \texttt{.m4v} format.


\section{Time-stepping performance on the heat equation}

We can apply our knowledge of the relative properties of schemes, from earlier in this Chapter, to this heat equation example.  To summarize the example, MOL converts the heat equation \eqref{eq:ts:heatequation} into a large, stiff, and linear system of ODEs in form \eqref{eq:ts:ode}.  If we choose an implicit time-stepping method then \PETSc allows us to solve equations \eqref{eq:ts:implicitgenericsystemAGAIN} at each time step in parallel using the stack of tools shown in Figure \ref{fig:ts:stack}.  The code \texttt{heat.c} sets the backward Euler method as the default method (\texttt{-ts\_type beuler}).

Here is a run on which we can compare various solver choices:\sidenote{For issues other than timeing, consider adding options \texttt{ -snes\_converged\_reason -ts\_monitor\_solution draw} to monitor convergence and visualize.  Some effects below are seen in the visualizations.}
\begin{cline}
$ ./heat -da_refine 5 -ts_monitor -heat_monitor
\end{cline}
%$
This run, which takes $M=10$ steps to reach the final time $t_f=0.1$ using an $O(\Delta t^1)$ method, gives reasonable results in a few seconds,  % 4.11 seconds
on a modestly-refined grid of $N \approx 1.6 \times 10^4$ points, using the default settings.

Recalling Table \ref{tab:ts:odebasictypes}, we now consider other ODE methods.  These cases, which add the options shown to the run above, are compared by number of steps $M$ and---this is very informal---by their relative wall-clock time $R$:\sidenote{Thus $R=1$ for a method which is the same speed as backward Euler, $R<1$ for a faster method, and $R>1$ for a slower method.}
\renewcommand{\labelenumi}{\alph{enumi})}
\begin{enumerate}
\item \texttt{-ts\_type beuler}  \hfill $R=1$, $M=10$
\begin{quote} The implicit, A-stable, stiff-decay, and $O(\Delta^1)$ local truncation error method which is the default for \texttt{heat.c}. \end{quote}
\item \texttt{-ts\_type rk}  \hfill $R=4.5$, $M=5201$
\begin{quote} The \RKthreebs adaptive method.  It detects the stiffness as a difference between the embedded schemes, and thus takes many inappropriately-short time steps and too much time.  (The effect gets worse, relative to a), on further-refined grids.) \end{quote}
\item \texttt{-ts\_type rk -ts\_adapt\_type none}  \hfill $R=0.012$, $M=10$
\begin{quote} The same method as b), but sans adaptivity.  The numerical solution explodes to size $10^{72}$ after $M=10$ steps!  (Fast is irrelevant.)\end{quote}
\item \texttt{-ts\_type cn}  \hfill $R=0.84$, $M=10$
\begin{quote} Crank-Nicolson is the $\theta=1/2$ method; it has $O(\Delta t^2)$ local truncation error.  Visualization shows a problem.  Instead of smoothly growing in size as with \texttt{beuler}, features oscillate unstably.  This is a well-known consequence of a method which is A-stable but not stiff-decay.  (The oscillations do eventually decay, unlike c).) \end{quote}
\item \texttt{-ts\_type theta -ts\_theta\_theta 0.7 -ts\_theta\_endpoint}

\hfill $R=0.91$, $M=10$
\begin{quote} By moving the $\theta$ value toward the more-stable $\theta=1$ (i.e.~\texttt{beuler}) end, we tame the instability of Crank-Nicolson.  But now the order is back to $O(\Delta t^1)$ like run a). \end{quote}
\item \texttt{-ts\_type cn -ts\_theta\_adapt}  \hfill $R=2.3$, $M=31$
\begin{quote} By adding adaptivity to Crank-Nicolson we also control the oscillations, and now we have $O(\Delta t^2)$ local truncation error as well.  The trade-off between speed and accuracy can be controlled by options \texttt{-ts\_rtol} and \texttt{-ts\_atol} (not shown).  \end{quote}
\item \texttt{-ts\_type bdf -ts\_bdf\_adapt -ts\_bdf\_order 2}

\hfill $R=2.4$, $M=28$
\begin{quote} This implicit, A-stable, and stiff-decay multistep method \citep{AscherPetzold1998} uses backward differentiation formula \eqref{eq:ts:bdf2}, but with additional tweaks to make it adaptive.  As a multi-step formula it requires additional techniques to get started (not addressed).\end{quote}
\end{enumerate}

For a heat or diffusion equation problem like the one under consideration, we may conclude that runs a), f), and g) represent a selection of good things to try.

Note that when we run \texttt{heat.c} it prints out the value of $\nu = D_0 \Delta t / (h_x h_y)$.\sidenote{The \emph{initial} $\Delta t$ value is used, so in adaptive cases this report is of minimal utility.}  This ratio controls the conditional stability of explicit methods,\sidenote{Note that $\nu$ is relevant to diffusions, and it is not the better-known ``CFL'' constant of hyperbolic problems \citep{LeVeque2002} which possess characteristic curves.  Of course the concepts are related.} and the degree of stiffness of the ODE problem.  Indeed, theorems showing that $\nu \lesssim 1$ is either necessary or sufficient, or both, for the conditional stability of fully-discrete explicit methods for the heat equation can be found in \citep{MortonMayers2005}.  (See \citep{LeVeque2007} for the connection to stiffness and the absolute stability of methods.)  Our treatment, based on MOL, will not pursue such essential numerical PDE theory further.


\section{Coupled reaction-diffusion equations}

The next example also uses a MOL discretization, a 2D \pDMDA, and centered finite differences to produce a stiff ODE system.  However, it is more interesting in three particular ways:
\begin{itemize}
\item It is a \emph{system} of two coupled, scalar PDEs.
\item They are \emph{nonlinear} reaction-diffusion equations.
\item We \emph{separate the stiff part} so that implicit/explicit (\emph{IMEX}) numerical methods are effective.
\end{itemize}

This reaction-diffusion system was studied by \citep{Pearson1993}:
\begin{align}
\frac{\partial u}{\partial t} &= D_u \grad^2 u - u v^2 + \phi (1 - u), \label{eq:ts:pattern:pde} \\
\frac{\partial v}{\partial t} &= D_v \grad^2 u + u v^2 - (\phi + \kappa) v. \notag
\end{align}
Here $u(t,x,y)$ and $v(t,x,y)$ are concentrations of two different chemicals.  Both chemicals diffuse in space but the \emph{diffusion coefficients} $D_u>0,D_v>0$ are generally different.  A chemical interaction between the two converts $u$ into $v$ and occurs at a rate $u v^2$; this term makes the system nonlinear.  Constant $\phi>0$ describes the external supply (``feed rate'') of $u$, while $\kappa>0$ gives the rate of an additional chemical reaction that eliminates $v$ by converting it into an inert product \citep{Pearson1993}.

We consider an initial/boundary value problem for \eqref{eq:ts:pattern:pde} \citep{HundsdorferVerwer2003} in which $u,v$ are defined for $t\in[0,T]$ and for $(x,y)$ in the square domain $\mathcal{S} = [0,L)\times [0,L)$.  An initial condition is chosen, optionally with noise added to break symmetries, to generate ``spot multiplication''; see \citep{Pearson1993}.  The boundary conditions on $\mathcal{S}$ are periodic, so in effect there is no boundary.

This reaction-diffusion model produces a ``surprising variety of irregular spatiotemporal patterns'' \citep{Pearson1993}, but of course one needs a computer to reveal them.  While Pearson used explicit, first-order forward-Euler to do the numerical experiments, which required a parallel supercomputer,\sidenote{Namely, a Connection Machine at the Los Alamos Advanced Computing Laboratory.} we will take implicit (or partly-implicit) time steps $1000$ times longer.  This problem illustrates the value of implicit time-stepping in the presence of stiff diffusive terms.

As in the heat equation, the Laplacian $\grad^2$ in \eqref{eq:ts:pattern:pde} generates a stiff ODE system when MOL is applied \citep{HundsdorferVerwer2003}.  With such situations in mind, \PETSc allows us to write the discretized system in the form
\begin{equation}
\bF(t,\by,\by') = \bG(t,\by), \label{eq:ts:pattern:imexform}
\end{equation}
and then put the stiff terms in $\bF$ and the nonstiff in $\bG$, so that methods can apply different schemes to each part.  This general form is also suitable for \emph{differential-algebraic equations} (DAEs) wherein $\partial \bF/\partial \by'$ is singular.  Note that form \eqref{eq:ts:ode} is the case of \eqref{eq:ts:pattern:imexform} in which $\partial \bF/\partial \by'$ is the identity.  If $\partial \bF/\partial \by'$ is invertible then in theory \eqref{eq:ts:pattern:imexform} can be converted to \eqref{eq:ts:ode}, but this is often impractical.

In the particular case of the MOL discretization of \eqref{eq:ts:pattern:pde}, either form \eqref{eq:ts:ode} or \eqref{eq:ts:pattern:imexform} can be made to work, but \eqref{eq:ts:pattern:imexform} increases options for effective time-stepping methods.  See Exercise \ref{chap:ts}.\ref{exer:ts:patternrhsonlyform}.

\cinputpart{pattern.c}{\CODELOC}{\texttt{Field} is for point-wise values of the solution.  \texttt{PatternCtx} is a ``context'' for use by call-backs.}{I}{//FIELDCTX}{//ENDFIELDCTX}{code:ts:pattern:fieldctx}

To get started with the implementation \texttt{c/ch5/pattern.c}, Code \ref{code:ts:pattern:fieldctx} shows the definition of \texttt{struct}s named \texttt{Field} and \texttt{PatternCtx}.  The former is the point-wise value of the solution of PDE \eqref{eq:ts:pattern:pde}, that is, a pair $(u,v)$.  The latter is the standard ``context'' struct so that call-backs can get the values of the constants in \eqref{eq:ts:pattern:pde}.

\cinputpart{pattern.c}{\CODELOC}{Create a 2D \pDMDA which has two degrees of freedom and a ``box'' stencil.}{II}{//DMDACREATE}{//ENDDMDACREATE}{code:ts:pattern:dmdacreate}

Next we display two extracts of \texttt{main()}.  First, Code \ref{code:ts:pattern:dmdacreate} shows how a 2D \pDMDA with two degrees of freedom (i.e.~$(u,v)$ at each point) is created.  We use a ``box'' stencil because of the 9-point finite difference stencil used for $\grad^2$ (below).

\cinputpart{pattern.c}{\CODELOC}{Set up the \pTS and its callbacks.}{III}{//TSSETUP}{//ENDTSSETUP}{code:ts:pattern:tssetup}

Second, Code \ref{code:ts:pattern:tssetup} shows the set-up of the \pTS object.  This part differs from the \texttt{heat.c} case because we set three different call-backs using the \PETSc API:
\begin{itemize}
\item \texttt{DMDATSSetIFunctionLocal()} and \texttt{DMDATSSetIJacobianLocal()} set call-backs for $\bF$ in \eqref{eq:ts:pattern:imexform}, and
\item \texttt{DMDATSSetRHSFunctionLocal()} sets a call-back for $\bG$ in \eqref{eq:ts:pattern:imexform}.
\end{itemize}
The call-back functions themselves are shown below.  Note that \texttt{DMDATSSetRHSJacobianLocal()} exists but we do not use it; compare Exercises \ref{chap:ts}.\ref{exer:ts:patternfulljacobian} and \ref{chap:ts}.\ref{exer:ts:patternrhsonlyform}.

We set the ARKIMEX = \emph{adaptive Runge-Kutta implicit/explicit} \citep{AscherRuuthSpiteri1997} as the time-stepping method.  This method will treat $\bF$ in \eqref{eq:ts:pattern:imexform} implicitly and $\bG$ explicitly.  However, other non-split implicit methods work quite well on this example, including Crank-Nicolson and BDF (Exercise \ref{chap:ts}.\ref{exer:ts:patternperformancestudy}).

Now consider the spatial discretization (MOL) of \eqref{eq:ts:pattern:pde}.  Speaking conceptually, MOL results in a vector $\bu$ of the discrete values $u_{i,j}(t) \approx u(t,x_i,y_j)$ and a similar vector $\bv$.  Together these form the state vector of the ODE system \eqref{eq:ts:pattern:imexform}, 
\begin{equation}
\by = \begin{bmatrix} \bu \\ \bv \end{bmatrix}.  \label{eq:ts:pattern:blockvars}
\end{equation}
Supposing \texttt{Field **aY} is an array for a \pVec holding the gridded approximation to $\by$, we can reference the components by their names, as \text{aY[j][i].u} and \text{aY[j][i].v}, respectively.

\cinputpart{pattern.c}{\CODELOC}{Evaluate $\bF$ in \eqref{eq:ts:pattern:imexform}.}{IV}{//IFUNCTION}{//ENDIFUNCTION}{code:ts:pattern:ifunction}

To implement the discretized form we write \texttt{FormIFunctionLocal()} which computes
\begin{equation}
\bF(t,\by,\by') = \by' - \begin{bmatrix} D_u A \bu \\ D_v A \bv \end{bmatrix},  \label{eq:ts:pattern:blockifunction}
\end{equation}
where $A$ is a centered finite-difference approximation of the scalar Laplacian $\grad^2$.  We also write \texttt{FormRHSFunctionLocal()} which computes
\begin{equation}
\bG(t,\by) = \begin{bmatrix}
- \bu \bv^2 + \phi (1-\bu) \\
+ \bu \bv^2 - (\phi + \kappa) \bv
\end{bmatrix}. \label{eq:ts:pattern:blockrhsfunction}
\end{equation}
These functions appear in Codes \ref{code:ts:pattern:ifunction} and \ref{code:ts:pattern:rhsfunction}, respectively.

\cinputpart{pattern.c}{\CODELOC}{Evaluate $\bG$ in \eqref{eq:ts:pattern:imexform}.}{V}{//RHSFUNCTION}{//ENDRHSFUNCTION}{code:ts:pattern:rhsfunction}

The ordering of unknowns implied in \eqref{eq:ts:pattern:blockvars}--\eqref{eq:ts:pattern:blockrhsfunction} is, in fact, only notional.  The actual order of the grid values in memory is
\begin{equation}
\by = \begin{bmatrix} u_{0,0} \\ v_{0,0} \\ u_{1,0} \\ v_{1,0} \\ \vdots \end{bmatrix}. \label{eq:ts:pattern:interleaved}
\end{equation}
That is, the components $u,v$ are interleaved.\sidenote{The particular $i,j$ grid order comes from the \pDMDA; see Chapter \ref{chap:st}.}  However, we do \emph{not} need to be aware of the order in memory to write the code because \texttt{DMDAVecGetArray()} and \texttt{MatSetValuesStencil()} allow us to refer to values of $u$ and $v$ by component name (for \pVecs, in Codes \ref{code:ts:pattern:ifunction}--\ref{code:ts:pattern:rhsfunction}) or component index (using \texttt{c=0,1} for \pMat entries in Code \ref{code:ts:pattern:ijacobian}).

\cinputpart{pattern.c}{\CODELOC}{Compute the derivative \eqref{eq:ts:pattern:jacobianshift} of $\bF$.}{VI}{//IJACOBIAN}{//ENDIJACOBIAN}{code:ts:pattern:ijacobian}

As recommended for reaction-diffusion models \citep[section III.6]{HundsdorferVerwer2003}, we use a $9$-point stencil for the Laplacian $\grad^2$.  The $5$-point stencil used in \texttt{heat.c}, and in Chapter \ref{chap:st} as well, corresponds to adding the following stencil submatrix for each grid point:
\begin{equation}
A^{[5]} = \frac{1}{h^2} \begin{bmatrix}
	 & 1 & \\
	1 & -4 & 1 \\
	  &  1 & 
	\end{bmatrix}.  \label{eq:ts:pattern:fivepoint}
\end{equation}
Note the ``star'' character to the nonzero entries.  In \texttt{pattern.c} we use
\begin{equation}
A^{[9]} = \frac{1}{6h^2} \begin{bmatrix}
	1 &   4 & 1\\
	4 & -20 & 4 \\
	1 &   4 & 1
	\end{bmatrix}.  \label{eq:ts:pattern:ninepoint}
\end{equation}
Note the ``box'' character.  Both of these approximations of the Laplacian have $O(h^2)$ local trunction error, but the latter is better at isotropic diffusion.  See Exercise \ref{chap:ts}.\ref{exer:ts:barkley}.

We write Jacobian code only for the stiff part $\bF$, with the idea that only that part needs to be treated implicitly for stability.  However, examples like \texttt{pattern.c}, which work with general ODE form \eqref{eq:ts:pattern:imexform}, require that the Jacobian function (\texttt{FormIJacobianLocal()} in Code \ref{code:ts:pattern:ijacobian}) computes a shifted combination of the derivatives of $\bF$:
\begin{equation}
J = (\text{\texttt{shift}}) \frac{\partial \bF}{\partial \by'} + \frac{\partial \bF}{\partial \by}.
\label{eq:ts:pattern:jacobianshift}
\end{equation}
In our case $\partial \bF/\partial \by'$ is the identity so $J$ equals $\partial \bF/\partial \by$ plus a constant along the diagonal.

Since we have now given a sketch of the whole code, it is time to try it our.  FIXME first show
\begin{cline}
-ts_type beuler -snes_monitor
-ts_type beuler -snes_monitor -snes_mf_operator
\end{cline}
In both cases there is one nonlinear solve per step.  Quadratic convergence is not seen on in the first case because we don't have full Jacobian.  The second recovers it.

FIXME now with ARKIMEX This run takes about 10 seconds and does something worth visualizing
\begin{cline}
$ ./pattern -ts_monitor -da_refine 5
\end{cline}
%$
FIXME unline BEULER runs above, here three \pSNES solves per step, but each is actually linear because the $\bF$ is linear
\begin{cline}
-snes_monitor -ksp_rtol 1.0e-10
\end{cline}

See Exercise \ref{chap:ts}.\ref{exer:ts:patternperformancestudy} for more on the relative performance of stiff solvers on this problem.

\begin{figure}
\includegraphics[width=0.45\textwidth]{figs/pattern000} \quad
\includegraphics[width=0.45\textwidth]{figs/pattern350}

\medskip \medskip
\includegraphics[width=0.45\textwidth]{figs/pattern510} \quad
\includegraphics[width=0.45\textwidth]{figs/pattern650}
% use run:
%    
\caption{Component $u$ as grayscale in frames at $t=0,350$ (top) and $t=510,650$ (bottom) from \texttt{pattern.c} run on a $256\times 256$ grid.  Compare Figure 4 in \citep{Pearson1993}.}
\label{fig:ts:pattern:figureversuspearson}
\end{figure}

FIXME To generate Figure \ref{fig:ts:pattern:figureversuspearson} we use
\begin{cline}
$ mpiexec -n 4 ./pattern -ts_monitor -da_refine 6 -ts_dt 10 -ts_final_time 650 \
   -ts_type cn -snes_mf_operator -snes_monitor
\end{cline}
%$
About one minute on a lap-top, and we see good quadratic convergence.  That is, $\Delta t=10.0$ and $65$ total time steps on a second-order method, while \citep{Pearson1993} used first-order forward Euler explicit time-stepping, $\Delta t = 0.01$, and $65000$ time steps.

\begin{figure}
\includegraphics[width=0.7\textwidth]{figs/snakes}
\caption{FIXME}
\label{fig:ts:pattern:snakes}
\end{figure}

FIXME add optional noisy initial condition; generate Figure \ref{fig:ts:pattern:snakes} using options
\begin{cline}
$ mpiexec -n 4 ./pattern -da_refine 6 -ptn_phi 0.05 -ptn_kappa 0.063 \
    -ts_final_time 15000 -ptn_noisy_init 0.15 \
    -ts_monitor binary:t.dat -ts_monitor_solution binary:uv.dat
\end{cline}
%$



\section{Exercises}

\renewcommand{\labelenumi}{\arabic{chapter}.\arabic{enumi}\quad}
\renewcommand{\labelenumii}{(\alph{enumii})}
\begin{enumerate}
\item \label{exer:ts:tan}  Consider the scalar ODE initial value problem $y'=1+y^2$, $y(0)=0$.  Show by-hand that $y(t)=\tan t$ is the (unique) solution.  Modify \texttt{ode.c} to solve this problem.  Run the code from $t_0=0$ to $t_f=2$.  What run-time evidence shows that your estimate of ``$y(2)$'' is totally meaningless?

\item \label{exer:ts:odepossible}  Which \pTS types work with \texttt{ode.c}, that is, do not give run-time errors?  Do \texttt{-help |grep ts\_type} to find possibilities, and try them.  Which ones work with the additional option \texttt{-snes\_fd}?  Which ones work with \texttt{odejac.c}?  Explain as much as you can.

\item \label{exer:ts:odeserial}  Code \texttt{ode.c} claims it is ``serial only.''  What happens with \texttt{mpiexec -n 2 ./ode}, and why?

\item \label{exer:ts:stiffexample}  Show by hand that the eigenvalues of matrix $B$ in \eqref{eq:ts:stiffexamplematrix} are $\lambda=i,-i,-101$.  Use a full eigen-decomposition $B = X \Lambda X^{-1}$, which can be computed by hand, to confirm \eqref{eq:ts:stiffsoln}, noting that
    $$e^{Bt} = X e^{\Lambda t} X^{-1} = X \begin{bmatrix} e^{\lambda_0 t} &  &  \\
                       & \ddots &  \\
                       & & e^{\lambda_{N-1} t} \end{bmatrix} X^{-1}.$$

\item \label{exer:ts:stiffcode}  Modify \texttt{odejac.c} to a similar code \texttt{stiff.c} which solves ODE IVP \eqref{eq:ts:stiffexample}.  It can use \texttt{VecView()} to print the computed solution and \texttt{TSGetTotalSteps()} for the number of steps.  Confirm the results shown in the text for \texttt{-ts\_type rk -ts\_rk\_type 2a} and for \texttt{-ts\_type cn}.

\item \label{exer:ts:stiffadaptive}  Continuing the above Exercise, use the default RK scheme RK3bs with adaptivity turned on, which is the default, but with an explicit accuracy goal set using options \texttt{-ts\_rtol}, \texttt{-ts\_atol}.  For example, run the following Bash loop:
\begin{code}
for POW in 2 3 4 5 6 7 8 9 10; do
    ./stiff -ts_type rk -ts_rtol 1.0e-$POW -ts_atol 1.0e-$POW
done
\end{code}
Conclude that for the Example problem on page \pageref{ex:ts:odestiff}, about 400 steps are necessary to get any accuracy at all (i.e.~at least one digit) using method RK3bs.

\item \label{exer:ts:absstabcases}  Confirm formulas \eqref{eq:ts:absstabcases}.  Sketch the regions of absolute stability shown in Figure \ref{fig:ts:absstabregions} and locate the $z$-values corresponding to the runs shown in Figure \ref{fig:ts:absstabfail}.

\item \label{exer:ts:stiffdecayasymptotic}  Show that \eqref{eq:ts:stiffdecaytestproblem} has solution
    $$y(t) = e^{\lambda t} - \lambda \int_0^t e^{\lambda (t-s)} \gamma(s)\,ds.$$
Then show that the family of functions $D_\lambda(x) = -\lambda e^{\lambda x}$ form a Dirac delta function \citep{Evans2010} in the sense that, for any continuous function $\varphi(x)$, $\int_{0}^\infty D_\lambda(x) \varphi(x)\,dx = \varphi(0)$ as $\Re\lambda \to -\infty$.  Conclude that the solution to \eqref{eq:ts:stiffdecaytestproblem} is asymptotic to $\gamma(t)$ as $\Re\lambda \to -\infty$.

\item \label{exer:ts:fdjacobian}   Adding option \texttt{-snes\_monitor} so a to look deeper, the results of these three runs
\begin{cline}
$ ./ode -ts_type cn -snes_fd -snes_monitor
$ ./ode -ts_type cn -snes_mf -snes_monitor
$ ./odejac -snes_monitor
\end{cline}
%$
differ in the size of residual norms and even in the number of Newton iterations.  Explain.

\item \label{exer:ts:zerototalsource}  Show \eqref{eq:ts:zerototalsource}.  Then fill in the details to show \eqref{eq:ts:conservedscalar}.  Observe that the periodic boundary condition amounts to treating the top/bottom boundary locations as interior points of a (topologically) cylindrical surface.

\item \label{exer:ts:heatneumanndmda} Write a new version of \texttt{heat.c} which creates a \texttt{DMDACreate1d()} and uses it to allocate and access the \pVec \texttt{gamma}.  The new code will not waste memory in a 2D array for this purpose, as does \texttt{heat.c}.

\item \label{exer:ts:patternfulljacobian}  Modify \texttt{pattern.c} by \emph{adding} \texttt{FormRHSJacobianLocal()} so that the full Jacobian is available; make no other changes.  Compare timing of the run that produced Figure \ref{fig:ts:pattern:figureversuspearson}, but with and without option \texttt{-snes\_mf\_operator}.

\item \label{exer:ts:patternrhsonlyform}  Modify \texttt{pattern.c} by \emph{converting to form} \eqref{eq:ts:ode}; make no other changes.  Thus \texttt{FormIFunctionLocal()} and \texttt{FormIJacobianLocal()} will be removed, while the new \texttt{FormRHSJacobianLocal()} will only compute the $\grad^2$ parts of the (new) function $\bg(t,\by)$.  Compare performance and demonstrate, in particular, that \texttt{-ts\_type arkimex} is no longer effective.

\item \label{exer:ts:barkley}  Consider this pair of reaction-diffusion equations from \citep{Barkley1995}:
\begin{align}
\frac{\partial u}{\partial t} &= \grad^2 u + \frac{1}{\eps} u (1-u) \left(u - \frac{1}{\alpha} (v + \beta)\right), \label{eq:ts:pattern:barkleypde} \\
\frac{\partial v}{\partial t} &= \delta \grad^2 v + u - v \notag
\end{align}
The spatial domain is $[0,80]\times[0,80]$, homogeneous Neumann boundary conditions apply on the whole boundary, and $t>0$.  The suggested parameter values in \citep[section III.6]{HundsdorferVerwer2003} are $\alpha=0.25,\beta=0.001,\delta=0,\eps=0.002$ and the initial conditions are
    $$u(0,x,y) = \begin{cases} 0, & x < 40, \\ 1, & x \ge 40 \end{cases}, \qquad
      v(0,x,y) = \begin{cases} 0, & y < 40, \\ \frac{1}{2}\alpha, & y \ge 40.\end{cases}$$
Hundsdorfer and Verwer show that there is a significant difference between using the $5$-point stencil \eqref{eq:ts:pattern:fivepoint} and the $9$-point stencil \eqref{eq:ts:pattern:ninepoint}, in the sense that the natural spiraling patterns generated by this problem become cartesian-grid-aligned.  Reproduce this result by modifying \texttt{pattern.c} into \texttt{barkley.c} which uses the new equations and which allows the user to choose between the $5$-point and $9$-point stencils at run-time.

\item \label{exer:ts:patternperformancestudy}  Consider a fixed run like the one which produced Figure \ref{fig:ts:pattern:figureversuspearson}.  Which is fastest among the following IMEX, BDF, and Crank-Nicolson runs, all of which are adaptive and at least second order?:
\begin{code}
-ts_type arkimex -ts_arkimex_type [a2|l2|ars122|2c|2d|2e|3|4|5]
-ts_type bdf -ts_bdf_adapt -ts_bdf_order [2|3|4|5|6]
-ts_type cn -ts_theta_adapt
\end{code}

\end{enumerate}

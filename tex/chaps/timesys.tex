
To the reader who has become lost in a maze of issues which have built up around the examples so far---Krylov solvers, preconditioner choices, line searches, finite element details, or whatever causes confusion---some good news.  In \emph{this} Chapter we restart with an easy question and a small example.

Solving ordinary differential equations (ODEs) with \PETSc ought to be easy.  Given that we have practice with \PETSc objects, it is.  Furthermore, the transition to numerical methods for time-dependent, parabolic PDEs like the heat equation is straightforward.

In this Chapter we look at three examples:
\begin{itemize}
\item a linear system of two ODEs,
\item an arbitrarily-large system of ODEs arising from spatial finite-difference approximation of the 2D time-dependent heat PDE,
\item and another large ODE system arising from two coupled nonlinear PDEs in 2D, forming a two-species reaction/diffusion model.
\end{itemize}
These problems are solved using a new \PETSc object, the \pTS time-stepping solver.

The particular integration method used by the \pTS can be chosen at run-time.  We will recall the basics of both \emph{explicit} and \emph{implicit} methods.  Because implicit time-stepping implies the full stack of \pSNES/\pKSP/\pPC solvers for the (generally) non-linear equations at each time step, our experience with \pSNES examples, from previous Chapters, will come in handy.

User code will discretize the two PDE examples in space only.  This \emph{method of lines} approach, which is the most common way to solve PDEs using \PETSc \pTS, avoids any need to write code specific to a temporal discretization.

The third example is the first \emph{system} of PDEs addressed so far.  While it requires some new constructs, implementing this system in \PETSc code is straightforward.  However, our next systems example, the Stokes equations in Chapter \ref{chap:ok}, will have more interesting implementation details and solver choices.

\section{Systems of ODEs}  Consider an ODE system in form
\begin{equation}
\by' = \bg(t,\by)  \label{eq:ts:ode}
\end{equation}
where $\by(t) \in \RR^N$, $\by'=d\by/dt$, and $g(t,\by)$ is continuous in both inputs.  Suppose an \emph{initial value} is given:
\begin{equation}
\by(t_0) = \by_0.  \label{eq:ts:ode:iv}
\end{equation}
Under additional, but reasonable, assumptions on the behavior of $\bg$, the continuum problem \eqref{eq:ts:ode}, \eqref{eq:ts:ode:iv} is a well-posed problem, at least for short times; this much is well-known.  Thus one can make predictions from such a ODE \emph{initial value problem} (IVP), forward or backward from the initial time.

Let us be specific about sufficient regularity of $\bg$ to imply well-posedness.  We will assume for simplicity that $\bg(t,\by)$ is continuous on a cylinder around the initial point $(t_0,\by_0)$,
   $$\mathcal{D} = \{(t,\by) \,:\, |t-t_0| \le \delta, \|\by - \by_0\| \le \omega\}$$
where $\delta > 0$ and $\omega > 0$.  Then we further assume that $\bg$ is \emph{Lipschitz} in its second argument, so that output differences are bounded by a multiple of input differences: there is $L\ge 0$ so that
\begin{equation}
\|\bg(t,\by_1) - \bg(t,\by_0)\| \le L \|\by_1-\by_0\|  \label{eq:ts:glipschitz}
\end{equation}
for $(t,\by_i) \in \mathcal{D}$.  Then the problem \eqref{eq:ts:ode}, \eqref{eq:ts:ode:iv} has a unique continuous, and continuously-differentiable, solution $\by(t)$ on a generally-shorter interval, namely $|t-t_0|<\eps$ for some $0 < \eps \le \delta$ \citep[section 17.5]{HirschSmaleDevaney2004}.  Furthermore the solution depends continuously on both the initial value $\by_0$ and the right-hand-side $\bg$.

Our brief attention to well-posedness is motivated by the following practical concern:  When we run a numerical differential equation code it will produce numbers.\sidenote{It \emph{always} produces numbers!}  These numbers are (essentially) never the exact solution of the differential equation, but they are ``correct'' in a numerical-analysis sense if we can demonstrate convergence to the solution of a well-posed continuum problem.  Thus we accept ``wrong'' numbers as correct in cases where demonstrably convergent numerical analysis is applied to a well-posed problem.

\emph{Much} more egregiously wrong, however, are numbers which represent no continuum solution at all.  Benign-looking scalar nonlinear ODEs can put us in such peril.  In fact, Exercise \ref{chap:ts}.\ref{exer:ts:tan} gives a simple, and well-known, example where the solution ceases to exist after a certain finite interval of time.  The approximating code sails right by the end of this interval of time, producing numbers that are infinitely-erroneous.  The examples in this book include several nonlinear PDEs, where this concern is yet greater.  Thus we will be cautious.  When possible we consider well-posedness, and perhaps even approximate-ability (regularity), prior to implementation.

For linear systems, solutions exist for all time.  The following example ODE system is thus a safe starting point.

\noindent\hrulefill
\begin{example}  \label{ex:ts:odeeasy} Consider the initial value problem in two dimensions
\begin{equation}
   \by' = \begin{bmatrix} y_1 \\ - y_0 + t\end{bmatrix}, \qquad \by(0) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \label{eq:ts:example}
\end{equation}
from initial time $t_0=0$.  This is a linear system with
    $$\bg(t,\by) = A \by + \bbf \,\text{ where } A = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \text{ and } \bbf(t) = \begin{bmatrix} 0 \\ t\end{bmatrix}.$$
The exact solution, shown in Figure \ref{fig:ts:ode}, is
\begin{equation}
    \by(t) = \begin{bmatrix} t - \sin t \\ 1 - \cos t \end{bmatrix}.\label{eq:ts:examplesoln}
\end{equation}

\vspace{-3mm}
\begin{figure}
\includegraphics[width=0.7\textwidth]{figs/ode}
\caption{The solution to \eqref{eq:ts:example} is given by \eqref{eq:ts:examplesoln}, with $y_0(t)$ solid and $y_1(t)$ dashed.}
\label{fig:ts:ode}
\end{figure}
\end{example}
\vspace{-5mm}
\noindent\hrulefill

\section{Methods, and their accuracy, for ODE initial value problems}  The above example ODE system is straightforward to approximate by any number of time-stepping numerical methods.  They all start from the initial values and use the right-hand-side function $g(t,\by)$ to update an approximate solution.  They all generate sequences $\{\bY_\ell\}$ which approximate the solution at discrete times $t_\ell$, so that $\bY_\ell \approx \by(t_\ell)$.  Because these ideas are easy to find in the literature\sidenote{A good reference is \citep{AscherPetzold1998}, for example.}, and are likely to be in the reader's background in some form, we only supply a gloss here.

Let $t_0<t_1<t_2<\dots<t_L$ be a finite sequence of times, with steps $\Delta t_\ell = t_\ell - t_{\ell-1}$ for $\ell=1,2,\dots,L$.  Let $\bY_0 = \by_0$ be the known starting value  \eqref{eq:ts:ode:iv}.  \emph{Euler's method}, also known as the forward Euler method, generates $\bY_1,\bY_2,\dots,\bY_L$ from
\begin{equation}
\frac{\bY_\ell - \bY_{\ell-1}}{\Delta t_\ell} = \bg(t_{\ell-1},\bY_{\ell-1}). \label{eq:ts:forwardeuler}
\end{equation}
One can also write this as a formula for the updated value $\bY_\ell$,
\begin{equation}
\bY_\ell = \bY_{\ell-1} + \Delta t_\ell \, \bg(t_{\ell-1},\bY_{\ell-1}). \label{eq:ts:forwardeulerupdate}
\end{equation}

The \emph{backward Euler method} is just as easy to state,
\begin{equation}
\frac{\bY_\ell - \bY_{\ell-1}}{\Delta t_\ell} = \bg(t_{\ell},\bY_{\ell}),  \label{eq:ts:backwardeuler}
\end{equation}
but much more work is required to use it in practice.  Indeed, just writing the analog of \eqref{eq:ts:forwardeulerupdate}, namely a formula by which to compute $\bY_\ell$, would require doing algebra on the function $\bg$.  Backward Euler is an \emph{implicit} method, as explored below.  At each time step \eqref{eq:ts:backwardeuler} generates exactly the kind of (generally) nonlinear system, with unknown values $\bY_\ell$, which we solved in previous Chapters by Newton's method and \pSNES.

Both methods \eqref{eq:ts:forwardeuler} and \eqref{eq:ts:backwardeuler}, which are the simplest approximations of \eqref{eq:ts:ode}, compute a first-order finite difference approximation of the derivative of $\by(t)$ by evaluating the right-hand side function $\bg(t,\by)$ at a particular point.  They differ only in the evaluation point.  They are ``first-order accurate'' in the simple sense that, for any twice-continuously-differentiable function $f(t)$, by Taylor's theorem
    $$\frac{f(t+\Delta t) - f(t)}{\Delta t} = f'(t) + O(\Delta t)$$
as $\Delta t \to 0$.  One says that the Euler methods have first-order \emph{local truncation error}.

However, under sufficient assumptions of smoothness, the deeper meaning of ``first-order accuracy'' also applies, as a rate of convergence as $\Delta t\to 0$.  One would hope that the approximations $\bY_\ell$ converge to the values $\by(t_\ell)$ in the same limit, but notice this requires knowing how the errors build up over many steps, and not just knowing the local truncation error at each step.

Regarding what is provable, by assuming that the solution $\by(t)$ exists, that a Lipschitz bound \eqref{eq:ts:glipschitz} applies to $\bg(t,\by)$, and that the solution has bounded second derivative ($|\by(t)''|\le M$), all on some interval $[t_0,t_f]$, one can show that the Euler methods satisfy $|\bY_\ell-\by(t_\ell)| = O(\Delta t^1)$.  In slightly more detail, one can show \citep{AscherPetzold1998}
\begin{equation}
|\bY_\ell-\by(t_\ell)| \le \frac{\Delta t\,M}{2L} (e^{L(t_\ell-t_0)}-1).  \label{eq:ts:eulerbound}
\end{equation}

The \emph{numerical error} $E_\ell = |\bY_\ell-\by(t_\ell)|$, also called the \emph{global truncation error}, is first-order in $\Delta t$ for the Euler methods: $E_\ell = O(\Delta t^1)$.  This statement is the intended meaning of ``first-order accuracy'' for these methods, while bound \eqref{eq:ts:eulerbound} itself is rarely used quantitatively.\sidenote{Not least because we apply higher-order methods, as below.}  The order of some method, stated as $E_\ell = O(\Delta t^p)$ for some $p>0$, tells us what to expect when we shorten the time step, namely by how much a reduction of the time step will reduce the numerical error.  Halving the time step in a first-order method should reduce the numerical error by half, for a second-order method by a fourth, and so on.

\newcommand{\RKtwoa}{RK$2$a\xspace}
\newcommand{\RKthreebs}{RK$3$bs\xspace}
\newcommand{\RKfour}{RK$4$\xspace}

It should be no surprise that there are methods with improved accuracy compared to the Euler methods.  Among these are the \emph{one-step multi-stage} methods, with the \emph{Runge-Kutta} family best known.  One-step methods use $\bY_{\ell-1}$ only, and not previous values, to build the next approximation $\bY_\ell$, but generally by multiple evaluations of the right-hand side $\bg$.  For example, denoting the time-step simply as $h=\Delta t_\ell$, the \emph{explicit trapezoidal} method \citep{AscherPetzold1998} is a second-order rule which takes a forward Euler step but then ``goes back'' and recomputes the step by the average of values of $\bg$:
\begin{align}
\hat\bY &= \bY_{\ell-1} + h\, \bg(t_{\ell-1},\bY_{\ell-1}) \label{eq:ts:rk2a} \\
\bY_\ell &= \bY_{\ell-1} + \frac{h}{2} \bg(t_{\ell-1},\bY_{\ell-1}) + \frac{h}{2} \bg(t_\ell,\hat\bY). \notag
\end{align}
Though two evaluations of $\bg$ (i.e.~the two \emph{stages}) occur at each time step, only the final result $\bY_\ell$ is used to inform the next time step (i.e.~the one which computes $\bY_{\ell+1}$).  In this sense the method is ``one-step'' despite being multi-stage.  Method \eqref{eq:ts:rk2a} is also called ``\RKtwoa;'' it is one of several possible second-order Runge-Kutta methods.

The well-known fourth-order Runge-Kutta method ``\RKfour'' has four stages.  Instead of stating this particular scheme using formulas like those in \eqref{eq:ts:rk2a} above, let us define tabular notation for \emph{any} one-step, $s$-stage method.  Namely, the $s+1$ formulas
\begin{align}
\hat\bY_i &= \bY_{\ell-1} + h \sum_{j=1}^s a_{ij}\, \bg(t_{\ell-1} + c_j h, \hat\bY_j), \qquad 1 \le i \le s \label{eq:ts:rkgeneral} \\
\bY_\ell  &= \bY_{\ell-1} + h \sum_{i=1}^s b_i\, \bg(t_{\ell-1} + c_i h, \hat\bY_i) \notag
\end{align}
correspond to the \emph{tableau} \citep{Butcher2008}
\begin{center}
\begin{tabular}{c|cccc}
$c_1$    & $a_{11}$ & $a_{12}$ & $\cdots$ & $a_{1s}$ \\
$c_2$    & $a_{21}$ & $a_{22}$ & $\cdots$ & $a_{2s}$ \\
$\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ & $\vdots$ \\
$c_s$    & $a_{s1}$ & $a_{s2}$ & $\cdots$ & $a_{ss}$ \\ \hline
         & $b_1$    & $b_2$    & $\cdots$ & $b_s$
\end{tabular}
\end{center}

For example, the $s=2$ method \RKtwoa is the left tableau in Table \ref{tab:ts:exampletableau}, and the $s=4$ method \RKfour is the middle one.  Spaces in the tableau correspond to zeros.  Note that, in all three methods in Table \ref{tab:ts:exampletableau}, $\hat\bY_1=\bY_{\ell-1}$ so the first rows are trivial.  Also, because these schemes are explicit they have nonzero $a_{ij}$ only strictly below the diagonal; we return to this idea below.

\begin{table}
\begin{tabular}{c|cc}
$0$   \\
$1$ & $1$ \\ \hline
    & $\frac{1}{2}$ & $\frac{1}{2}$
\end{tabular}
\hfill
\begin{tabular}{c|cccc}
$0$   \\
$\frac{1}{2}$ & $\frac{1}{2}$ \\
$\frac{1}{2}$ & $0$           & $\frac{1}{2}$ \\
$1$           & $0$           & $0$           & $1$  \\ \hline
              & $\frac{1}{6}$ & $\frac{1}{3}$ & $\frac{1}{3}$ & $\frac{1}{6}$
\end{tabular}
\hfill
\begin{tabular}{c|cccc}
$0$   \\
$\frac{1}{2}$ & $\frac{1}{2}$ \\
$\frac{3}{4}$ & $0$           & $\frac{3}{4}$ \\
$1$           & $\frac{2}{9}$ & $\frac{1}{3}$ & $\frac{4}{9}$ \\ \hline
              & $\frac{2}{9}$ & $\frac{1}{3}$ & $\frac{4}{9}$ & $0$ \\
              & $\frac{7}{24}$ & $\frac{1}{4}$ & $\frac{1}{3}$ & $\frac{1}{8}$
\end{tabular}
\medskip
\caption{Tableau for the explicit trapezoidal rule (\RKtwoa; left), the classical fourth-order Runge-Kutta method (\RKfour; middle), and an embedded four-stage, third-order scheme (\RKthreebs, the \PETSc default; right).} \label{tab:ts:exampletableau}
\end{table}

In summary, a one-step, multi-stage method, whether described by \eqref{eq:ts:rkgeneral} or a tableau, computes a new value $\bY_\ell$ via intermediate results $\hat\bY_i$.  For a more interesting example of this approach, we describe the default Runge-Kutta scheme in \PETSc, an \emph{adaptive} scheme which is \emph{embedded}.  Let us give meaning to these terms.

An ODE numerical solution method is \emph{adaptive} if it adjusts the time step by a procedure which uses information from the already-computed values.  An adaptive scheme will take shorter steps when the values from $\bg$ are rapidly-changing or irregular, and longer steps when they are smoother and more predictable.  Successfully implementing such strategies, the error estimation and control \citep{AscherPetzold1998} done internally by the \PETSc time-steppers (below), and not visible in our user code,\sidenote{Thank goodness!} is nontrivial and outside our scope.

One strategy to build an adaptive Runge-Kutta method is to make it \emph{embedded}.  Such a scheme is in fact two methods which have the same $c_j$ and $a_{ij}$ coefficients in their tableau, but different $b_j$ \emph{and} different orders of accuracy.  The Bogacki-Shampine scheme \RKthreebs is such an embedded pair.\sidenote{The reader may have seen it as the \texttt{ode23} method in \Matlab.}  Each is four stage, but one is of third order and one of second order.  Their tableau are shown at right in Table \ref{tab:ts:exampletableau}, with the $b_j$ coefficients listed on different rows below the horizontal line.  The different results from these two schemes is used to decide if the time step was short enough to satisfy the error tolerance/goal \citep{Butcher2008}.  In \RKthreebs specifically, one scheme (first $b_j$ row) makes $O(\Delta t^3)$ local truncation errors, while the other (second $b_j$ row) makes $O(\Delta t^2)$ errors, and the difference estimates the accuracy of the lower order scheme.  The work of computing two approximations is the same cost as just one, because they share the same evaluations of $\bg$; this is the central meaning of ``embedded.''  Further details of the \RKthreebs scheme, including its stability and efficiency properties, are beyond our scope, but see \citep{BogackiShampine1989}.

There are alternative families other than one-step schemes.  For example, the \emph{second-order Adams-Bashforth} method
\begin{equation}
\bY_\ell = \bY_{\ell-1} - \frac{h}{2} \bg(t_{\ell-2},\bY_{\ell-2}) + \frac{3 h}{2} \bg(t_{\ell-1},\bY_{\ell-1})  \label{eq:ts:ab2}
\end{equation}
is a \emph{multi-step} method.  It computes a new value $\bY_\ell$ with only one additional evaluation of $\bg$ (i.e.~one stage), but using two previous values $\bY_{\ell-2},\bY_{\ell-1}$.  This works in a straightforward way only if the step-size $h=\Delta t$ is independent of $\ell$.  A goal of such methods is to achieve order greater than one with only one evaluation of $\bg$ per time step, and this is often achieved in practice.  Other schemes combine the multiple-stages and multi-step ideas such as the \emph{general linear methods} \citep{Butcher2008}.  In our quick treatment of time-stepping, we will say no more about multi-step and general linear methods, though the latter appear as possible \PETSc options in Table \ref{tab:ts:tstypes} below.

The general theme is that the cost of an ODE numerical method is proportional to the number of (distinct) evaluations of the right-hand side $\bg$ of the ODE, especially if $\bg$ has significant nonlinearities.  The additional arithmetic in implementing a particular scheme, represented for example by the number of coefficients in the tableau, is usually less important.

Clearly there are a very large number of possible methods, many of which are available from the \PETSc command line, whether one-step or multi-step, adaptive or not, and so on.   Choosing the ``best'' one for a given ODE system is not a reasonable goal.  Our goal in this Chapter is only to suggest some reasonable choices for broad classes of problems, but we invoke an essential \PETSc mantra here, namely: a rich set of command-line, run-time options is \emph{not a bad thing}.

\section{\PETSc \pTS objects, and a first example}  The example ODE system on page \pageref{ex:ts:odeeasy} is a good starting point for our first \PETSc ODE-solving program because it is of small dimension $N=2$ and because we have an exact solution with which to evaluate numerical error.  The program \texttt{c/ch6/ode.c}, shown in Codes \ref{code:ts:ode:main} and \ref{code:ts:ode:callbacks}, uses a \PETSc time-stepping object of type \pTS.  We will first summarize the code's actions on the \pTS object itself, and then show the part which specifies the right-hand-side $\bg(t,\by)$.

\cinputpart{ode.c}{\CODELOC}{The \texttt{main()} part of \texttt{ode.c} creates and configures a \pTS object, plus the \pVecs for approximate and exact solutions}{I}{//MAIN}{//ENDMAIN}{code:ts:ode:main}

Consider these commands extracted from Code \ref{code:ts:ode:main}:
\begin{code}
  TSCreate(PETSC_COMM_WORLD,&ts);
  TSSetProblemType(ts,TS_NONLINEAR);
  TSSetRHSFunction(ts,NULL,FormRHSFunction,NULL);
  TSSetRHSJacobian(ts,J,J,FormRHSJacobian,NULL);
\end{code}
It turns out there is little reason to design \PETSc examples for the linear case only.  By setting the problem type to \texttt{TS\_NONLINEAR} we are saying that the problem is in form \eqref{eq:ts:ode} with a function $\bg(t,\by)$ which will be supplied by user code.\sidenote{As we will see in later examples, a \pTS of type \texttt{TS\_NONLINEAR} may have an even more general form than equation \eqref{eq:ts:ode}.}

Applying \texttt{TSSetRHSFunction()} sets a call-back to our own function---here named \texttt{FormRHSFunction()}---which evaluates $\bg(t,\by)$; see Code \ref{code:ts:ode:callbacks}.  We also set a Jacobian call-back to our function \texttt{FormRHSJacobian()} which sets the entries of the \pMat \texttt{J} corresponding to
    $$\frac{\partial \bg}{\partial \by}(t,\by) = \begin{bmatrix}\phantom{\bigg|} \frac{\partial g_i}{\partial y_j}(t,\by) \phantom{\bigg|}\end{bmatrix}.$$
Because an allocated \pMat must be supplied to \texttt{TSSetRHSJacobian()}, the above \pTS set-up lines are preceded, in Code \ref{code:ts:ode:main}, by a \texttt{Create/SetUp} sequence for \pMat \texttt{J}.  As usual when setting Jacobians, we assign \texttt{J} to both the Jacobian and preconditioner-material arguments of \texttt{TSSetRHSJacobian()}; compare usage of \texttt{SNESSetJacobian()} and \texttt{DMDASNESSetJacobianLocal()} in Chapters \ref{chap:nl} and \ref{chap:of}.

Next are additional commands for configuring the \pTS.  All of the following choices can, and often will, be overridden by command-line options:
\begin{code}
  TSSetType(ts,TSRK);
  TSSetInitialTimeStep(ts,t0,dt);
  TSSetDuration(ts,100*(int)((tf-t0)/dt),tf-t0);
  TSSetExactFinalTime(ts,TS_EXACTFINALTIME_MATCHSTEP);
  TSSetFromOptions(ts);
\end{code}
Note that, despite its name, \texttt{TSSetInitialTimeStep()} sets both the initial time \texttt{t0} and the initial time step \texttt{dt}.  (Note that if the numerical ODE solver chosen at run-time is in fact adaptive then \texttt{dt} is indeed only the \emph{initial} time step, generally modified by the solver after the first step.)  Next, \texttt{TSSetDuration()} sets both the duration of the solve (\texttt{tf-t0}) and the maximum number of steps that the solver is allowed to take.  In this case we set the latter quantity to 100 times the intended number of steps, namely the duration over the initial time step (\texttt{(tf-t0)/dt}), based on our other choices.


\begin{code}
  TSGetTime(ts,&t0);
  SetFromExact(t0,y);
  TSGetTimeStep(ts,&dt);
  PetscPrintf(...,"solving from t0 = ...",t0,dt);
  TSSolve(ts,y);
  TSGetTime(ts,&tf);
  SetFromExact(tf,yexact);
  ...
  PetscPrintf(...,"error at tf = ...",tf,err);
\end{code}

\cinputpart{ode.c}{\CODELOC}{FIXME}{II}{//CALLBACKS}{//ENDCALLBACKS}{code:ts:ode:callbacks}

\begin{cline}
$ cd c/ch6/
$ make ode
$ ./ode -ts_monitor
$ ./ode -ts_monitor -ts_final_time 20 -ts_monitor_lg_solution -draw_pause 0.1
\end{cline}

\begin{cline}
$ ./ode 
solving from t0 = 0.000 with initial dt = 0.10000 ...
error at tf = 1.000 :  |y-y_exact|_inf = 0.000144484
$ ./ode -ts_init_time 19.0 -ts_dt 0.01 -ts_final_time 20.0
solving from t0 = 19.000 with initial dt = 0.01000 ...
error at tf = 20.000 :  |y-y_exact|_inf = 0.000303815
\end{cline}

FIXME control \texttt{rk} by \texttt{-ts\_type rk -ts\_atol Z -ts\_rk\_type XX}

FIXME use \texttt{-help} for additional:

\begin{cline}
$ ./ode -ts_monitor -ts_type rk -help |grep ts_rk
  -ts_rk_type <3bs> (choose one of) 5dp 5f 4 3bs 3 2a 1fe
\end{cline}
%$

FIXME generate figures by, and explain \texttt{-ts\_monitor binary:t.dat -ts\_monitor\_solution binary:y.dat} usage, via \texttt{PetscBinaryIO.py}; we have helper script \texttt{plottrajectory.py}

%// ./ode -ts_monitor
%// ./ode -ts_monitor_solution
%// ./ode -ts_monitor_solution draw -draw_pause 0.1
%
%// compare
%// ./ode -ts_view   # for default explicit RK
%// ./ode -ts_view -ts_type beuler  # has nonlinear solver
%
%//-ts_final_time
%//-ts_init_time
%//-ts_dt
%
%// ./ode -ts_monitor -ts_type beuler -ode_steps 1000    // finally close to default RK
%// ./ode -log_view |grep Eval   // compare rk, beuler, cn
%
%// ./ode -ts_type euler   // time-stepping failure; see petsc issue #119
%// ./ode -ts_monitor -ts_type rk -ts_rk_type 1fe -ts_adapt_type none   // correct Euler

\section{Time-stepping stability and implicitness}

FIXME: methods above are \emph{explicit} if $a_{ij}=0$ for $j\ge i$, that is, if the array $a_{ij}$ in the Butcher tableau is zero on and above the diagonal

FIXME: methods \emph{implicit} otherwise, e.g.~backward Euler \eqref{eq:ts:backwardeuler} or  ``$\theta$''-methods as in 
\begin{equation}
FIXME
\end{equation}
Shown as this tableau
\begin{center}
\begin{tabular}{c|cc}
$0$   \\
$1$ & $1-\theta$ & $\theta$ \\ \hline
    & $1-\theta$ & $\theta$
\end{tabular}
\end{center}

FIXME: explain goal of stability

FIXME: demonstrate transformation of an implicit time step into form $\bF(\bx)=0$

FIXME \pTS contains \pSNES inside (and thus the rest)

In a \PETSc code which uses the \pTS type for solving an ODE initial value problem, one chooses the methods mentioned above by options \texttt{-ts\_type TYPE} where FIXME

\begin{table}
\small
\begin{tabular}{lllll}
\emph{method}                                & \texttt{-ts\_type} & \emph{adaptive?} & \emph{common options} \\ \hline
forward Euler \eqref{eq:ts:forwardeuler}    & \texttt{euler}  & no & \\
Runge-Kutta  FIXME                          & \texttt{rk}     & yes & \texttt{-ts\_rk\_type} FIXME \\ \hline
backward Euler \eqref{eq:ts:backwardeuler}  & \texttt{beuler} & no & \\
$\theta>0$   FIXME                          & \texttt{theta}  & no & FIXME \\
Crank-Nicolson                              & \texttt{cn}     & no & 
\end{tabular}
\caption{Choices of the best-known ODE integration methods.  Explicit schemes are above the line and implicit below.  Compare Table \ref{tab:ts:tstypes}, which is more complete.} \label{tab:ts:odebasictypes}
\end{table}

\cinputpart{odejac.c}{\CODELOC}{FIXME}{I}{//JACOBIAN}{//ENDJACOBIAN}{code:ts:odejac:jacobian}

\cinputpart{odejac.c}{\CODELOC}{FIXME}{II}{//MATJ}{//ENDMATJ}{code:ts:odejac:mat}

FIXME function $\bg$ is \texttt{TSSetRHSFunction()}, Jacobian $\partial \bg/\partial \by$ is \texttt{TSSetRHSJacobian()}

FIXME \texttt{ode.c} is thus sort of similar to \texttt{ecjac.c}

FIXME build all tools around the ``hard'' time dependent case of stiff + nonlinear, though not DAE

FIXME control \texttt{euler,beuler,cn,theta} by setting steps

FIXME use \texttt{-help} for additional:

\begin{cline}
$ ./ode -ts_monitor -ts_type theta -help |grep ts_theta
  -ts_theta_theta <0.5>: Location of stage (0<Theta<=1)
  -ts_theta_extrapolate: <FALSE> Extrapolate stage solution from previous ...
  -ts_theta_endpoint: <FALSE> Use the endpoint instead of midpoint form of ...
  -ts_theta_adapt: <FALSE> Use time-step adaptivity with the Theta method
\end{cline}
%$

\section{Time-dependent heat equation}

FIXME solve heat equation by implicit

\cinputpart{heat.c}{\CODELOC}{FIXME}{I}{//HEATCTX}{//ENDHEATCTX}{code:ts:heat:heatctx}

\cinputpart{heat.c}{\CODELOC}{FIXME}{II}{//RHSFUNCTION}{//ENDRHSFUNCTION}{code:ts:heat:rhsfunction}

\cinputpart{heat.c}{\CODELOC}{FIXME}{III}{//RHSJACOBIAN}{//ENDRHSJACOBIAN}{code:ts:heat:rhsjacobian}

\cinputpart{heat.c}{\CODELOC}{FIXME}{IV}{//TSSETUP}{//ENDTSSETUP}{code:ts:heat:tssetup}

\cinputpart{heat.c}{\CODELOC}{FIXME}{V}{//MONITOR}{//ENDMONITOR}{code:ts:heat:monitor}

\begin{figure}
% usage: \standardTSstack{scale}{Jacobian}{DMDA}{DMPlex}
\standardTSstack{0.775}{}{}{dashed}
\caption{The \PETSc stack used for the time-dependent heat and reaction/diffusion problems (\texttt{heat.c,pattern.c}).  Compare Figure \ref{fig:of:standardstack}.}
\label{fig:of:tsstack}
\end{figure}

FIXME show \texttt{-ts\_type theta,beuler,cn}


\section{Coupled reaction-diffusion equations}

FIXME see pages 21--22 of \citep{HundsdorferVerwer2003} and see \citep{Pearson1993}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{I}{//FIELDCTX}{//ENDFIELDCTX}{code:ts:pattern:fieldctx}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{II}{//RHSFUNCTION}{//ENDRHSFUNCTION}{code:ts:pattern:rhsfunction}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{III}{//IFUNCTION}{//ENDIFUNCTION}{code:ts:pattern:ifunction}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{IV}{//IJACOBIAN}{//ENDIJACOBIAN}{code:ts:pattern:ijacobian}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{V}{//TSSETUP}{//ENDTSSETUP}{code:ts:pattern:tssetup}

FIXME show use of \texttt{-ts\_type arkimex,theta}

FIXME final-time greyscale plots using \texttt{PetscBinaryIO.py}


\section{Exercises}

\renewcommand{\labelenumi}{\arabic{chapter}.\arabic{enumi}\quad}
\renewcommand{\labelenumii}{(\alph{enumii})}
\begin{enumerate}
\item \label{exer:ts:tan}  Consider the scalar ODE initial value problem $y'=1+y^2$, $y(0)=0$.  Show by-hand that $y(t)=\tan t$ is the unique solution to this problem.  Modify \texttt{ode.c} to solve this problem.  Run the code from $t=0$ to $t=t_f=2$.  What run-time observable, actual evidence shows that your estimate of ``$y(2)$'' is meaningless?
\item FIXME which \texttt{-ts\_type} work with \texttt{ode.c}?  do \texttt{-help |grep ts\_type} to find all the possibilities, and try 
% euler, beuler, rk, theta, cn
\item FIXME confirm the claim in the text that the Jacobian is not used in explicit methods \texttt{-ts\_type euler,rk} by deleting \texttt{FormRHSJacobian()} from \texttt{ode.c}; with it still deleted check that \texttt{-ts\_type beuler,theta,cn} work when used in combination with \texttt{-snes\_fd} or \texttt{-snes\_mf}
\item FIXME \texttt{ode.c} says it is ``serial only''  what happens with \texttt{mpiexec -n 2 ./ode}, and why?
\item FIXME version of \texttt{ode.c} which solves DAE system
    $$\bbf(t,\by,\by') = \bg(t,\by)$$
where $\partial \bg/\partial \by'$ may be singular
\item FIXME for run like
\begin{cline}
./pattern -da_refine 6 -ptn_tf 500 -ptn_steps 100 -ts_monitor -snes_converged_reason
\end{cline}
which is fastest among these nine ARKIMEX and three $\theta$ methods?:
\begin{code}
-ts_type arkimex -ts_arkimex_type [a2|l2|ars122|2c|2d|2e|3|4|5]
-ts_type theta -ts_theta_endpoint -ts_theta_theta [0.5|0.75|1]
\end{code}
Are any other adaptive \pTS types faster?  Also compare final frames from \texttt{-ts\_monitor\_solution draw}; how worried should we be about the effect of time-stepping on the solution of this problem?
\end{enumerate}

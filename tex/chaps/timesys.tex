
To the reader who has become lost in a maze of issues which have built up around the examples so far---Krylov solvers, preconditioner choices, line searches, finite element details, or whatever causes confusion---some good news.  In \emph{this} Chapter we restart with an easy question and a small example.

Solving ordinary differential equations (ODEs) with \PETSc ought to be easy.  Given that we have practice with \PETSc objects, it is.  Furthermore, the transition to numerical methods for time-dependent, parabolic PDEs like the heat equation is straightforward.

In this Chapter we look at three examples:
\begin{itemize}
\item a linear system of two ODEs,
\item an arbitrarily-large system of ODEs arising from spatial finite-difference approximation of the 2D time-dependent heat PDE,
\item and another large ODE system arising from two coupled nonlinear PDEs in 2D, forming a two-species reaction/diffusion model.
\end{itemize}
These problems are solved using a new \PETSc object, the \pTS time-stepping solver.

The particular integration method used by the \pTS can be chosen at run-time.  We will recall the basics of both \emph{explicit} and \emph{implicit} methods.  Because implicit time-stepping implies the full stack of \pSNES/\pKSP/\pPC solvers for the (generally) non-linear equations at each time step, our experience with \pSNES examples, from previous Chapters, will come in handy.

User code will discretize the two PDE examples in space only.  This \emph{method of lines} approach, which is the most common way to solve PDEs using \PETSc \pTS, avoids any need to write code specific to a temporal discretization.

The third example is the first \emph{system} of PDEs addressed so far.  While it requires some new constructs, implementing this system in \PETSc code is straightforward.  However, our next systems example, the Stokes equations in Chapter \ref{chap:ok}, will have more interesting implementation details and solver choices.


\section{Systems of ODEs}

Consider an ODE system in form
\begin{equation}
\by' = \bg(t,\by)  \label{eq:ts:ode}
\end{equation}
where $\by(t) \in \RR^N$ and $\by'=d\by/dt$.  Suppose an \emph{initial value} is given:
\begin{equation}
\by(t_0) = \by_0.  \label{eq:ts:ode:iv}
\end{equation}
Under reasonable assumptions on the behavior of $\bg$, the continuum problem \eqref{eq:ts:ode}, \eqref{eq:ts:ode:iv} is a well-posed problem, at least for short times; this much is well-known.\sidenote{See \citep{HirschSmaleDevaney2004}, for example.}  Thus one can make predictions from such a ODE \emph{initial value problem} (IVP), forward or backward from the initial time, supposing that the integration to solve the ODE is performed exactly.

Let us be specific about sufficient regularity of $\bg$ to imply well-posedness.  We will assume for simplicity that $\bg(t,\by)$ is continuous on a cylinder around the initial point $(t_0,\by_0)$,
   $$\mathcal{D} = \{(t,\by) \,:\, |t-t_0| \le \delta, \|\by - \by_0\| \le \omega\}$$
where $\delta > 0$ and $\omega > 0$.  Then we further assume that $\bg$ is \emph{Lipschitz} in its second argument, so that output differences are bounded by a multiple of input differences: there is $L\ge 0$ so that
\begin{equation}
\|\bg(t,\by_1) - \bg(t,\by_0)\| \le L \|\by_1-\by_0\|  \label{eq:ts:glipschitz}
\end{equation}
for $(t,\by_i) \in \mathcal{D}$.  Then the problem \eqref{eq:ts:ode}, \eqref{eq:ts:ode:iv} has a unique continuous, and continuously-differentiable, solution $\by(t)$ on a generally-shorter interval, namely $|t-t_0|<\eps$ for some $0 < \eps \le \delta$ \citep[section 17.5]{HirschSmaleDevaney2004}.  Furthermore the solution depends continuously on both the initial value $\by_0$ and the right-hand-side $\bg$.

Our brief attention to well-posedness is motivated by the following practical concern:  When we run a numerical differential equation code it will produce numbers.\sidenote{It \emph{always} produces numbers!}  These numbers are (essentially) never the exact solution of the differential equation, but they are ``correct'' in a numerical-analysis sense if we can demonstrate convergence to the solution of a well-posed continuum problem.  Thus we accept ``wrong'' numbers as correct in cases where demonstrably convergent numerical analysis is applied to a well-posed problem.

\emph{Much} more egregiously wrong, however, are numbers which represent no continuum solution at all.  Benign-looking scalar nonlinear ODEs can put us in such peril.  In fact, Exercise \ref{chap:ts}.\ref{exer:ts:tan} gives a simple, and well-known, example where the solution ceases to exist after a certain finite interval of time.  The approximating code sails right by the end of this interval of time, producing numbers that are infinitely-erroneous.  The examples in this book include several nonlinear PDEs, where this concern is yet greater.  Thus we will be cautious.  When possible we consider well-posedness, and perhaps even approximate-ability (regularity), prior to implementation.

For linear systems, solutions exist for all time.\sidenote{Assuming continuous coefficients and source functions \citep[section 17.5]{HirschSmaleDevaney2004}.}  The following example ODE system is thus a safe starting point.

\noindent\hrulefill
\begin{example}  \label{ex:ts:odeeasy} Consider the initial value problem in two dimensions
\begin{equation}
   \by' = \begin{bmatrix} y_1 \\ - y_0 + t\end{bmatrix}, \qquad \by(0) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}, \label{eq:ts:example}
\end{equation}
from initial time $t_0=0$.  This is a linear system with
    $$\bg(t,\by) = A \by + \bbf \,\text{ where } A = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \text{ and } \bbf(t) = \begin{bmatrix} 0 \\ t\end{bmatrix}.$$
The exact solution, shown in Figure \ref{fig:ts:ode}, is
\begin{equation}
    \by(t) = \begin{bmatrix} t - \sin t \\ 1 - \cos t \end{bmatrix}.\label{eq:ts:examplesoln}
\end{equation}

\vspace{-3mm}
\begin{figure}
\includegraphics[width=0.7\textwidth]{figs/ode}
\caption{The solution to \eqref{eq:ts:example} is given by \eqref{eq:ts:examplesoln}, with $y_0(t)$ solid and $y_1(t)$ dashed.}
\label{fig:ts:ode}
\end{figure}
\end{example}
\vspace{-5mm}
\noindent\hrulefill


\section{Methods and accuracy for ODE initial value problems}  The above Example is straightforward to approximate by any number of time-stepping numerical methods.  They all start from the initial values and use the right-hand-side function $g(t,\by)$ to update an approximate solution $\bY(t) \approx \by(t)$.  They generate sequences $\bY_\ell$ which approximate the solution at discrete times $t_\ell$, so that $\bY_\ell \approx \by(t_\ell)$.  Because these ideas are easy to find in the literature\sidenote{A good reference is \citep{AscherPetzold1998}, for example.}, and are likely to be in the reader's background in some form, we only supply a gloss here.

Let $t_0<t_1<t_2<\dots<t_L$ be a finite sequence of times, with steps $\Delta t_\ell = t_\ell - t_{\ell-1}$ for $\ell=1,2,\dots,L$.  Let $\bY_0 = \by_0$ be the known starting value  \eqref{eq:ts:ode:iv}.  \emph{Euler's method}, also known as the forward Euler method, generates $\bY_1,\bY_2,\dots,\bY_L$ from
\begin{equation}
\frac{\bY_\ell - \bY_{\ell-1}}{\Delta t_\ell} = \bg(t_{\ell-1},\bY_{\ell-1}). \label{eq:ts:forwardeuler}
\end{equation}
This is the simplest approximation of \eqref{eq:ts:ode}.  The \emph{backward Euler method} is just as easy to state,
\begin{equation}
\frac{\bY_\ell - \bY_{\ell-1}}{\Delta t_\ell} = \bg(t_{\ell},\bY_{\ell}),  \label{eq:ts:backwardeuler}
\end{equation}
but much more work is required to use it in practice.  It is an \emph{implicit} method, as explored below.  In fact, at each step \eqref{eq:ts:backwardeuler} generates exactly the kind of (generally) nonlinear system which we solved in Chapter \ref{chap:nl} by Newton's method and \pSNES.

Both methods \eqref{eq:ts:forwardeuler} and \eqref{eq:ts:backwardeuler} compute a first-order finite difference approximation of the derivative of the trajectory $\by(t)$ by evaluating the right-hand side function $\bg(t,\by)$ at a particular point.  They differ only in the evaluation point.

They are first-order accurate in the simple sense that, for any twice-continuously-differentiable function $f(t)$, by Taylor's theorem
    $$\frac{f(t+\Delta t) - f(t)}{\Delta t} = f'(t) + O(\Delta t^1)$$
as $\Delta t \to 0$.  One says that the Euler methods have first-order \emph{local truncation error}.

However, under sufficient assumptions of smoothness, the deeper meaning of ``first-order accuracy'' also applies, as a rate of convergence as $\Delta t\to 0$.  Certainly one would hope that the approximations $\bY_\ell$ converge to the values $\by(t_\ell)$.  In fact, by assuming that the solution $\by(t)$ exists, that a Lipschitz bound \eqref{eq:ts:glipschitz} applies to $\bg(t,\by)$, and that the solution has bounded second derivative ($|\by(t)''|\le M$), all on some interval $[t_0,t_f]$, one can show that the Euler methods satisfy $|\bY_\ell-\by(t_\ell)| = O(\Delta t^1)$.  In slightly more detail, one can show
\begin{equation}
|\bY_\ell-\by(t_\ell)| \le \frac{\Delta t\,M}{2L} (e^{L(t_\ell-t_0)}-1)  \label{eq:ts:eulerbound}
\end{equation}
\citep{AscherPetzold1998}.

The idea that the \emph{numerical error}\sidenote{Sometimes ``global truncation error''.} $E_\ell = |\bY_\ell-\by(t_\ell)|$ is first-order in $\Delta t$ is the intended meaning of ``first-order accuracy'' for these methods.  Bound \eqref{eq:ts:eulerbound} itself is rarely used quantitatively.  (Not least because we apply higher-order methods, as below.)  Given the order of a method, we know---or expect---by what power a reduction of the time step will reduce the numerical error.  That is, halving the time step in a first-order method should reduce the numerical error by half, for a second-order method by a fourth, and so on.

It should be no surprise that there are methods with improved accuracy.

FIXME: RK methods



\section{Implicit time-stepping schemes}

FIXME: theta methods




\section{\PETSc \pTS objects}

FIXME introduce \pTS, which contains \pSNES inside (and thus the rest)

In a \PETSc code which uses the \pTS type for solving an ODE initial value problem, one chooses the methods mentioned above by options \texttt{-ts\_type TYPE} where FIXME

\begin{table}
\small
\begin{tabular}{lllll}
\emph{method}                                & \texttt{-ts\_type} & \emph{adaptive?} & \emph{common options} \\ \hline
forward Euler \eqref{eq:ts:forwardeuler}    & \texttt{euler}  & no & \\
Runge-Kutta  FIXME                          & \texttt{rk}     & yes & \texttt{-ts\_rk\_type} FIXME \\ \hline
backward Euler \eqref{eq:ts:backwardeuler}  & \texttt{beuler} & no & \\
$\theta>0$   FIXME                          & \texttt{theta}  & no & FIXME \\
Crank-Nicolson                              & \texttt{cn}     & no & 
\end{tabular}
\caption{Choices of the best-known ODE integration methods.  Explicit schemes are above the line and implicit below.  Compare Table \ref{tab:ts:tstypes}, which is more complete.} \label{tab:ts:odebasictypes}
\end{table}


\section{Fixed-dimension example }

The Example ODE system on page \pageref{ex:ts:odeeasy} is solved by \texttt{c/ch6/ode.c}, a program shown in Codes \ref{code:ts:ode:main} and \ref{code:ts:ode:callbacks}.  We will first summarize our code's actions on the \pTS object itself, and then those parts which describe the right-hand-side $\bg(t,\by)$.

\cinputpart{ode.c}{\CODELOC}{The \texttt{main()} part of \texttt{ode.c} creates and configures a \pTS object, plus the \pVecs for approximate and exact solutions, and a \pMat for storing the Jacobian.}{I}{//MAIN}{//ENDMAIN}{code:ts:ode:main}

Consider these commands extracted from Code \ref{code:ts:ode:main}:
\begin{code}
  TSCreate(PETSC_COMM_WORLD,&ts);
  TSSetProblemType(ts,TS_NONLINEAR);
  TSSetRHSFunction(ts,NULL,FormRHSFunction,NULL);
  TSSetRHSJacobian(ts,J,J,FormRHSJacobian,NULL);
\end{code}
It turns out there is little reason to design \PETSc examples for the linear case only.  By setting the problem type to \texttt{TS\_NONLINEAR} we are saying that the problem is in form \eqref{eq:ts:ode} with a function $\bg(t,\by)$ which will be supplied by user code.\sidenote{As we will see in later examples, a \pTS of type \texttt{TS\_NONLINEAR} may have an even more general form than equation \eqref{eq:ts:ode}.}

Applying \texttt{TSSetRHSFunction()} sets a call-back to our own function---here named \texttt{FormRHSFunction()}---which evaluates $\bg(t,\by)$; see Code \ref{code:ts:ode:callbacks}.  We also set a Jacobian call-back to our function \texttt{FormRHSJacobian()} which sets the entries of the \pMat \texttt{J} corresponding to
    $$\frac{\partial \bg}{\partial \by}(t,\by) = \begin{bmatrix}\phantom{\bigg|} \frac{\partial g_i}{\partial y_j}(t,\by) \phantom{\bigg|}\end{bmatrix}.$$
Because an allocated \pMat must be supplied to \texttt{TSSetRHSJacobian()}, the above \pTS set-up lines are preceded, in Code \ref{code:ts:ode:main}, by a \texttt{Create/SetUp} sequence for \pMat \texttt{J}.  As usual when setting Jacobians, we assign \texttt{J} to both the Jacobian and preconditioner-material arguments of \texttt{TSSetRHSJacobian()}; compare usage of \texttt{SNESSetJacobian()} and \texttt{DMDASNESSetJacobianLocal()} in Chapters \ref{chap:nl} and \ref{chap:of}.

Next are additional commands for configuring the \pTS.  All of the following choices can, and often will, be overridden by command-line options:
\begin{code}
  TSSetType(ts,TSRK);
  TSSetInitialTimeStep(ts,t0,dt);
  TSSetDuration(ts,100*(int)((tf-t0)/dt),tf-t0);
  TSSetExactFinalTime(ts,TS_EXACTFINALTIME_MATCHSTEP);
  TSSetFromOptions(ts);
\end{code}
Note that, despite its name, \texttt{TSSetInitialTimeStep()} sets both the initial time \texttt{t0} and the initial time step \texttt{dt}.  (Note that if the numerical ODE solver chosen at run-time is in fact adaptive then \texttt{dt} is indeed only the \emph{initial} time step, generally modified by the solver after the first step.)  Next, \texttt{TSSetDuration()} sets both the duration of the solve (\texttt{tf-t0}) and the maximum number of steps that the solver is allowed to take.  In this case we set the latter quantity to 100 times the intended number of steps, namely the duration over the initial time step (\texttt{(tf-t0)/dt}), based on our other choices.


\begin{code}
  TSGetTime(ts,&t0);
  SetFromExact(t0,y);
  TSGetTimeStep(ts,&dt);
  PetscPrintf(...,"solving from t0 = ...",t0,dt);
  TSSolve(ts,y);
  TSGetTime(ts,&tf);
  SetFromExact(tf,yexact);
  ...
  PetscPrintf(...,"error at tf = ...",tf,err);
\end{code}

\cinputpart{ode.c}{\CODELOC}{FIXME}{II}{//CALLBACKS}{//ENDCALLBACKS}{code:ts:ode:callbacks}

\begin{cline}
$ cd c/ch6/
$ make ode
$ ./ode -ts_monitor
$ ./ode -ts_monitor -ts_final_time 20 -ts_monitor_lg_solution -draw_pause 0.1
\end{cline}

\begin{cline}
$ ./ode 
solving from t0 = 0.000 with initial dt = 0.10000 ...
error at tf = 1.000 :  |y-y_exact|_inf = 0.000144484
$ ./ode -ts_init_time 19.0 -ts_dt 0.01 -ts_final_time 20.0
solving from t0 = 19.000 with initial dt = 0.01000 ...
error at tf = 20.000 :  |y-y_exact|_inf = 0.000303815
\end{cline}


FIXME function $\bg$ is \texttt{TSSetRHSFunction()}, Jacobian $\partial \bg/\partial \by$ is \texttt{TSSetRHSJacobian()}

FIXME \texttt{ode.c} is most similar to \texttt{ecjacobian.c}

FIXME build all tools around the ``hard'' time dependent case of stiff + nonlinear, though not DAE

FIXME control \texttt{euler,beuler,cn,theta} by setting steps

FIXME control \texttt{rk} by \texttt{-ts\_type rk -ts\_atol Z -ts\_rk\_type XX}

FIXME generate figures by, and explain \texttt{-ts\_monitor binary:t.dat -ts\_monitor\_solution binary:y.dat} usage, via \texttt{PetscBinaryIO.py}; we have helper script \texttt{plottrajectory.py}


\section{Time-dependent heat equation}

FIXME solve heat equation by implicit

\cinputpart{heat.c}{\CODELOC}{FIXME}{I}{//HEATCTX}{//ENDHEATCTX}{code:ts:heat:heatctx}

\cinputpart{heat.c}{\CODELOC}{FIXME}{II}{//RHSFUNCTION}{//ENDRHSFUNCTION}{code:ts:heat:rhsfunction}

\cinputpart{heat.c}{\CODELOC}{FIXME}{III}{//RHSJACOBIAN}{//ENDRHSJACOBIAN}{code:ts:heat:rhsjacobian}

\cinputpart{heat.c}{\CODELOC}{FIXME}{IV}{//TSSETUP}{//ENDTSSETUP}{code:ts:heat:tssetup}

\cinputpart{heat.c}{\CODELOC}{FIXME}{V}{//MONITOR}{//ENDMONITOR}{code:ts:heat:monitor}

\begin{figure}
% usage: \standardTSstack{scale}{Jacobian}{DMDA}{DMPlex}
\standardTSstack{0.775}{}{}{dashed}
\caption{The \PETSc stack used for the time-dependent heat and reaction/diffusion problems (\texttt{heat.c,pattern.c}).  Compare Figure \ref{fig:of:standardstack}.}
\label{fig:of:tsstack}
\end{figure}

FIXME show \texttt{-ts\_type theta,beuler,cn}


\section{Coupled reaction-diffusion equations}

FIXME see pages 21--22 of \citep{HundsdorferVerwer2003} and see \citep{Pearson1993}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{I}{//FIELDCTX}{//ENDFIELDCTX}{code:ts:pattern:fieldctx}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{II}{//RHSFUNCTION}{//ENDRHSFUNCTION}{code:ts:pattern:rhsfunction}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{III}{//IFUNCTION}{//ENDIFUNCTION}{code:ts:pattern:ifunction}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{IV}{//IJACOBIAN}{//ENDIJACOBIAN}{code:ts:pattern:ijacobian}

\cinputpart{pattern.c}{\CODELOC}{FIXME}{V}{//TSSETUP}{//ENDTSSETUP}{code:ts:pattern:tssetup}

FIXME show use of \texttt{-ts\_type arkimex,theta}

FIXME final-time greyscale plots using \texttt{PetscBinaryIO.py}


\section{Exercises}

\renewcommand{\labelenumi}{\arabic{chapter}.\arabic{enumi}\quad}
\renewcommand{\labelenumii}{(\alph{enumii})}
\begin{enumerate}
\item \label{exer:ts:tan}  Consider the scalar ODE initial value problem $y'=1+y^2$, $y(0)=0$.  Show by-hand that $y(t)=\tan t$ is the unique solution to this problem.  Modify \texttt{ode.c} to solve this problem.  Run the code from $t=0$ to $t=t_f=2$.  What run-time observable, actual evidence shows that your estimate of ``$y(2)$'' is meaningless?
\item FIXME which \texttt{-ts\_type} work with \texttt{ode.c}?  do \texttt{-help |grep ts\_type} to find all the possibilities, and try 
% euler, beuler, rk, theta, cn
\item FIXME confirm the claim in the text that the Jacobian is not used in explicit methods \texttt{-ts\_type euler,rk} by deleting \texttt{FormRHSJacobian()} from \texttt{ode.c}; with it still deleted check that \texttt{-ts\_type beuler,theta,cn} work when used in combination with \texttt{-snes\_fd} or \texttt{-snes\_mf}
\item FIXME \texttt{ode.c} says it is ``serial only''  what happens with \texttt{mpiexec -n 2 ./ode}, and why?
\item FIXME version of \texttt{ode.c} which solves DAE system
    $$\bbf(t,\by,\by') = \bg(t,\by)$$
where $\partial \bg/\partial \by'$ may be singular
\item FIXME for run like
\begin{cline}
./pattern -da_refine 6 -ptn_tf 500 -ptn_steps 100 -ts_monitor -snes_converged_reason
\end{cline}
which is fastest among these nine ARKIMEX and three $\theta$ methods?:
\begin{code}
-ts_type arkimex -ts_arkimex_type [a2|l2|ars122|2c|2d|2e|3|4|5]
-ts_type theta -ts_theta_endpoint -ts_theta_theta [0.5|0.75|1]
\end{code}
Are any other adaptive \pTS types faster?  Also compare final frames from \texttt{-ts\_monitor\_solution draw}; how worried should we be about the effect of time-stepping on the solution of this problem?
\end{enumerate}

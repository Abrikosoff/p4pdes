
\section{A code that does almost nothing, but in parallel}

The purpose of the \PETSc library is to help you solve scientific and engineering problems on multi-processor computers.  As \PETSc is built on top of the Message Passing Interface (MPI; \citep{Groppetal1999}) library, some of its flavor comes through.  Therefore we start with an introductory \PETSc code which calls MPI for some basic tasks.

This code \texttt{e.c}, shown in its entirety in Code \ref{code:e}, approximates Euler's constant
\begin{equation}
e = \sum_{n = 0}^\infty \frac{1}{n!} \approx 2.718281828 \label{introeseries}
\end{equation}
It does the computation in a distributed manner by computing one term of the infinite series on each process, giving a better estimate of $e$ when run on more MPI processes. While this is a silly use of \PETSc, it is an easy-to-understand parallel computation.

As with most C codes we include needed headers, but only \texttt{petsc.h} is needed because it includes MPI too.  Also, like any C program, \texttt{e.c} has a function called \texttt{main()} which takes inputs from the command line, namely \texttt{argc} and \texttt{argv}.  The former is an \texttt{int} holding the argument count and the latter is an array of strings---i.e.~\texttt{argv} is of type \texttt{char**}---holding the command line, including all arguments.  In all our codes we will simply pass these arguments to \PETSc through the \texttt{PetscInitialize()} which extracts options through this mechanism.  The \texttt{main()} function also outputs an \texttt{int}.  This return value is $0$ if the program succeeds, and so we see ``\texttt{return 0;}'' as the last line, but the error-related commands ``\texttt{CHKERRQ(ierr);}'' can also generate a non-zero return value from \texttt{main()}.  (We will return to this ``trace-back'' mechanism later in the Chapter.)

The substance of \texttt{e.c} is to declare some variables, do a computation on each process, and communicate the results between processes to get an estimate of $e$.  We will return to the parallel computation itself momentarily.

\inputwhole{../c/ch1/e.c}{\texttt{ch1/e.c}}{Compute $e$ with \PETSc.}{code:e}

Before we can compile and run \texttt{e.c}, \PETSc must be installed.  If it is not already available, go to
\begin{quote}
\href{http://www.mcs.anl.gov/petsc/download/index.html}{\texttt{www.mcs.anl.gov/petsc/download/}}
\end{quote}
to download the source code, and then follow the instructions at
\begin{quote}
\href{http://www.mcs.anl.gov/petsc/documentation/installation.html}{\texttt{www.mcs.anl.gov/petsc/documentation/installation.html}}
\end{quote}
to install.  Be sure to run ``\texttt{make test}'' and see it pass the tests.  When \PETSc is correctly installed the environment variables \texttt{PETSC\_DIR} and \texttt{PETSC\_ARCH} point to a valid installation, and the MPI command \texttt{mpiexec} is from the same MPI installation as was used in configuring \PETSc.\sidenote{Type ``\texttt{which mpiexec}'' to find which one you are running.  You may need to modify your \texttt{PATH} environment variable to get the right \texttt{mpiexec}.}

Do the following to compile \texttt{e.c}:
\begin{cline}
$ cd p4pdes/c/
$ cd ch1/
$ make e
\end{cline}
%$
Calling ``\texttt{make}'' uses \texttt{makefile} in the \texttt{ch1/} directory.  An extract of this is shown in Code \ref{code:ch1makefile}.  For all the codes in this book, the makefile has this form, as recommended in the \PETSc \emph{User's Manual} \citep{petsc-user-ref}.

Run the code like this:
\begin{cline}
$ ./e
e is about 1.000000000000000
rank 0 did 1 flops
\end{cline}
%$
The value $1.0$ is a very poor estimate of $e$, but this code does better with more MPI processes:
\begin{cline}
$ mpiexec -n 5 ./e
e is about 2.708333333333333
rank 0 did 0 flops
rank 4 did 3 flops
rank 2 did 1 flops
rank 3 did 2 flops
rank 1 did 0 flops
\end{cline}
%$
With $N=20$ processes, and thus $N=20$ terms in series \eqref{introeseries}, we get a good double-precision estimate:
\begin{cline}
$ mpiexec -n 20 ./e
e is about 2.718281828459045
rank 0 did 0 flops
...
\end{cline}
%$

``Double precision,'' of course, refers to the 64-bit representation of real numbers.  This type, which is normally aliased to \texttt{PetscReal} inside \PETSc codes, corresponds to about 15 decimal digit accuracy in scientific notation, i.e.~floating point representation \citep{TrefethenBau}.

\inputwhole{ch1makefile.frag}{extract from \texttt{ch1/makefile}}{All our \texttt{makefile}s for \PETSc examples look like this.}{code:ch1makefile}

Perhaps the reader is now worried that this book was written using a cluster with 20 physical processors whereas the reader has a little laptop with only a couple of cores.  Not so.  In fact these 5 and 20 process runs work just fine on the author's 4-core laptop.  MPI processes are created as needed, using an old feature of operating systems: multitasking.  Actual speedup from parallelism is another matter entirely, to which we will return.

The main job in \texttt{e.c} is to collect the partial sum of series \eqref{introeseries} onto each process.  Each process computes term $1/n!$, where $n$ is returned by \texttt{MPI\_Comm\_rank()}.  More precisely, \texttt{PETSC\_COMM\_WORLD} is an MPI communicator \citep{Groppetal1999} containing all processes generated by ``\texttt{mpiexec -n N}'' in the above calls, and $n$ is the rank of the process in this communicator.\sidenote{A call to \texttt{MPI\_Comm\_size()} would return \texttt{N}, the communicator's \emph{size}.}  A call to \texttt{MPI\_Allreduce()} does the sum and sends the result back to each process.  These direct uses of the MPI library are a part of using \PETSc, which generally avoids duplicating MPI functionality.

We print the computed estimate of $e$ once, but each process also prints its rank and the work it did.  The formatted print command \texttt{PetscPrintf()}, which acts like \texttt{fprintf()} from the C standard library, is called twice in the code.  The first time it uses MPI communicator \texttt{PETSC\_COMM\_WORLD} and the second time \texttt{PETSC\_COMM\_SELF}.  The first of these printing jobs is thus \emph{collective} over all processes, and thus only one line of output is produced, while the second is individual to each rank\sidenote{A process is often just called a \emph{rank} in MPI language.}  and we get \texttt{N} printed lines.  The \texttt{PETSC\_COMM\_SELF} lines can appear in apparently random order because the print occurs as soon as that rank reaches the \texttt{PetscPrintf()} command in the code.

\section{Every \PETSc program}

Every \PETSc program should start and end with the commands \texttt{PetscInitialize()} and \texttt{PetscFinalize()}:
\begin{code}
PetscInitialize(&argc,&args,NULL,help);
... everything else goes here ...
PetscFinalize();
\end{code}
As an argument to \texttt{PetscInitialize()} we supply a \texttt{help} string.  This string is a good place to say what is the purpose of the code.  To see the help string, and a longer list of possible \PETSc options, do:
\begin{cline}
$ ./e -help
\end{cline}
%$
or
\begin{cline}
$ ./e -help | less
\end{cline}
%$
Through \texttt{-help}, \PETSc programs have a built-in help system for runtime options that is both light-weight and surprisingly-effective.  For example, to see options related to logging performance, do
\begin{cline}
$ ./e -help | grep log_
\end{cline}
%$
We will see later how to add options to our own programs so that they will be documented in the same way.

Unfortunately with respect to their appearance, all our \PETSc example codes have error-checking clutter.  While languages other than C might help with decluttering, we are stuck with ugly lines that look like
\begin{code}
ierr = PetscCommand(...); CHKERRQ(ierr);
\end{code}
The explanation is that almost all \PETSc methods, and most user-written methods in \PETSc programs, return an \texttt{int} for error checking, with value $0$ if successful.  In the line above, \texttt{ierr} is passed to the \texttt{CHKERRQ()} macro.  It does nothing if \texttt{ierr == 0} but it stops the program with a ``traceback'' otherwise.\sidenote{A traceback is a list of the nested methods, in reverse order, showing the line numbers and method names of the location where the error occurred.  Traceback is most effective if \PETSc is configured with debugging symbols, i.e.~with the option \texttt{----with-debugging=1}.}  This traceback mechanism is a first line of defense when debugging run-time errors, so in this book we always capture-and-check the returned error code using the \texttt{CHKERRQ()} macro.  After this Chapter we will strip the ``\texttt{ierr =}'' and ``\texttt{CHKERRQ(ierr);}'' clutter from the code displayed in the text, but they are always present in the C sources in \texttt{p4pde/c/}.


\bigskip
\section{Exercises}

\renewcommand{\labelenumi}{\arabic{chapter}.\arabic{enumi}\quad}
\begin{enumerate}
\item Program \texttt{e.c} does redundant work, and a terrible job of load-balancing, because the computation of the factorial $n!$ on the rank $n$ process requires $n-1$ flops.  Modify the code to balance the load almost perfectly, with exactly one divide operation on each \texttt{rank} $>0$ process, by using blocking send and receive operations (\texttt{MPI\_Send(),MPI\_Recv()}) to pass the result of the last factorial to the next rank.  (\emph{Now the code does lots of unnecessary communication and waiting.})
% e1balanced.c

%from Matt Knepley in a petsc-users reply: "In parallel, a total order on summation is not guaranteed, and thus you will have jitter in the result."  Make this into a subsection "Another numerical fact of life, in parallel" and an exercise about modifying e.c to print lots of digits and see that number of processors affects result.  This code could add reading an option (PetscOptionReal to get a tolerance, with number of terms in sum to use) and getting number of processors (with MPI_Comm_size()) to do the sum with enough terms to get desired tolerance and with equal numbers of terms per processor.

\end{enumerate}

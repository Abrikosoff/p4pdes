
\section{A code that does almost nothing, but in parallel}

The purpose of the \PETSc library is to help you solve scientific and engineering problems on multi-processor computers.  As \PETSc is built on top of the Message Passing Interface (MPI; \citep{Groppetal1999}) library, some of its flavor comes through.  Therefore we start with an introductory \PETSc code which calls MPI for some basic tasks.

This code \texttt{e.c}, shown in its entirety in Code \ref{code:e}, approximates Euler's constant
\begin{equation}
e = \sum_{n = 0}^\infty \frac{1}{n!} \approx 2.718281828 \label{eq:gs:introseries}
\end{equation}
It does the computation in a distributed manner by computing one term of the infinite series on each process, giving a better estimate of $e$ when run on more MPI processes. While this is a silly use of \PETSc, it is an easy-to-understand parallel computation.

As with most C codes we include needed headers, but only \texttt{petsc.h} is needed because it includes MPI too.  Also, like any C program, \texttt{e.c} has a function called \texttt{main()} which takes inputs from the command line, namely \texttt{argc} and \texttt{argv}.  The former is an \texttt{int} holding the argument count and the latter is an array of strings---i.e.~\texttt{argv} is of type \texttt{char**}---holding the command line, including all arguments.  In all our codes we will simply pass these arguments to \PETSc through the \texttt{PetscInitialize()} which extracts options through this mechanism.  The \texttt{main()} function also outputs an \texttt{int}.  This return value is $0$ if the program succeeds, and so we see ``\texttt{return 0;}'' as the last line, but the error-related commands ``\texttt{CHKERRQ(ierr);}'' can also generate a non-zero return value from \texttt{main()}.  (We will return to this ``trace-back'' mechanism later in the Chapter.)

The substance of \texttt{e.c} is to declare some variables, do a computation on each process, and communicate the results between processes to get an estimate of $e$.  We will return to the parallel computation itself momentarily.

\inputwhole{../c/ch1/e.c}{\texttt{ch1/e.c}}{Compute $e$ with \PETSc.}{code:e}

Before we can compile and run \texttt{e.c}, \PETSc must be installed.  If it is not already available, go to
\begin{quote}
\href{http://www.mcs.anl.gov/petsc/download/index.html}{\texttt{www.mcs.anl.gov/petsc/download/}}
\end{quote}
to download the source code, and then follow the instructions at
\begin{quote}
\href{http://www.mcs.anl.gov/petsc/documentation/installation.html}{\texttt{www.mcs.anl.gov/petsc/documentation/installation.html}}
\end{quote}
to install.  Be sure to run ``\texttt{make test}'' and see it pass the tests.  When \PETSc is correctly installed the environment variables \texttt{PETSC\_DIR} and \texttt{PETSC\_ARCH} point to a valid installation, and the MPI command \texttt{mpiexec} is from the same MPI installation as was used in configuring \PETSc.\sidenote{Type ``\texttt{which mpiexec}'' to find which one you are running.  You may need to modify your \texttt{PATH} environment variable to get the right \texttt{mpiexec}.}

Do the following to compile \texttt{e.c}:
\begin{cline}
$ cd p4pdes/c/
$ cd ch1/
$ make e
\end{cline}
%$
Calling ``\texttt{make}'' uses \texttt{makefile} in the \texttt{ch1/} directory.  An extract of this is shown in Code \ref{code:ch1makefile}.  For all the codes in this book, the makefile has this form, as recommended in the \PETSc \emph{User's Manual} \citep{petsc-user-ref}.

Run the code like this:
\begin{cline}
$ ./e
e is about 1.000000000000000
rank 0 did 1 flops
\end{cline}
%$
The value $1.0$ is a very poor estimate of $e$, but this code does better with more MPI processes:
\begin{cline}
$ mpiexec -n 5 ./e
e is about 2.708333333333333
rank 0 did 0 flops
rank 4 did 3 flops
rank 2 did 1 flops
rank 3 did 2 flops
rank 1 did 0 flops
\end{cline}
%$
With $N=20$ processes, and thus $N=20$ terms in series \eqref{eq:gs:introseries}, we get a good double-precision estimate:
\begin{cline}
$ mpiexec -n 20 ./e
e is about 2.718281828459045
rank 0 did 0 flops
...
\end{cline}
%$

``Double precision,'' of course, refers to the 64-bit representation of real numbers.  This type, which is normally aliased to \texttt{PetscReal} inside \PETSc codes, corresponds to about 15 decimal digit accuracy in scientific notation, i.e.~floating point representation \citep{TrefethenBau1997}.

\inputwhole{ch1makefile.frag}{extract from \texttt{ch1/makefile}}{All our \texttt{makefile}s for \PETSc examples look like this.}{code:ch1makefile}

Perhaps the reader is now worried that this book was written using a cluster with 20 physical processors whereas the reader has a little laptop with only a couple of cores.  Not so.  In fact these 5 and 20 process runs work just fine on the author's 4-core laptop.  MPI processes are created as needed, using an old feature of operating systems: multitasking.  Actual speedup from parallelism is another matter entirely, to which we will return.

The main job in \texttt{e.c} is to collect the partial sum of series \eqref{eq:gs:introseries} onto each process.  Each process computes term $1/n!$, where $n$ is returned by \texttt{MPI\_Comm\_rank()}.  More precisely, \texttt{PETSC\_COMM\_WORLD} is an MPI communicator \citep{Groppetal1999} containing all processes generated by ``\texttt{mpiexec -n N}'' in the above calls, and $n$ is the rank of the process in this communicator.\sidenote{A call to \texttt{MPI\_Comm\_size()} would return \texttt{N}, the communicator's \emph{size}.}  A call to \texttt{MPI\_Allreduce()} does the sum and sends the result back to each process.  These direct uses of the MPI library are a part of using \PETSc, which generally avoids duplicating MPI functionality.

We print the computed estimate of $e$ once, but each process also prints its rank and the work it did.  The formatted print command \texttt{PetscPrintf()}, which acts like \texttt{fprintf()} from the C standard library, is called twice in the code.  The first time it uses MPI communicator \texttt{PETSC\_COMM\_WORLD} and the second time \texttt{PETSC\_COMM\_SELF}.  The first of these printing jobs is thus \emph{collective} over all processes, and thus only one line of output is produced, while the second is individual to each rank\sidenote{A process is often just called a \emph{rank} in MPI language.}  and we get \texttt{N} printed lines.  The \texttt{PETSC\_COMM\_SELF} lines can appear in apparently random order because the print occurs as soon as that rank reaches the \texttt{PetscPrintf()} command in the code.


\section{Every \PETSc program}

Every \PETSc program should start and end with the commands \texttt{PetscInitialize()} and \texttt{PetscFinalize()}:
\begin{code}
PetscInitialize(&argc,&args,NULL,help);
... everything else goes here ...
PetscFinalize();
\end{code}
As an argument to \texttt{PetscInitialize()} we supply a \texttt{help} string.  This string is a good place to say what is the purpose of the code.  To see the help string, and a longer list of possible \PETSc options, do:
\begin{cline}
$ ./e -help
\end{cline}
%$
or
\begin{cline}
$ ./e -help | less
\end{cline}
%$
Through \texttt{-help}, \PETSc programs have a built-in help system for runtime options that is both light-weight and surprisingly-effective.  For example, to see options related to logging performance, do
\begin{cline}
$ ./e -help | grep log_
\end{cline}
%$
We will see later how to add options to our own programs so that they will be documented in the same way.

Unfortunately with respect to their appearance, all our \PETSc example codes have error-checking clutter.  While languages other than C might help with decluttering, we are stuck with ugly lines that look like
\begin{code}
ierr = PetscCommand(...); CHKERRQ(ierr);
\end{code}
The explanation is that almost all \PETSc methods, and most user-written methods in \PETSc programs, return an \texttt{int} for error checking, with value $0$ if successful.  In the line above, \texttt{ierr} is passed to the \texttt{CHKERRQ()} macro.  It does nothing if \texttt{ierr == 0} but it stops the program with a ``traceback'' otherwise.\sidenote{A traceback is a list of the nested methods, in reverse order, showing the line numbers and method names of the location where the error occurred.  Traceback is most effective if \PETSc is configured with debugging symbols, i.e.~with the option \texttt{----with-debugging=1}.}  This traceback mechanism is a first line of defense when debugging run-time errors, so in this book we always capture-and-check the returned error code using the \texttt{CHKERRQ()} macro.  After this Chapter we will strip the ``\texttt{ierr =}'' and ``\texttt{CHKERRQ(ierr);}'' clutter from the code displayed in the text, but they are always present in the C sources in \texttt{p4pde/c/}.


\section{Parallel reductions are generally not bit-wise repeatable}

The code \texttt{e.c} does a finite sum in parallel, namely an $N$-term truncation of \eqref{eq:gs:introseries} on  $N$ processors, by a call to \texttt{MPI\_Allreduce()}.  Such parallel reduction operations are also key steps in linear algebra algorithms like GMRES which we will use throughout, starting in Chapter \ref{chap:ls}.  Floating-point arithmetic is not, however, commutative or associative, though nearly so.\sidenote{See \citep{Higham2002,TrefethenBau1997} for theory.  See Exercise \ref{chap:gs}.\ref{exer:gs:nondeterminant} for an example of non-commutativity.}

When done in parallel a total order on the sum is not pre-determined,\sidenote{One can suppose when using \texttt{MPI\_Allreduce()} for summation that the terms are collected on the rank $0$ processor and that they are added to a running sum as they arrive; this is the stage at which the non-determinacy applies.  After the sum is done on rank $0$ the result is sent (``scattered'') to all other ranks, without further floating-point consequences.  See \citep{Groppetal1999}.} and this is similarly true for other parallel reductions like products.  Such non-determinacy coming from the order of operations done in a parallel reduction is well-known; see for example the entry on ``determinacy'', and on ``associative non-determinacy'' in particular, in \citep{Padua2011}.  Because of this basic fact a seemingly-deterministic program like \texttt{e.c} could, at least in theory, have a different outcome at the level of rounding error when run a second time in parallel.  Parallel reductions are not bit-wise deterministic.

Though the fact of non-determinism of parallel reductions may never bite the reader, it is a numerical fact of life.  \emph{Stable} algorithms, in particular backward stable ones \citep{TrefethenBau1997}, do not suffer large output changes between runs as a result of this fact.

Demonstrating the non-determinacy on a simple sum like \eqref{eq:gs:introseries} is difficult because it helps to have many processors\sidenote{The example on page 563 of \citep{Padua2011} proposes 10000 processes.} \emph{and} significant noise in the interconnect between processors, so that terms are collected in a different order on each run.  These conditions are hard to achieve using software MPI processes on a small (low core count) machine, so Exercise \ref{chap:gs}.\ref{exer:gs:nondeterminant} simply demonstrates the non-commutativity of the addition operation in a serial run.

Looking ahead, Chapter \ref{chap:ls} introduces two related, but more important, facts of life.  The first relates to floating-point arithmetic, namely the concept of conditioning of linear systems.  Conditioning is intimately related to the stability of algorithms \citep{TrefethenBau1997}, but it is not (fundamentally) related to parallel computations.  Preconditioning of linear systems\sidenote{``Preconditioning'' is defined in Chapter \ref{chap:ls} and it continues as a major topic throughout this book.} is the second related fact-of-life.  The parallel application of many of these preconditioning methods is dependent on the number of processors, and \emph{this} dependence is very likely will be seen by the reader.  However, if floating point calculations were exact then bit-wise repeatability would apply to preconditioner applications where the number of processors is fixed.  The unifying theme is that both the processor-count dependence of preconditioner methods and the conditioning of problems drives us to be concerned with the stability, and not just the efficiency, of the algorithms implemented in \PETSc.


\bigskip
\section{Exercises}

\renewcommand{\labelenumi}{\arabic{chapter}.\arabic{enumi}\quad}
\begin{enumerate}

\item \label{exer:gs:nondeterminant}  \emph{Demonstrating the non-commutativity of floating-point addition is an excuse to practice a bit more \PETSc programming in a serial (single-process) context.}  A code called \texttt{shuffle.c}, in subdirectory \texttt{c/ch1/}, permutes the order of the elements stored in a one-dimensional C array and prints the result (not shown).  Look at this code and see how it uses the \PETSc options mechanism, to which we will return, and a \PETSc random number generator; note that the current time in seconds is used to seed the latter.  Modify it to create a code \texttt{lntwo.c} which does a $N$-term truncation (partial sum) of the slowly-converging infinite series
\begin{equation}
\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n} = \ln 2.
\label{eq:gs:lntwo}
\end{equation}
The new code will do the finite sum by computing all the terms in an array, then randomly permuting the array, and then summing.  The summation can be done by any convenient loop; no \texttt{MPI\_Allreduce()} is needed.  By doing repeated runs with one MPI process but $N=1000$ terms, show that that the last couple of decimal digits, in a 16 significant digit display, vary from run to run.  If we did permuted partial sums of series \eqref{eq:gs:introseries} would we show the same variation?

\item \label{exer:gs:balance} Program \texttt{e.c} does redundant work, and a terrible job of load-balancing, because the computation of the factorial $n!$ on the rank $n$ process requires $n-1$ flops.  Modify \texttt{e.c} to a new code \texttt{balance.c} which balances the load almost perfectly, giving one divide operation on each \texttt{rank} $>0$ process.  Use blocking send and receive operations (\texttt{MPI\_Send(),MPI\_Recv()}) to pass the result of the last factorial to the next rank.  Note that now the code does lots of unnecessary communication and waiting, so neither \texttt{e.c} nor \texttt{balance.c} are good examples for future use.

\end{enumerate}

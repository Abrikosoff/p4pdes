
\chapter{1. Getting started with PETSc}

\section{A code that does almost nothing}

The purpose of the \PETSc library is to help you solve scientific and engineering problems, like PDEs, on distributed computers.  But \PETSc is built ``on top of'' the Message Passing Interface (MPI; \citep{Groppetal1999}) library, and some of the flavor of that library comes through in examples.  We start with an example C code \texttt{c1e.c} in Figure \ref{code:e} that is more-or-less just a introductory MPI example, but it does call \PETSc for some of the basic tasks.

\inputwhole{../c/c1e.c}{\texttt{p4pdes/c/c1e.c}}{Compute $e$ in parallel.}{code:e}

This code computes Euler's constant
  $$e = \sum_{n = 0}^\infty \frac{1}{n!} \approx 2.718281828$$
in parallel.  It does the computation in a very distributed manner, by computing one term of the infinite series on each process.  Thus it computes a better estimate of $e$ if run on many MPI processes.\sidenote{It's kind of silly.}

As with any C source that compiles into an executable, its overall structure is just a function called \texttt{main()}.  This function takes inputs from the command line, \texttt{argc}, the argument count, and \texttt{argv}, the arguments themselves.  It outputs an \texttt{int} which is $0$ if the program succeeds.  Like all C codes we include the needed libraries---only \texttt{petscsys.h} is needed here.  Then we declare some variables, call various methods, and return $0$ at the end.

Additional features of \texttt{c1e.c} will be worth describing once we compile and run our code.  Please do the following to start:
\begin{Verbatim}[fontsize=\small]
$ cd p4pdes/c/  # this book is distributed by
                #     "git clone https://github.com/bueler/p4pdes.git"
                #     or similar
$ make c1e
\end{Verbatim}
For the command ``\texttt{make}'' to work there must be a makefile, and there is one.  The relevant part of it looks like Figure \ref{code:c1emakefile}.  For all of the \PETSc examples in this book, the makefile will have this form, exactly as recommended in the \PETSc User's Manual \citep{petsc-user-ref}.

\cinputraw{c1emakefile.frag}{extract from \texttt{p4pdes/c/makefile}}{All \texttt{makefile}s for the \PETSc codes in this book like like this.}{}{//START}{//STOP}{code:c1emakefile}

Run the code on one MPI process like this:
\begin{Verbatim}[fontsize=\small]
$ ./c1e
e is about 1.000000000000000
rank 0 did 1 flops
\end{Verbatim}
The value $1.000\dots$ is a very poor estimate of $e$, but this code does better with more processes:
\begin{Verbatim}[fontsize=\small]
$ mpiexec -n 5 ./c1e
rank 3 did 7 flops
rank 4 did 9 flops
e is about 2.708333333333333
rank 0 did 1 flops
rank 1 did 3 flops
rank 2 did 5 flops
\end{Verbatim}
That's a better estimate of $e$, but hardly impressive.  On the other hand, with $N=20$ processes we get a good estimate:
\begin{Verbatim}[fontsize=\small]
$ mpiexec -n 20 ./c1e
rank 9 did 19 flops
...
e is about 2.718281828459045
rank 0 did 1 flops
...
rank 18 did 37 flops
\end{Verbatim}

Now, perhaps the reader is worried that this book was written using a large supercomputer whereas the reader has a little laptop with a coupld of cores.  Not so; these $N=5$ and $N=20$ process runs can be done just fine on your little laptop or mine.  MPI creates processes as needed, and indeed operating systems have been multitasking for quite some time.

An interesting aspect of the code and output is that there are two printing jobs, one of which is \emph{collective} over all processes---i.e.~\emph{ranks}, in MPI language---and one of which is individual to each rank.  In the code in Figure \ref{code:e} we see the difference between \texttt{PetscPrintf()}, a formatted print command, called with either the MPI communicator \texttt{PETSC\_COMM\_WORLD} or \texttt{PETSC\_COMM\_SELF}, respectively.  In this program each MPI process (i.e.~rank) report its rank and the work it did as soon as it is done, and thus these lines appear in almost random order.  Note that we get the rank of the current process with a call \texttt{MPI\_Comm\_rank()} directly to the MPI library; when a job \emph{can} be done by MPI, \PETSc will leave it that way.

The main job, however, is to collect the terms in the (truncated) infinite series onto each process, and then print this estimate of $e$ just once.  Thus the code is basically structured as a computation of $1/n!$ on each rank $n$, a call to \texttt{MPI\_Allreduce()} to sum all contributions onto all processes, and then a collective print statement giving the value of $e$.

This is about all there is to say, except that every \PETSc program should start and end with the commands \texttt{PetscInitialize()} and \texttt{PetscFinalize()}:
\begin{Verbatim}[fontsize=\small]
  PetscInitialize(&argc,&args,(char*)0,help);  // <-- always call
  ... everything else goes here ...
  PetscFinalize();  // <-- always call
\end{Verbatim}
Also, so that this \PETSc code can provide useful usage help, we add a \texttt{help} string at the start; it is a good place to say what the purpose of the code is.


\section{A bit of linear algebra}

At the core of most \PETSc computations is a finite-dimensional linear system.  Before giving any of the details for solving such systems in \PETSc, it is useful to recall the most basic ideas of numerical linear algebra.

Suppose $\bb\in \RR^{N}$ is a column vector and $A\in\RR^{N\times N}$ is a square matrix.  The linear system
\begin{equation}
A \bu = \bb \label{introsystem}
\end{equation}
has a unique solution if $A$ is invertible, namely
\begin{equation}
\bu = A^{-1} \bb. \label{introsolution}
\end{equation}
This is simple in theory.

It is not so simple in practice, however, when solving large systems on a computer.  There are two key facts to keep in mind while working numerically  \citep{TrefethenBau}.
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
\item \emph{limit to accuracy}:  If real numbers are represented on the computer with machine precision $\eps$ then the solution of \eqref{introsystem} can only be computed within an error $\kappa(A) \eps$ where $\kappa(A) = \|A\|_2/\|A^{-1}\|_2$ is the (2-norm) \emph{condition number} of $A$.\sidenote{Fact i) is about \emph{conditioning} not \emph{methods}.  Informally speaking, there are linear systems $A,\bb$ that are the same to within $\eps$ but for which the infinite-precision solutions $\bu$ are different by the amount $\kappa(A) \eps$.}
\item \emph{cost of direct solutions}:  Computation of solution \eqref{introsolution} by a direct method like Gauss elimination, whether actually forming $A^{-1}$ or not, is an $O(N^3)$ operation.
\end{enumerate}

For a sense of the consequences of these facts, let's put in some numbers.  On most computers the precision for the C \texttt{double} type, the modern default 64-bit representation of real numbers, is $\eps = 2.2 \times 10^{-16}$.  By i), a linear system having $\kappa(A) \approx 10^{10}$, for example, can only be solved to about six digits of precision.  Because of ii), a linear system with $N=10^6$ equations\sidenote{We will do problems of this size on a single processor in a few seconds, and in $O(N)$ operations, in Chapter 4.}
%time ./c4poisson -da_refine 7 -ksp_type cg -pc_type mg
%on 1153 x 1153 grid:  iterations 2, residual norm = 1.88082e-05
%real 11.18
requires $\sim 10^{18}$ operations to solve by Gauss elimination.  Even modern processors take a while to do a quintillion operations.

Many methods other than Gauss elimination are possible for solving linear systems, and this is a topic we will return to in Chapters 2, 4, and 5 especially.  Usually these methods are iterative, and they often use the concept of \emph{residual}.  If $\bu_0\in \RR^N$ is a vector, then by definition the residual for $\bu_0$. as an estimate of the solution to equation \eqref{introsystem}, is the vector
\begin{equation}
\br_0 = \bb - A \bu_0. \label{residualdefn}
\end{equation}

Evaluating the residual for a known vector $\bu_0$ requires only applying $A$ to it, an $O(N^2)$ operation in general, plus a vector subtraction.  However, most discretization schemes for PDEs generate system matrices $A$ that are sparse, meaning simply that they have many more zero entries than nonzeros.  Also these matrices are likely to have a pattern to their nonzero entries in some sense.  In this case the operation $A\bu_0$ can often be implemented in $O(N)$ cost.

The \emph{Richardson iteration} is an example of an iterative method based on the residual.  If $\bu_0$ is an initial estimate of the solution then the iteration simply adds a multiple of the residual to get the next iterate,
\begin{equation}
\bu_{k+1} = \bu_k + \omega (\bb - A \bu_k),  \label{introrichardson}
\end{equation}
where $\omega\in\RR$ is called the ``weight.''  Each step of this iteration is $O(N)$ work if the matrix-vector product $A\bu_k$ can be computed in $O(N)$ work.  If significantly fewer than $O(N^2)$ steps are needed to make $\bu_k$ an adequately-good approximation of the exact solution $\bu$, then the Richardson iteration is an improvement on Gauss elimination.  On the other hand, the Richardson iteration may not even converge.  If we write \eqref{introrichardson} as $\bu_{k+1} = (I - \omega A) \bu_k + \omega \bb$ then it is easy to see that the ``size'' of the matrix $I-\omega A$ will determine whether $\lim_{k\to\infty} \bu_k$ exists.

To examine questions like the convergence of the Richardson iteration, we recall the definitions of \emph{eigenvalue} and \emph{singular value}.  A complex number $\lambda \in \CC$ is an eigenvalue of $A$ if there is a nonzero vector $\bv\in\RR^N$ so that $A \bv = \lambda \bv$.  The set of all eigenvalues of $A$ is the \emph{spectrum} $\sigma(A)$ of $A$.  The singular values are the square roots of the eigenvalues of the matrix $A^*A$.\sidenote{The matrix $A^*A$ is symmetric and positive-definite so its eigenvalues are nonnegative, and thus have square roots.}  Singular values are also geometrically-defined as the lengths of semi-axes of the ellipsoid in $\RR^N$ that results from applying $A$ to all vectors in the unit sphere of $\RR^N$ \citep{TrefethenBau}.  Properties of $A$ that can be described in terms of its eigenvalues or singular values are generically called ``spectral properties'' of $A$.

Note that $\|A\|_2$ is equal to the largest singular value of $A$, while $\|A^{-1}\|_2$ is equal to the inverse of the smallest singular value of $A$.  Thus $\kappa(A)$ is the ratio of largest to smallest singular values of $A$, and is well-visualized as the eccentricity of the just-mentioned ellipsoid.  The condition number of $A$ is thus a spectral property.

It is an easy exercise to show that the Richardson iteration \eqref{introrichardson} will converge if and only if all the eigenvalues of $I-\omega A$ are inside the unit circle.  Defining the \emph{spectral radius} $\rho(A)$ of a matrix $A$ as the maximum norm of the eigenvalues of $A$, we can describe the convergence of the Richardson iteration this way:
\begin{equation}
\text{\eqref{introrichardson} converges if and only if } \rho(I-\omega A) < 1. \label{introconvergethm}
\end{equation}

Other examples of iterative methods include the classical Jacobi and Gauss-Siedel iterations \citep{Greenbaum1997}, but the really powerful ones proceed by iteratively generating an \emph{optimal}, in some sense, estimate $\bu_k$ which is a linear combination of vectors $\bb,A\bb,A^2\bb,\dots,A^{k-1}\bb$, and these methods are collectively called \emph{Krylov space methods} because the span of these vectors is a Krylov space.  Typically the sense of ``optimal'' also depends on the eigenvalues or singular values of $A$.  We use Krylov space methods in Chapter 2 and all later Chapters.

Returning to non-numerical linear algebra for a moment, note that there are many other systems which are equivalent to \eqref{introsystem}.  In fact, if $P\in\RR^{N\times N}$ is an invertible square matrix then the systems
\begin{equation}
(P^{-1} A) \bu = P^{-1} \bb \label{introleftpre}
\end{equation}
and
\begin{equation}
(A P^{-1}) (P\bu) = \bb \label{introrightpre}
\end{equation}
obviously have the same solution $\bu$ as \eqref{introsystem}.

Compared to $A$, the matrices $P^{-1} A$ or $A P^{-1}$ may have very different eigenvalues, condition numbers, and so on.  While the accuracy of the solution $\bu$ cannot be improved beyond the $\kappa(A) \eps$ level---fact i) cannot be overcome in that sense---methods can indeed take advantage of better conditioning or other spectral properties to generate an approximate solution more quickly.  Especially if $P^{-1}$ is easy to apply\sidenote{I.e.~if a system $P\bv = \bc$ is easy to solve for $\bv$ in the sense of low computational cost.} then this can be an advantageous idea when trying to approximate $\bu$ quickly.  Equivalent systems \eqref{introleftpre} and \eqref{introrightpre} are referred to as \emph{preconditioned} systems, with the former called \emph{left preconditioning} and the latter \emph{right-preconditioning}.

Now we conclude this little bit of numerical linear algebra by giving a small example of the role of preconditioning in making the simple Richardson iteration converge.  This small example is to help make the idea of preconditioning, to improve spectral properties, concrete for the reader.  Many methods far superior to the Richardson iteration are implemented in \PETSc, and we will use the best ones.

\medskip\noindent\hrulefill
\begin{example} Consider the linear system
\begin{equation}
A \bu =
\begin{bmatrix}
10 & -1 \\ -1 & 1
\end{bmatrix} 
\begin{bmatrix}
u_1 \\ u_2
\end{bmatrix}
=
\begin{bmatrix}
8 \\
1 
\end{bmatrix}
= \bb
 \label{introexample}
\end{equation}
which has solution $\bu = [1\,\, 2]^*$.  If we start with estimate $\bu_0 = [0\,\, 0]^*$ then the unweighted ($\omega=1$) Richardson iteration \eqref{introrichardson} gives a sequence of vectors % see ../matlab/richardsonex.m
\newcommand{\rvect}[3]{\ensuremath{\bu_{#1} = \begin{bmatrix} #2 \\ #3 \end{bmatrix}}}
\begin{equation}
\rvect{0}{0}{0}, \rvect{1}{8}{1}, \rvect{2}{-63}{9}, \rvect{3}{584}{-62}, \dots
\end{equation}
This sequence is not heading toward the solution $\bu = [1\,\, 2]^*$.  Suppose, however, that we use the diagonal matrix from system \eqref{introexample} to build $P$:
\begin{equation}
P = \begin{bmatrix}
10 & 0 \\ 0 & 1
\end{bmatrix}.  \label{introP}
\end{equation}
Being diagonal, this $P$ is very easy to invert and apply.  The preconditioned Richardson iteration using $P$, namely
\begin{equation}
\bu_{k+1} = \bu_k + \omega (P^{-1} \bb - P^{-1} A \bu_k),  \label{introprerichardson}
\end{equation}
is much better behaved.  With $\bu_0 = [0\,\, 0]^*$ again we get this sequence from \eqref{introprerichardson}:
\begin{equation}
\rvect{0}{0}{0}, \rvect{1}{0.8}{1.0}, \rvect{2}{0.9}{1.8}, \rvect{3}{0.98}{1.90}, \dots
\end{equation}
This sequence is apparently going to $\bu = [1\,\, 2]^*$.  Of course, the explanation is not hard to see; in this case
\begin{equation}
\rho(I-A) = -9.1, \qquad \rho(I-P^{-1} A) = 0.32,
\end{equation}
so our convergence claim \eqref{introconvergethm} matches the evidence.
\end{example}
\noindent\hrulefill


\section{\pVecs and \pMats}

FIXME: introduce/illustrate object Create/Set/SetFromOptions/<use>/Destroy

FIXME: parallel layout of \pVec and \pMat


\section{A linear system in \PETSc}

\cinputpartnostrip{c1matvec.c}{Initialize \PETSc and allocate \pVec s and \pMat.}{I}{}{//ENDSETUP}{code:matvecpartone}

\cinputpartnostrip{c1matvec.c}{Assemble \pMat $A$ and right-hand side $b$.}{II}{//ENDSETUP}{//ENDASSEMBLY}{code:matvecparttwo}

\cinputpartnostrip{c1matvec.c}{Set up \pKSP.  Solve.  Finalize.}{III}{//ENDASSEMBLY}{//END}{code:matvecpartthree}

FIXME: gloss PETSc traceback mechanism, before noting we don't show that stuff


\caveat{But \Matlab is all you want if scale does not matter.}



\chapter{Getting started with PETSc}
\label{chap:getstarted}

\section{A code that does almost nothing, but in parallel}

The purpose of the \PETSc library is to help you solve scientific and engineering problems, like PDEs, on distributed computers.  But \PETSc is built ``on top of'' the Message Passing Interface (MPI; \citep{Groppetal1999}) library, and some of the flavor of that library comes through in examples.  We start with an example C code that is more-or-less just a introductory MPI example, but which calls \PETSc for some basic tasks.

This code \texttt{c1e.c}, shown in Figure \ref{code:e}, computes Euler's constant
\begin{equation}
e = \sum_{n = 0}^\infty \frac{1}{n!} \approx 2.718281828 \label{introeseries}
\end{equation}
in parallel.  It does the computation in a very distributed manner, by computing one term of the infinite series on each process.  Thus it computes a better estimate of $e$ when run on more MPI processes.  In other words, this is a silly use of \PETSc, but an easy to understand parallel computation.

As with any C source for an executable, in Figure \ref{code:e} we have a function called \texttt{main()}.  It takes inputs from the command line, \texttt{argc} and \texttt{argv},\sidenote{Here \texttt{argc} is an \texttt{int} with the argument count and \texttt{argv} are the arguments themselves as an array of strings (i.e.~C type \texttt{char**}).  But in this code, and all other codes in this book, we simply pass these arguments on to \PETSc through the \texttt{PetscInitialize()} method.}  and it outputs an \texttt{int} which is $0$ if the program succeeds.  Like all C codes we include the needed headers; only \texttt{petscsys.h} is needed here, but later codes will include other \PETSc headers.  Then we declare some variables, do a computation on each process, communicate the results between processes to get an estimate of $e$.  Finally we report success (i.e.~return $0$) at the end.

More details of \texttt{c1e.c} can be described once we compile and run the code.  Please do the following to start:
\begin{code}
$ cd p4pdes/c/  # download this book, and its codes in p4pdes/c/, by
                #     "git clone https://github.com/bueler/p4pdes.git"
$ make c1e
\end{code}
For the command ``\texttt{make}'' to work there must be a makefile, of course, and it is \texttt{/p4pdes/c/makefile}, shown in Figure \ref{code:c1emakefile}.  For all the codes in this book, the makefile will have this form, exactly as recommended in the \PETSc User's Manual \citep{petsc-user-ref}.

\inputwhole{../c/c1e.c}{\texttt{p4pdes/c/c1e.c}}{Compute $e$ in parallel.}{code:e}

Run the code on one MPI process like this:
\begin{code}
$ ./c1e
e is about 1.000000000000000
rank 0 did 1 flops
\end{code}
The value $1.0$ is a very poor estimate of $e$, but this code does better with more processes:
\begin{code}
$ mpiexec -n 5 ./c1e
rank 3 did 7 flops
rank 4 did 9 flops
e is about 2.708333333333333
rank 0 did 1 flops
rank 1 did 3 flops
rank 2 did 5 flops
\end{code}
That's a better estimate of $e$, but hardly impressive.  On the other hand, with $N=20$ processes, and thus $N=20$ terms in series \eqref{introeseries}, we get a good estimate:
\begin{code}
$ mpiexec -n 20 ./c1e
rank 9 did 19 flops
...
e is about 2.718281828459045
rank 0 did 1 flops
...
rank 18 did 37 flops
\end{code}

\cinputraw{c1emakefile.frag}{extract from \texttt{p4pdes/c/makefile}}{All \texttt{makefile}s for the \PETSc codes in this book look like this.}{}{//START}{//STOP}{code:c1emakefile}

Now, perhaps the reader is worried that this book was written using a large supercomputer whereas the reader has a little laptop with a coupld of cores.  Not so; these $N=5$ and $N=20$ process runs can be done just fine on your little laptop or mine.  MPI creates processes as needed, and indeed operating systems have been multitasking for quite some time.

An interesting aspect of the code and output is that there are two printing jobs, one of which is \emph{collective} over all processes---i.e.~\emph{ranks}, in MPI language---and one of which is individual to each rank.  In the code in Figure \ref{code:e} we see the difference between \texttt{PetscPrintf()}, a formatted print command, called with either the MPI communicator \texttt{PETSC\_COMM\_WORLD} or \texttt{PETSC\_COMM\_SELF}, respectively.  In this program each MPI process (i.e.~rank) report its rank and the work it did as soon as it is done, and thus these lines appear in almost random order.  Note that we get the rank of the current process with a call \texttt{MPI\_Comm\_rank()} directly to the MPI library; when a job \emph{can} be done by MPI, \PETSc will leave it that way.

The main job, however, is to collect the terms in the (truncated) infinite series onto each process, and then print this estimate of $e$ just once.  Thus the code is basically structured as a computation of $1/n!$ on each rank $n$, a call to \texttt{MPI\_Allreduce()} to sum all contributions onto all processes, and then a collective print statement giving the value of $e$.

This is about all there is to say, except that every \PETSc program should start and end with the commands \texttt{PetscInitialize()} and \texttt{PetscFinalize()}:
\begin{code}
  PetscInitialize(&argc,&args,(char*)0,help);  // <-- always call
  ... everything else goes here ...
  PetscFinalize();  // <-- always call
\end{code}
Also, so that this \PETSc code can provide useful usage help, we add a \texttt{help} string at the start; it is a good place to say what the purpose of the code is.


\section{A bit of linear algebra}

At the core of most \PETSc computations is a finite-dimensional linear system.  Before giving any of the details for solving such systems in \PETSc, it is useful to recall the most basic ideas of numerical linear algebra.

Suppose $\bb\in \RR^{N}$ is a column vector and $A\in\RR^{N\times N}$ is a square matrix.  The linear system
\begin{equation}
A \bu = \bb \label{introsystem}
\end{equation}
has a unique solution if $A$ is invertible, namely
\begin{equation}
\bu = A^{-1} \bb. \label{introsolution}
\end{equation}
This is simple in theory.

It is not so simple in practice, however, when solving large systems on a computer.  There are two key facts to keep in mind while working numerically  \citep{TrefethenBau}.
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
\item \emph{limit to accuracy}:  If real numbers are represented on the computer with machine precision $\eps$ then the solution of \eqref{introsystem} can only be computed within an error $\kappa(A) \eps$ where $\kappa(A) = \|A\|_2 \|A^{-1}\|_2$ is the (2-norm) \emph{condition number} of $A$.\sidenote{Fact i) is about \emph{conditioning} not \emph{methods}.  Informally speaking, there are linear systems $A,\bb$ that are the same to within $\eps$ but for which the infinite-precision solutions $\bu$ are different by the amount $\kappa(A) \eps$.}
\item \emph{cost of direct solutions}:  Computation of solution \eqref{introsolution} by a direct method like Gauss elimination, whether actually forming $A^{-1}$ or not, is an $O(N^3)$ operation.
\end{enumerate}

For a sense of the consequences of these facts, let's put in some numbers.  On most computers the precision for the C \texttt{double} type, the modern default 64-bit representation of real numbers, is $\eps = 2.2 \times 10^{-16}$.  By i), a linear system having $\kappa(A) \approx 10^{10}$, for example, can only be solved to about six digits of precision.  Because of ii), a linear system with $N=10^6$ equations\sidenote{We will do problems of this size on a single processor in a few seconds, and in $O(N)$ operations, in Chapter \ref{chap:multigrid}.}
%time ./c4poisson -da_refine 7 -ksp_type cg -pc_type mg
%on 1153 x 1153 grid:  iterations 2, residual norm = 1.88082e-05
%real 11.18
requires $\sim 10^{18}$ operations to solve by Gauss elimination.  Even modern processors take a while to do a quintillion operations.

Many methods other than Gauss elimination are possible for solving linear systems, and this is a topic we will return to several times in later Chapters.  Usually these methods are iterative, and they often use the concept of \emph{residual}.  If $\bu_0\in \RR^N$ is a vector, then by definition the residual for $\bu_0$. as an estimate of the solution to equation \eqref{introsystem}, is the vector
\begin{equation}
\br_0 = \bb - A \bu_0. \label{residualdefn}
\end{equation}

Evaluating the residual for a known vector $\bu_0$ requires only applying $A$ to it, an $O(N^2)$ operation in general, plus a vector subtraction.  However, most discretization schemes for PDEs generate system matrices $A$ that are sparse, meaning simply that they have many more zero entries than nonzeros.  Also these matrices are likely to have a pattern to their nonzero entries in some sense.  In this case the operation $A\bu_0$ can often be implemented in $O(N)$ cost.

The \emph{Richardson iteration} is an example of an iterative method based on the residual.  If $\bu_0$ is an initial estimate of the solution then the iteration simply adds a multiple of the residual to get the next iterate,
\begin{equation}
\bu_{k+1} = \bu_k + \omega (\bb - A \bu_k),  \label{introrichardson}
\end{equation}
where $\omega\in\RR$ is called the ``weight.''  Each step of this iteration is $O(N)$ work if the matrix-vector product $A\bu_k$ can be computed in $O(N)$ work.  If significantly fewer than $O(N^2)$ steps are needed to make $\bu_k$ an adequately-good approximation of the exact solution $\bu$, then the Richardson iteration is an improvement on Gauss elimination.  On the other hand, the Richardson iteration may not even converge.  If we write \eqref{introrichardson} as $\bu_{k+1} = (I - \omega A) \bu_k + \omega \bb$ then it is easy to see that the ``size'' of the matrix $I-\omega A$ will determine whether $\lim_{k\to\infty} \bu_k$ exists.

To examine questions like the convergence of the Richardson iteration, we recall the definitions of \emph{eigenvalue} and \emph{singular value}.  A complex number $\lambda \in \CC$ is an eigenvalue of $A$ if there is a nonzero vector $\bv\in\RR^N$ so that $A \bv = \lambda \bv$.  The set of all eigenvalues of $A$ is the \emph{spectrum} $\sigma(A)$ of $A$.  The singular values are the square roots of the eigenvalues of the matrix $A^*A$.\sidenote{The matrix $A^*A$ is symmetric and positive-definite so its eigenvalues are nonnegative, and thus have square roots.}  Singular values are also geometrically-defined as the lengths of semi-axes of the ellipsoid in $\RR^N$ that results from applying $A$ to all vectors in the unit sphere of $\RR^N$ \citep{TrefethenBau}.  Properties of $A$ that can be described in terms of its eigenvalues or singular values are generically called ``spectral properties'' of $A$.

Note that $\|A\|_2$ is equal to the largest singular value of $A$, while $\|A^{-1}\|_2$ is equal to the inverse of the smallest singular value of $A$.  Thus $\kappa(A)$ is the ratio of largest to smallest singular values of $A$, and is well-visualized as the eccentricity of the just-mentioned ellipsoid.  The condition number of $A$ is thus a spectral property.

It is an easy exercise to show that the Richardson iteration \eqref{introrichardson} will converge if and only if all the eigenvalues of $I-\omega A$ are inside the unit circle.  Defining the \emph{spectral radius} $\rho(A)$ of a matrix $A$ as the maximum norm of the eigenvalues of $A$, we can describe the convergence of the Richardson iteration this way:
\begin{equation}
\text{\eqref{introrichardson} converges if and only if } \rho(I-\omega A) < 1. \label{introconvergethm}
\end{equation}

Other examples of iterative methods include the classical Jacobi and Gauss-Siedel iterations \citep{Greenbaum1997}, but the really powerful ones proceed by iteratively generating an \emph{optimal}, in some sense, estimate $\bu_k$ which is a linear combination of vectors $\bb,A\bb,A^2\bb,\dots,A^{k-1}\bb$, and these methods are collectively called \emph{Krylov space methods} because the span of these vectors is a Krylov space.  Typically the sense of ``optimal'' also depends on the eigenvalues or singular values of $A$.  We use Krylov space methods in Chapter \ref{chap:structured} and all later Chapters.

Returning to non-numerical linear algebra for a moment, note that there are many other systems which are equivalent to \eqref{introsystem}.  In fact, if $P\in\RR^{N\times N}$ is an invertible square matrix then the systems
\begin{equation}
(P^{-1} A) \bu = P^{-1} \bb \label{introleftpre}
\end{equation}
and
\begin{equation}
(A P^{-1}) (P\bu) = \bb \label{introrightpre}
\end{equation}
obviously have the same solution $\bu$ as \eqref{introsystem}.

Compared to $A$, the matrices $P^{-1} A$ or $A P^{-1}$ may have very different eigenvalues, condition numbers, and so on.  While the accuracy of the solution $\bu$ cannot be improved beyond the $\kappa(A) \eps$ level---fact i) cannot be overcome in that sense---methods can indeed take advantage of better conditioning or other spectral properties to generate an approximate solution more quickly.  Especially if $P^{-1}$ is easy to apply\sidenote{I.e.~if a system $P\bv = \bc$ is easy to solve for $\bv$ in the sense of low computational cost.} then this can be an advantageous idea when trying to approximate $\bu$ quickly.  Equivalent systems \eqref{introleftpre} and \eqref{introrightpre} are referred to as \emph{preconditioned} systems, with the former called \emph{left preconditioning} and the latter \emph{right-preconditioning}.

Now we conclude this little bit of numerical linear algebra by giving a small example of the role of preconditioning in making the simple Richardson iteration converge.  This small example is to help make the idea of preconditioning, to improve spectral properties, concrete for the reader.  Many methods far superior to the Richardson iteration are implemented in \PETSc, and we will use the best ones.

\medskip\noindent\hrulefill
\begin{example} Consider the linear system
\begin{equation}
A \bu =
\begin{bmatrix}
10 & -1 \\ -1 & 1
\end{bmatrix} 
\begin{bmatrix}
u_1 \\ u_2
\end{bmatrix}
=
\begin{bmatrix}
8 \\
1 
\end{bmatrix}
= \bb
 \label{introexample}
\end{equation}
which has solution $\bu = [1\,\, 2]^*$.  If we start with estimate $\bu_0 = [0\,\, 0]^*$ then the unweighted ($\omega=1$) Richardson iteration \eqref{introrichardson} gives a sequence of vectors % see ../matlab/richardsonex.m
\newcommand{\rvect}[3]{\ensuremath{\bu_{#1} = \begin{bmatrix} #2 \\ #3 \end{bmatrix}}}
\begin{equation}
\rvect{0}{0}{0}, \rvect{1}{8}{1}, \rvect{2}{-63}{9}, \rvect{3}{584}{-62}, \dots
\end{equation}
This sequence is not heading toward the solution $\bu = [1\,\, 2]^*$.  Suppose, however, that we use the diagonal matrix from system \eqref{introexample} to build $P$:
\begin{equation}
P = \begin{bmatrix}
10 & 0 \\ 0 & 1
\end{bmatrix}.  \label{introP}
\end{equation}
Being diagonal, this $P$ is very easy to invert and apply.  The preconditioned Richardson iteration using $P$, namely
\begin{equation}
\bu_{k+1} = \bu_k + \omega (P^{-1} \bb - P^{-1} A \bu_k),  \label{introprerichardson}
\end{equation}
is much better behaved.  With $\bu_0 = [0\,\, 0]^*$ again we get this sequence from \eqref{introprerichardson}:
\begin{equation}
\rvect{0}{0}{0}, \rvect{1}{0.8}{1.0}, \rvect{2}{0.9}{1.8}, \rvect{3}{0.98}{1.90}, \dots
\end{equation}
This sequence is apparently going to $\bu = [1\,\, 2]^*$.  Of course, the explanation is not hard to see; in this case
\begin{equation}
\rho(I-A) = -9.1, \qquad \rho(I-P^{-1} A) = 0.32,
\end{equation}
so our convergence claim \eqref{introconvergethm} matches the evidence.
\end{example}
\noindent\hrulefill


\section{\PETSc \pVec and \pMat objects}

Although \PETSc is written in C, not C++ in particular, it is a relentlessly object-oriented software library.  Consider the operations which might touch the matrix object \texttt{A} in a linear system like \eqref{introsystem}:
\begin{code}
  Mat A;
  MatCreate(COMM,&A);
  MatSetSizes(A,PETSC_DECIDE,PETSC_DECIDE,N,N);
  PetscObjectSetName((PetscObject)A,"A");
  MatSetOptionsPrefix(A,"a_");
  MatSetFromOptions(A);
  ... fill entries of (i.e. assemble) A ...
  ... solve system with A ...
  MatDestroy(&A);
\end{code}
where \texttt{COMM} is an MPI communicator.

Evidently \texttt{A} has an internal representation with nontrivial structure, but that is hidden.  Indeed the data structure inside \texttt{A} depends on runtime choices, the most basic being that the number of bytes used to store \texttt{A} on a given MPI process will depend on the number of processes.  At a deeper level, a \PETSc \pMat object need not even \emph{have} entries, but it may instead represent code that applies a linear operator to vectors.

Once \texttt{A} is created and set up by the first five commands, then various methods become valid for \texttt{A}, for example including the \texttt{MatSetValues()} method to set entries in \texttt{A}.  Also the run-time option \texttt{-a\_mat\_view} will print out the entries of \texttt{A}; the option prefix ``\texttt{a\_}'' is helpful in distinguishing \pMat objects at the command line in a context with multiple \pMats.

For the ``\pVec'' objects storing vectors, the ``\pMat'' objects storing matrices, and indeed all \PETSc object types, this basic sequence of operations applies:\sidenote{Of course ``\texttt{Object}'' here is merely a meta-name for the object type.}
\begin{code}
  Object X;
  ObjectCreate(COMM,&X);
  ... set properties of X from code ...
  ObjectSetFromOptions(X);  // allows run-time setting of properties
  ... use X ...
  ObjectDestroy(&X);
\end{code}
The first argument of \texttt{ObjectCreate()} takes an MPI communicator because \PETSc objects are, generically, distributed across, and accessible from, multiple MPI processes. 

FIXME: \pVec and \pMat allow parallel assembly

FIXME: illustrate parallel layout of \pVec and \pMat

FIXME: assembly of \pMat a bit idiosyncratic; needs either MatXXXSetPreallocation() or MatSetUp() before MatGetOwnershipRange()


\section{Solve a linear system in \PETSc}

\cinputpartnostrip{c1matvec.c}{Initialize \PETSc and set up \pVecs and \pMat.}{I}{}{//ENDSETUP}{code:matvecpartone}

\cinputpartnostrip{c1matvec.c}{Assemble \pMat $A$.  Assemble right-hand side $b$ via exact solution to system.}{II}{//ENDSETUP}{//ENDASSEMBLY}{code:matvecparttwo}

\cinputpartnostrip{c1matvec.c}{Set up \pKSP.  Solve.  Finalize.}{III}{//ENDASSEMBLY}{//END}{code:matvecpartthree}

FIXME: gloss PETSc traceback mechanism, before noting we will strip that stuff

FIXME: show sparse and Matlab-format output for \texttt{A}, noting we get this at the \texttt{MatAssemblyEnd()} stage %$ ./c1matvec -a_mat_view ::ascii_matlab

\caveat{But \Matlab is all you want if scale does not matter.}


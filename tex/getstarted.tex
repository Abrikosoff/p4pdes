
\section{Get the example codes}

Before starting into our first example, please use Git\sidenote{See  \href{https://git-scm.com/}{\texttt{git-scm.com}} if unfamiliar.} to get the example C codes:
\begin{cline}
$ git clone https://github.com/bueler/p4pdes.git
\end{cline}
%$
Codes from subdirectory \texttt{p4pdes/c/ch}$N$\texttt{/} are for Chapter $N$ of this book.

\section{A code that does almost nothing, but in parallel}

The purpose of the \PETSc library is to help you solve scientific and engineering problems on multi-processor computers.  As \PETSc is built on top of the Message Passing Interface (MPI; \citep{Groppetal1999}) library, some of its flavor comes through.  Therefore we start with an introductory \PETSc code which calls MPI for some basic tasks.

Our program \texttt{e.c}, shown in its entirety in Code \ref{code:e}, approximates Euler's constant by its Maclaurin series:
\begin{equation}
e = \sum_{n = 0}^\infty \frac{1}{n!} \approx 2.718281828 \label{eq:gs:introseries}
\end{equation}
It does the computation in a distributed manner by computing one term of the infinite series on each process, giving a better estimate of $e$ when run on more MPI processes. This is a silly use of \PETSc, but it is also an easy-to-understand parallel computation.

As with most C programs, Code \ref{code:e} starts by including needed headers.  However, as with most codes in this book, only \texttt{petsc.h} is needed because it includes MPI too.

Like any C program, \texttt{e.c} has a function called \texttt{main()} which takes inputs from the command line, namely \texttt{argc} and \texttt{argv}.  The former is an \texttt{int} holding the argument count and the latter is an array of strings holding the command line, including all arguments.  In all our codes we will simply pass these arguments to \PETSc through \texttt{PetscInitialize()}.  \PETSc extracts command-line options through this mechanism.

The \texttt{main()} function also outputs an \texttt{int}.  This return value is $0$ if the program succeeds, and so we see ``\texttt{return 0;}'' as the last line.  However, the error-related commands ``\texttt{CHKERRQ(ierr);}'' can also generate a non-zero return value from \texttt{main()}.  We will return to this ``trace-back'' mechanism later in the Chapter.  The substance of \texttt{e.c} is to declare some variables, do a computation on each process, and communicate the results between processes to get an estimate of $e$.

\inputwhole{../c/ch1/e.c}{\texttt{ch1/e.c}}{Compute $e$ with \PETSc.}{code:e}

%\lstset{fancyvrb=true,language=C,
%        showspaces=false,showstringspaces=false}
%\VerbatimInput[frame=single,%
%               framesep=3mm,%
%               label=\fbox{\small \textsl{\,ch1/e.c\,}},%
%               fontsize=\footnotesize]{../c/ch1/e.c}
%\lstset{fancyvrb=false}

Before we can compile and run \texttt{e.c}, \PETSc must be installed.  If it is not already installed on your machine, go to
\begin{quote}
\href{http://www.mcs.anl.gov/petsc/download/index.html}{\texttt{www.mcs.anl.gov/petsc/download/}}
\end{quote}
to download the source code, and then follow the instructions at
\begin{quote}
\href{http://www.mcs.anl.gov/petsc/documentation/installation.html}{\texttt{www.mcs.anl.gov/petsc/documentation/installation.html}}
\end{quote}
to install.  You will do steps that look like
\begin{cline}
export PETSC_DIR=/home/bueler/petsc
export PETSC_ARCH=linux-c-dbg
./configure --download-mpich --download-triangle --with-debugging=1
make all
\end{cline}
though the specific configure options may be quite different in your environment.  Here we download the MPICH package, because on this machine there no existing MPI installation, and the \texttt{triangle} package, because it is used in Chapter \ref{chap:un}.  We configure a version of \PETSc with debugging symbols because this is essential for understanding the trace-back which happens at run-time errors.  After above steps work successfully, run ``\texttt{make test}'' and see if it passes the tests.

When \PETSc is correctly installed the environment variables \texttt{PETSC\_DIR} and \texttt{PETSC\_ARCH} point to a valid installation, and the MPI command \texttt{mpiexec} is from the same MPI installation as was used in configuring \PETSc.\sidenote{Type ``\texttt{which mpiexec}'' to find which one you are running.  You may need to modify your \texttt{PATH} environment variable to get the right \texttt{mpiexec}.}

Do the following to compile \texttt{e.c}:
\begin{cline}
$ cd p4pdes/c/
$ cd ch1/
$ make e
\end{cline}
%$
Calling ``\texttt{make}'' uses \texttt{makefile} in the \texttt{ch1/} directory; an extract is shown in Code \ref{code:ch1makefile}.  All the \texttt{makefile}s in this book have this recommended \citep{petsc-user-ref} form.

\inputwhole{ch1makefile.frag}{extract from \texttt{ch1/makefile}}{All our \texttt{makefile}s look like this.}{code:ch1makefile}

Run the code like this:
\begin{cline}
$ ./e
e is about 1.000000000000000
rank 0 did 1 flops
\end{cline}
%$
The value $1.0$ is a very poor estimate of $e$, but it does better with more MPI processes:
\begin{cline}
$ mpiexec -n 5 ./e
e is about 2.708333333333333
rank 0 did 0 flops
rank 4 did 3 flops
rank 2 did 1 flops
rank 3 did 2 flops
rank 1 did 0 flops
\end{cline}
%$
With $N=20$ processes, and thus $N=20$ terms in series \eqref{eq:gs:introseries}, we get a good double-precision estimate:
\begin{cline}
$ mpiexec -n 20 ./e
e is about 2.718281828459045
rank 0 did 0 flops
...
\end{cline}
%$

``Double precision,'' of course, refers to the 64-bit floating-point representation of real numbers.  This type, which is normally aliased to \texttt{PetscReal} inside \PETSc codes, corresponds to about 15 decimal digit accuracy \citep{TrefethenBau1997}.

Perhaps the reader is now worried that this book was written using a cluster with 20 physical processors whereas the reader has a little laptop with only a couple of cores.  Not so.  In fact these 5 and 20 process runs work just fine on the author's 4-core laptop.  MPI processes are created as needed, using an old feature of operating systems: multitasking.  Actual speedup from parallelism is another matter entirely, to which we will return.

Returning to \texttt{e.c} in Code \ref{code:e}, each MPI process computes term $1/n!$ where $n$ is returned by \texttt{MPI\_Comm\_rank()}.\sidenote{A call to \texttt{MPI\_Comm\_size()} would return \texttt{N}, the communicator's \emph{size}.}  Note \texttt{PETSC\_COMM\_WORLD} is an MPI communicator \citep{Groppetal1999} containing all processes generated by using ``\texttt{mpiexec -n N}'' at the command line.  A call to \texttt{MPI\_Allreduce()} computes a partial sum of \eqref{eq:gs:introseries} and sends the result back to each process.  These direct uses of the MPI library are a part of using \PETSc, because it generally avoids duplicating MPI functionality.

We print the computed estimate of $e$ once, but each process also prints its rank and the work it did.  The formatted print command \texttt{PetscPrintf()}, similar to \texttt{fprintf()} from the C standard library, is called twice in the code.  The first time it uses MPI communicator \texttt{PETSC\_COMM\_WORLD} and the second time \texttt{PETSC\_COMM\_SELF}.  The first of these printing jobs is thus \emph{collective} over all processes, and only one line of output is produced, while the second is individual to each rank\sidenote{A process is often just called a \emph{rank} in MPI language.}  and we get \texttt{N} printed lines.  The \texttt{PETSC\_COMM\_SELF} output lines can appear in apparently random order because the print occurs as soon as that rank reaches the \texttt{PetscPrintf()} command in the code.


\section{Every \PETSc program}

Every \PETSc program should start and end with the commands \texttt{PetscInitialize()} and \texttt{PetscFinalize()}:
\begin{code}
PetscInitialize(&argc,&args,NULL,help);
... everything else goes here ...
PetscFinalize();
\end{code}

As the last argument to \texttt{PetscInitialize()} we supply a \texttt{help} string.  This string is a good place to say what is the purpose of the code,and thus it is often declared in the very first line of a \PETSc code.  To see the help string, and a longer list of possible \PETSc options, do:
\begin{cline}
$ ./e -help
\end{cline}
%$
or
\begin{cline}
$ ./e -help | less
\end{cline}
%$
Through option \texttt{-help}, \PETSc programs have a built-in help system for runtime options that is both light-weight and surprisingly-effective.  For example, to see options related to logging performance, do
\begin{cline}
$ ./e -help | grep log_
\end{cline}
%$
See Exercise \ref{chap:gs}.\ref{exer:gs:nondeterminant} for an example of how to add a new option to your own program.

Unfortunately with respect to source-code aesthetics, all our \PETSc example codes have error-checking clutter.  While languages other than C might help with decluttering, we are stuck with ugly lines that look like
\begin{code}
ierr = PetscCommand(...); CHKERRQ(ierr);
\end{code}
The explanation is that almost all \PETSc methods, and most user-written methods in \PETSc programs, return an \texttt{int} for error checking, with value $0$ if successful.  In the line above, \texttt{ierr} is passed to the \texttt{CHKERRQ()} macro.  It does nothing if \texttt{ierr == 0} but it stops the program with a ``trace-back'' otherwise.  A trace-back is a list of the nested function calls, in reverse order, showing the line numbers and functions names of the location where the error occurred.  Trace-back is most effective if \PETSc is configured with debugging symbols as we did above, i.e.~with configure option \texttt{----with-debugging=1}.

This traceback is a first line of defense when debugging run-time errors, so in this book we always capture-and-check the returned error code using the \texttt{CHKERRQ()} macro.  However, after Chapter \ref{chap:gs} we will strip the ``\texttt{ierr =}'' and ``\texttt{CHKERRQ(ierr);}'' clutter from each code when it is displayed in the text.


\section{Parallel reductions are generally not bit-wise deterministic}

The code \texttt{e.c} does a finite sum in parallel, namely an $N$-term truncation of \eqref{eq:gs:introseries} on $N$ MPI processes, by a call to \texttt{MPI\_Allreduce()}.  Such parallel reduction operations are also key steps in linear algebra algorithms like GMRES (Chapter \ref{chap:ls}).

On one hand, floating-point arithmetic is not, however, commutative or associative, though nearly so.\sidenote{See \citep{Higham2002,TrefethenBau1997} for theory.  See Exercise \ref{chap:gs}.\ref{exer:gs:nondeterminant} for an example of non-commutativity.}  On the other hand, when done in parallel a total order on the sum is not pre-determined, and this is similarly true for other parallel reductions like products.  For example, one can suppose when using \texttt{MPI\_Allreduce()} for summation that the terms are collected on the rank $0$ process and that they are added to a running sum as they arrive; this is the stage at which the non-determinacy applies.  After the sum is done on rank $0$ the result is sent (``scattered'') to all other ranks, without further floating-point consequences.  See \citep{Groppetal1999}.

It follows that parallel reductions generally exhibit non-determinacy coming from the order of operations.  Such non-determinacy is well-known; see for example the entry on ``determinacy'', and on ``associative non-determinacy'' in particular, in \citep{Padua2011}.  Because of this basic fact a seemingly-deterministic program like \texttt{e.c} could, at least in theory, have a different outcome at the level of rounding error when run a second time in parallel.

Though the fact of non-determinism of parallel reductions may never bite the reader, it is a numerical fact of life.  \emph{Stable} algorithms, in particular backward stable ones \citep{TrefethenBau1997}, do not suffer large output changes between runs as a result of this fact.

Demonstrating the non-determinacy on a simple sum like \eqref{eq:gs:introseries} is difficult because it helps to have many physical processors\sidenote{The example on page 563 of \citep{Padua2011} proposes 10000 of them.} \emph{and} significant noise in the interconnect between the processors, so that terms are collected in a different order on each run.  These conditions are hard to achieve using software MPI processes on a small (low core count) machine, so Exercise \ref{chap:gs}.\ref{exer:gs:nondeterminant} simply demonstrates the non-commutativity of the addition operation in a serial run.

Looking ahead, Chapter \ref{chap:ls} introduces more important ``facts of life,'' of which we mention two here.  The first relates to floating-point arithmetic, namely that some linear systems are intrinsically ill-conditioned.  Conditioning is related to the stability of algorithms \citep{TrefethenBau1997}, but it is not (fundamentally) related to parallel computations.  The need for, and parallel non-determinism of, preconditioning of linear systems\sidenote{``Preconditioning'' is defined in Chapter \ref{chap:ls} and it continues as a major topic throughout this book.} is the second fact-of-life.  The parallel application of many preconditioning methods is dependent on the number of MPI processes, a dependence which we will see in examples.

The unifying theme of these ``facts'', namely the physical processor-count dependence of parallel reductions, the stronger MPI process-count dependence of some preconditioner methods, and the ill-conditioning of problems, is that they drive us to be concerned with the stability, and not just the efficiency, of the algorithms implemented in \PETSc.


\bigskip
\section{Exercises}

\renewcommand{\labelenumi}{\arabic{chapter}.\arabic{enumi}\quad}
\begin{enumerate}

\item \label{exer:gs:expx}  Modify \texttt{e.c} to create a new code \texttt{expx.c} which approximates $e^x$ by its $N$-term Maclaurin series.  Read $x$ at the command line using the \PETSc options mechanism like this:
\begin{code}
  PetscOptionsBegin(PETSC_COMM_WORLD,"","options for expx","");
  PetscOptionsReal("-x","input to exp(x) function",NULL,x,&x,NULL);
  PetscOptionsEnd();
\end{code}
Find $N$ so that the run
\begin{cline}
mpiexec -n N ./expx -x -20.0
\end{cline}
gives an approximation of $e^{-20}$ accurate to six digits, and check that it works.

\item \label{exer:gs:nondeterminant}  \emph{Demonstrating the non-commutativity of floating-point addition is an excuse to practice more \PETSc programming in a serial (single-process) context.}

A code called \texttt{shuffle.c}, in subdirectory \texttt{c/ch1/}, permutes the order of the elements stored in a one-dimensional C array and prints the result (not shown).  Look at this code and see how it uses the \PETSc options mechanism and a \PETSc random number generator; note that the wall clock time in seconds is used to seed the latter.  Modify it to create a code \texttt{lntwo.c} which does a $N$-term truncation (partial sum) of the slowly-converging infinite series
\begin{equation}
\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n} = \ln 2.
\label{eq:gs:lntwo}
\end{equation}
The new code will do the finite sum by computing all the terms in an array, then randomly permuting the array, and then summing.  The summation can be done by any convenient loop; no \texttt{MPI\_Allreduce()} is needed.  By doing repeated runs with one MPI process but $N=1000$ terms, show that that the last couple of decimal digits, in a 16 significant digit display, vary from run to run.  If we did permuted partial sums of series \eqref{eq:gs:introseries} would we see the same variation?

\item \label{exer:gs:balance} Program \texttt{e.c} does redundant work, and a terrible job of load-balancing, because the computation of the factorial $n!$ on the rank $n$ process requires $n-1$ flops.  Modify \texttt{e.c} to a new code \texttt{balance.c} which balances the load almost perfectly, giving one divide operation on each \texttt{rank} $>0$ process.  Use blocking send and receive operations (\texttt{MPI\_Send(),MPI\_Recv()}) to pass the result of the last factorial to the next rank.  (Now the code does lots of unnecessary communication and waiting, so neither \texttt{e.c} nor \texttt{balance.c} are good examples for future use!)

\end{enumerate}

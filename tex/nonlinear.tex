\renewcommand{\CODELOC}{c/ch4/}

\chapter{Nonlinear elliptic PDEs on structured grids}
\label{chap:nonlinear}

\section{Newton's method}

How do nonlinear equations arise?  From applications, of course; from physical models with nonlinearities.  But how should nonlinear equations first ``arise'' in a code which use solvers like \PETSc?

We will think of nonlinear equations as a ``simple'' change to the functional form of the residual.  That is, for a linear system we could say that the residual is a function of the unknowns with a particular form,
\begin{equation}
\br = \bF(\bu) = \bb - A \bu. \label{eq:nl:linres}
\end{equation}
Nonlinear equations are a generalization of this, where we allow $\bF$ to be arbitrary in form, perhaps a higher-order polynomial or transcendental function.

That is, suppose $\bF : \RR^N \to \RR^N$ is merely differentiable.  The input $\bx$ and output $\bF(\bx)$ are column vectors,\sidenote{The name change of the unknown $\bu\to\bx$, relative to the last section, is because we will think geometrically about changes in the location of the unknown $\bx$ as stepwise movement through the space $\RR^n$.} so in that sense $\bF$ acts like multiplying by a square matrix $\bx\mapsto A\bx$.  Exactly as we want to reduce the residual to zero in iterative linear algebra, for nonlinear $\bF$ we want to solve
\begin{equation}
   \bF(\bx) = 0.   \label{eq:nl:equation}
\end{equation}

In this section we will follow Isaac Newton in building an iterative method by linearizing \eqref{eq:nl:equation} around each iterate, and in treating the problem as that of ``moving'' $\bx$ to a location closer to the solution of \eqref{eq:nl:equation}.  Each iteration starts by solving a linear system, and of course we already have some technology for that.  However, choosing the right distance to move requires additional choices, beyond the linear solver issues in the last two sections, and the cost of performing the linearization must be taken seriously as well.  Also we need to connect Newton-type ideas to the discretizations of PDEs.

Given $\bx_k$ and a step $\Delta\bx$, both vectors in $\RR^n$, we define $\bx_{k+1} = \bx_k + \Delta\bx$.  If $\bF$ is differentiable then
\begin{equation}
    \bF(\bx_{k+1}) = \bF(\bx_k) + J_\bF(\bx_k) \Delta\bx + o(\|\Delta\bx\|)  \label{eq:nl:expandF}
\end{equation}
for some matrix
\begin{equation}
J = J_\bF(\bx_k) = \begin{bmatrix}
    \frac{\partial F_0}{\partial x_0} & \dots & \frac{\partial F_0}{\partial x_{N-1}} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial F_{N-1}}{\partial x_0} & \dots & \frac{\partial F_{N-1}}{\partial x_{N-1}}  \end{bmatrix}.  \label{eq:nl:jacdefn}
\end{equation}
By definition, $J$ is the \emph{Jacobian (matrix)} of $\bF$ at $\bx_k$; in fact we also refer to the function $\bx \mapsto J_\bF(\bx)$ as the Jacobian.

An iteration of Newton's method approximately solves nonlinear equation \eqref{eq:nl:equation} by truncating \eqref{eq:nl:expandF} and seeking $\Delta \bx$ so that the updated value $\bF(\bx_{k+1})$ is zero in the truncated equation.  That is,  each step computes $\Delta x$ so that
\begin{equation}
    0 = \bF(\bx_k) + J_\bF(\bx_k) \Delta\bx.
\end{equation}
Writing this equation in our previous style ``$A\bu=\bb$'' for linear systems, at each iteration $k$ we solve a linear system and then do a vector update:
\begin{align}
    J_\bF(\bx_k) \Delta\bx &= - \bF(\bx_k)  \label{eq:nl:newtoneq}  \\
    \bx_{k+1} &= \bx_k + \Delta\bx  \label{eq:nl:newtonupdate}
\end{align}
This is fairly-simple in theory.

Actual practice is not \emph{that} much more complicated!  Despite the reputation of Newton iteration as fragile or scary, with a bit of caution in ``moving'' to the new iterate as in \eqref{eq:nl:newtonupdate}, this will work out.  Newton iteration provides the best available technology for solving nonlinear problems including PDEs.  On the other hand, care is needed in solving linear system \eqref{eq:nl:newtoneq}; it should be clear that the nonlinear problem must also require all of the tools for linear systems already considered, and more.

Such considerations will be easier if we see an example.

\medskip\noindent\hrulefill
\begin{example}  Nonlinear systems can be thought of as describing the intersections of curves, surfaces, or hypersurfaces, depending on dimension.  For example, this pair of nonlinear equations
    $$y = \frac{1}{2} e^x, \qquad x^2+y^2 = 1$$
are an exponential curve and a circle around the origin.  They are easy to visualize, and also shown in Figure FIXME.  The curves clearly have two points of intersection, one each in the first and second quadrants.  Finding those points of intersection means solving the system of simultaneous nonlinear equations.

To put the system in standard form \eqref{eq:nl:equation} we write
\begin{equation}
\bF(\bx) = \begin{bmatrix}
           e^{x_0} - 2 x_1 \\
           x_0^2 + x_1^2 - 1
           \end{bmatrix}
\end{equation}
for $\bx\in \RR^2$.  Thus
\begin{equation}
J_\bF(\bx) = \begin{bmatrix}
    e^{x_0} & -2 \\
    2 x_0   & 2 x_1 \end{bmatrix}
\end{equation}
If we start the Newton iteration with $\bx_0 = [1,1]$ then the sequence of iterates from \eqref{eq:nl:Newton} is
    $$\twovect{\bx}{0}{1}{1}, \twovect{\bx}{1}{FIXME}{FIXME}, \dots$$
as shown in Figure FIXME
\end{example}
\noindent\hrulefill

\medskip
There are only two main ideas, beyond the construction of the Newton iteration \eqref{eq:nl:Newton} itself, which turn Newton iteration into an effective, indeed profoundly-effective, tool:
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
\item FIXME linesearch or trust region needed \citep{Kelley2003}
\item FIXME full range of linear tools (e.g.~Chapter \ref{chap:linearsystem}) should be applied to the linear system \eqref{eq:nl:Newton}
\end{enumerate}

What make's Newton so good?  FIXME: quadratic convergence


\section{\pSNES and call-backs}

FIXME  we solve the above example in code \texttt{expcircle.c} as shown in Figure \ref{code:expcircle}.  this is the entire code, but note that the Jacobian $J_\bF(\bx)$ is not implemented in code.  run it
\begin{cline}
$ cd c/ch4/
$ make expcircle
$ ./expcircle -snes_fd
\end{cline}
%$
Better is to add option \texttt{-snes\_monitor}.  Also \texttt{-snes\_mf} works.

FIXME the code works through \emph{call-back}.  Specifically FIXME

\vfill
\cinput{expcircle.c}{\CODELOC}{FIXME}{//START}{//END}{code:expcircle}


\section{Jacobians, exact and approximate}

FIXME

\vfill
\cinputpart{expcircleJAC.c}{\CODELOC}{FIXME}{I}{//STARTJAC}{//ENDJAC}{code:expcircleJACI}

\cinputpart{expcircleJAC.c}{\CODELOC}{FIXME}{II}{//STARTADDJAC}{//ENDADDJAC}{code:expcircleJACII}


\section{Example: 1D reaction-diffusion equation}

FIXME

\vfill
\cinputpart{reaction.c}{\CODELOC}{FIXME}{I}{//SETUP}{//ENDSETUP}{code:reactionI}

\cinputpart{reaction.c}{\CODELOC}{FIXME}{II}{//FUNCTION}{//ENDFUNCTION}{code:reactionII}

\cinputpart{reaction.c}{\CODELOC}{FIXME}{III}{//JACOBIAN}{//ENDJACOBIAN}{code:reactionIII}

\cinputpart{reaction.c}{\CODELOC}{FIXME}{IV}{//MAIN}{//ENDMAIN}{code:reactionIV}

\section{Optimization and nonlinear PDEs: $p$-Laplacian equation}

FIXME for $p>1$,
    $$I[u] = \int_\Omega \frac{1}{p} |\grad u|^p - fu$$
the variational equation is the weak form of a PDE:
\begin{align*}
I[u+\eps v] - I[u] &= \int_\Omega \frac{1}{p} |\grad u + \eps \grad v|^p + \frac{1}{p} |\grad u|^p - \eps f v \\
   &= \eps \left(\int_\Omega |\grad u|^{p-2} \grad u \cdot \grad v - f v\right) + O(\eps^2)
\end{align*}
so we want
    $$0 = \int_\Omega |\grad u|^{p-2} \grad u \cdot \grad v - f v$$
for all $v \in W^{1,p}_0(\Omega)$.  this can become a strong form by another integration by parts,
    $$0 = - \Div\left(|\grad u|^{p-2} \grad u\right) - f$$
which is \eqref{poissonsquare} if $p=2$

FIXME introduce $Q^1$ FEM on structured grid

\begin{marginfigure}
\input{q1hat.tikz}
\caption{FIXME}
\label{fig:q1hat}
\end{marginfigure}

FIXME code uses \texttt{SNESSetObjective()} only, though also \texttt{SNESSetFunction()}; no hand-made Jacobian at all

FIXME try NCG

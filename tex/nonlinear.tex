\renewcommand{\CODELOC}{c/ch4/}

\chapter{Nonlinear elliptic PDEs on structured grids}
\label{chap:nonlinear}

\section{Newton's method}

How do nonlinear equations arise?  From applications, of course.  They come from physical models with nonlinearities, and we will see examples later in this section.  But how should they first appear in a code which uses \PETSc?

Nonlinear equations merely change the functional form of the residual.  For a linear system the residual is this function of the unknowns,
\begin{equation}
\br = \bF(\bu) = \bb - A \bu. \label{eq:nl:linres}
\end{equation}
We now consider cases in which function $\bF(\cdot)$ is a higher-order polynomial, a transcendental function, or some more general function.

Suppose $\bF : \RR^N \to \RR^N$ is differentiable.  The input $\bx$ and output $\bF(\bx)$ are column vectors,\sidenote{The name change of the unknown $\bu\to\bx$, relative to \eqref{eq:nl:linres}, is because we will now think more geometrically about changes in the location of $\bx$.} so in that sense $\bF$ acts like multiplying by a square matrix $\bx\mapsto A\bx$.  As when we reduce the residual to zero in an iterative linear algebra method, for nonlinear $\bF$ we want to solve
\begin{equation}
   \bF(\bx) = 0   \label{eq:nl:equation}
\end{equation}
by iteration.

In this section we follow Isaac Newton in building an iterative method by linearizing \eqref{eq:nl:equation} around each iterate and then treating the problem as that of ``moving'' $\bx$ to a location closer to the solution.  Each iteration therefore solves a linear system; we already have some technology for that!  However, choosing the right distance to move requires additional choices.  Furthermore, the cost of performing the linearization must be taken into account, and all existing issues regarding linear solver choices, as discussed in the last two sections, remain active.  Also, we will learn how to tell \PETSc about the specific function in \eqref{eq:nl:equation}.

First, here is the idea of Newton's method.  If we had determined a step $\bs$ from the current iterate $\bx_k$, where both are vectors in $\RR^n$, we would let
\begin{equation}
\bx_{k+1} = \bx_k + \bs \label{eq:nl:firstupdate}
\end{equation}
be the next iterate.  By definition, because $\bF$ is differentiable,
\begin{equation}
    \bF(\bx_{k+1}) = \bF(\bx_k) + J_\bF(\bx_k) \bs + o(\|\bs\|)  \label{eq:nl:expandF}
\end{equation}
for some square matrix
\begin{equation}
J_\bF(\bx_k) = \begin{bmatrix}
    \frac{\partial F_0}{\partial x_0} & \dots & \frac{\partial F_0}{\partial x_{N-1}} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial F_{N-1}}{\partial x_0} & \dots & \frac{\partial F_{N-1}}{\partial x_{N-1}}  \end{bmatrix},  \label{eq:nl:jacdefn}
\end{equation}
and where $o(\|\bs\|)$ denotes some quantity that goes to zero as the length of the step $\|\bs\|$ goes to zero.  The matrix $J = J_\bF(\bx_k)$ is called the \emph{Jacobian} of $\bF$ at $\bx_k$, though we also refer to the function $\bx \mapsto J_\bF(\bx)$ itself as the Jacobian.

An iteration of Newton's method approximately solves nonlinear equation \eqref{eq:nl:equation} by truncating \eqref{eq:nl:expandF} and then seeking $\bs$ so that the updated value $\bF(\bx_{k+1})$ is zero in the truncated equation.  That is, each Newton step computes $\bs$ so that
\begin{equation}
    0 = \bF(\bx_k) + J_\bF(\bx_k) \bs.
\end{equation}
Writing this equation in our previous style ``$A\bu=\bb$'' for linear systems, at each iteration $k$ we solve a linear system and then do a vector addition:
\begin{align}
    J_\bF(\bx_k) \bs &= - \bF(\bx_k)  \label{eq:nl:newtoneq}  \\
    \bx_{k+1} &= \bx_k + \bs  \label{eq:nl:newtonupdate}
\end{align}
This is Newton's method.  It is simple in theory.

Actual practice is not that much more complicated, especially having \PETSc in hand!  Despite the reputation of Newton iteration as fragile or scary, by using a bit of caution in ``moving'' to the new iterate as in \eqref{eq:nl:newtonupdate}, this will work out just fine on many small nonlinear systems \citep{Kelley2003}.  On big nonlinear systems we will need to pay more attention to the details of the linear solve at each step.\sidenote{It should be clear that the nonlinear problem must also require all of the tools for linear systems already considered in Chapters \ref{chap:linearsystem} and \ref{chap:structured}, and more.  Nonlinear problems cannot somehow be easier than linear ones!}  In either case, Newton iteration is the core technology for solving nonlinear problems, including nonlinear PDEs.

A small example gives us a start on exploring the details.

\medskip\noindent\hrulefill
\begin{example}  Nonlinear systems can be visualized as the intersections of curves, surfaces, or hypersurfaces, depending on dimension.  For example, given $b > 1$ this pair of nonlinear equations
    $$y = \frac{1}{b} e^{bx}, \qquad x^2+y^2 = 1,$$
form intersecting curves in the plane.  As shown in Figure \ref{fig:expcirclebasic} for the $b=2$ case, the curves intersect twice, once each in the first and second quadrants.

These nonlinear equations are put in standard form \eqref{eq:nl:equation} by writing
\begin{equation}
\label{eq:nl:expcircleF}
\bF(\bx) = \begin{bmatrix}
           \frac{1}{b} e^{b x_0} - x_1 \\
           x_0^2 + x_1^2 - 1
           \end{bmatrix}
\end{equation}
for $\bx\in \RR^2$ with scalar components $\bx = [x_0 \, x_1]^\top$.  Thus
\begin{equation}
J_\bF(\bx) = \begin{bmatrix}
    e^{b x_0} & -1 \\
    2 x_0   & 2 x_1 \end{bmatrix}
\end{equation}
If $b=2$ and we start the Newton iteration with $\bx_0 = [1 \, 1]^\top$ then the sequence of iterates from \eqref{eq:nl:newtoneq} and \eqref{eq:nl:newtonupdate} is
    $$\twovect{\bx}{0}{1}{1}, \quad \twovect{\bx}{1}{0.619203}{0.880797}, \quad \twovect{\bx}{2}{0.394157}{0.948623}, \quad \dots$$
as also shown in Figure \ref{fig:expcirclebasic}.

\noindent\hrulefill
%  FROM $ for N in 0 1 2; do ./expcircle -snes_fd -snes_max_it $N; done
\end{example}

\begin{marginfigure}
\includegraphics[width=1.2\textwidth]{expcirclebasic}
\caption{Newton iterates approach a solution of $\bF(\bx)=0$ for $\bF$ in \eqref{eq:nl:expcircleF} and $b=2$.}
\label{fig:expcirclebasic}
\end{marginfigure}


\section{\pSNES and call-backs}

We will do this calculation in \PETSc using a nonlinear-solver object of type \pSNES, for ``scalable nonlinear equation solver.''  This object has the usual \texttt{Create/SetFromOptions/Destroy} sequence, but additionally we must tell it about the function $\bF$.  One would generally also want to provide the derivative of $\bF$, the Jacobian function $J_{\bF}$, to the \pSNES, but that matrix can be approximated by finite differences.

Figure \ref{code:expcircle} shows our first \pSNES-using code \texttt{expcircle.c}, which solves the problem \eqref{eq:nl:equation} with $\bF$ from \eqref{eq:nl:expcircleF}.  The \texttt{main()} routine is mostly not surprising, but we summarize the contents anyway.  We start by allocating a \pVec \texttt{x}, of fixed dimension 2, which will hold both the initial iterate $\bx_0$ and, once the Newton iteration is ended, the converged estimate of the solution to system \eqref{eq:nl:equation}.  Because both components of $\bx_0$ equal $1$ in the above example, it is easy to initialize the \pVec with \texttt{VecSet()}.  Finally, a duplicate \pVec \texttt{r} is created because we need to supply it to the \pSNES as space for the (nonlinear) residual.

Next, the \pSNES is created and configured.  Through a ``call-back'' the formula \eqref{eq:nl:expcircleF} is supplied by a call to \texttt{SNESSetFunction()}.  The third argument of \texttt{SNESSetFunction()} is the name of the C function \texttt{FormFunction()} which we wrote.  In order to match the calling sequence of \texttt{SNESSetFunction()}, our \texttt{FormFunction()} must have a particular ``signature'' as a C function:
\begin{code}
PetscErrorCode (*f)(SNES,Vec,Vec,void*)
\end{code}
Then \texttt{SNESSetFromOptions()} is called so that, in particular, we will have run-time control both on how the Jacobian is calculated and on how the length of the step $\bs$ is actually determined; see below.

Finally the system is solved by a call to \texttt{SNESSolve()}.  With this call the initial iterate in \texttt{x} is also supplied.  After that the new state of \texttt{x}, presumably the converged solution, is printed at the command line using \texttt{VecView()} with a \texttt{STDOUT} viewer.

\vfill
\cinput{expcircle.c}{\CODELOC}{FIXME}{//START}{//END}{code:expcircle}

Looking inside our function \texttt{FormFunction()} we see an example of how to extract values from a \pVec and then a new method for how to set values.  Previously we have used \texttt{VecSetValues()} to set values at given indices but it is also possible to access the C array underlying the \pVec.  In this case we need to read entries of input \texttt{x} and then set entries of output \texttt{F}.  FIXME

Run it
\begin{cline}
$ cd c/ch4/
$ make expcircle
$ ./expcircle -snes_fd
\end{cline}
%$
 
FIXME: Clearly the iteration \eqref{eq:nl:newtoneq} requires the Jacobian.  FIXME: explain \texttt{-snes\_fd}

FIXME  we solve the above example in code \texttt{expcircle.c} as shown in Figure \ref{code:expcircle}.  this is the entire code, but note that the Jacobian $J_\bF(\bx)$ is not implemented in code.

FIXME Better is to add option \texttt{-snes\_monitor}.  Also \texttt{-snes\_mf} works.  See result from \texttt{-snes\_view} in combination with \texttt{-snes\_fd} and \texttt{-snes\_mf}.

FIXME use \texttt{-snes\_monitor}; result is quadratic convergence in Figure \ref{fig:newtonconvbasic}

\begin{figure}
\includegraphics[width=0.8\textwidth]{newtonconvbasic}
\caption{The characteristic look of the quadratic convergence of the Newton iteration: the residual drops abruptly.}
\label{fig:newtonconvbasic}
\end{figure}

FIXME if you run w/o \texttt{-snes\_fd} then error message
\begin{cline}
$ ./expcircle
[0]PETSC ERROR: --------------------- Error Message -------------------------
[0]PETSC ERROR: Object is in wrong state
[0]PETSC ERROR: Matrix must be assembled by calls to MatAssemblyBegin/End();
...
\end{cline}
%$

FIXME use \texttt{-snes\_rtol}


\section{Jacobians, exact and approximate}

FIXME

\vfill
\cinputpart{expcircleJAC.c}{\CODELOC}{FIXME}{I}{//STARTJAC}{//ENDJAC}{code:expcircleJACI}

\cinputpart{expcircleJAC.c}{\CODELOC}{FIXME}{II}{//STARTADDJAC}{//ENDADDJAC}{code:expcircleJACII}

There remain two major ideas not covered above, i.e.~beyond the construction of the Newton iteration \eqref{eq:nl:newtoneq} itself, to turn Newton iteration into an effective tool:
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
\item FIXME linesearch or trust region needed \citep{Kelley2003}
\item FIXME full range of linear tools (e.g.~Chapter \ref{chap:linearsystem}) should be applied to the linear system
\end{enumerate}


\section{Example: 1D reaction-diffusion equation}

FIXME

\vfill
\cinputpart{reaction.c}{\CODELOC}{FIXME}{I}{//SETUP}{//ENDSETUP}{code:reactionI}

\cinputpart{reaction.c}{\CODELOC}{FIXME}{II}{//FUNCTION}{//ENDFUNCTION}{code:reactionII}

\cinputpart{reaction.c}{\CODELOC}{FIXME}{III}{//JACOBIAN}{//ENDJACOBIAN}{code:reactionIII}

\cinputpart{reaction.c}{\CODELOC}{FIXME}{IV}{//MAIN}{//ENDMAIN}{code:reactionIV}

\section{Optimization and nonlinear PDEs: $p$-Laplacian equation}

FIXME for $p>1$,
    $$I[u] = \int_\Omega \frac{1}{p} |\grad u|^p - fu$$
the variational equation is the weak form of a PDE:
\begin{align*}
I[u+\eps v] - I[u] &= \int_\Omega \frac{1}{p} |\grad u + \eps \grad v|^p + \frac{1}{p} |\grad u|^p - \eps f v \\
   &= \eps \left(\int_\Omega |\grad u|^{p-2} \grad u \cdot \grad v - f v\right) + O(\eps^2)
\end{align*}
so we want
    $$0 = \int_\Omega |\grad u|^{p-2} \grad u \cdot \grad v - f v$$
for all $v \in W^{1,p}_0(\Omega)$.  this can become a strong form by another integration by parts,
    $$0 = - \Div\left(|\grad u|^{p-2} \grad u\right) - f$$
which is \eqref{poissonsquare} if $p=2$

FIXME introduce $Q^1$ FEM on structured grid

\begin{marginfigure}
\input{q1hat.tikz}
\caption{FIXME}
\label{fig:q1hat}
\end{marginfigure}

FIXME code uses \texttt{SNESSetObjective()} only, though also \texttt{SNESSetFunction()}; no hand-made Jacobian at all

FIXME try NCG

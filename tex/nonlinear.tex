\renewcommand{\CODELOC}{c/ch4/}

\chapter{Nonlinear elliptic PDEs on structured grids}
\label{chap:nonlinear}

\section{Newton's method}

How should nonlinear equations first appear in a code which uses \PETSc?  The answer is that they merely change the functional form of the residual.  For a linear system the residual is this function of the unknowns,
\begin{equation}
\br = \bF(\bu) = \bb - A \bu, \label{eq:nl:linres}
\end{equation}
but we now consider cases in which function $\bF(\cdot)$ is a higher-order polynomial, a transcendental function, or some more general function.  That is, we suppose $\bF : \RR^N \to \RR^N$ is differentiable.  The input $\bx$ and output $\bF(\bx)$ are column vectors,\sidenote{The name change of the unknown $\bu\to\bx$, relative to \eqref{eq:nl:linres}, is because we will now think more geometrically about changes in the location of $\bx$.} so in that sense $\bF$ acts like multiplying by a square matrix $\bx\mapsto A\bx$.  Just as we would reduce the residual to zero in an iterative linear algebra method, for nonlinear $\bF$ we want to solve
\begin{equation}
   \bF(\bx) = 0   \label{eq:nl:equation}
\end{equation}
by iteration.

In this section we follow Isaac Newton in building an iterative method to solve \eqref{eq:nl:equation}.  This method linearizes \eqref{eq:nl:equation} around the most recent iterate and then ``moves'' $\bx$ to the location which solves the linear problem and is, we hope, closer to the solution.  Each iteration therefore solves a linear system; we already have some \PETSc technology for that!  However, choosing the ``smart'' distance to move will require additional choices.  Furthermore, the cost of performing the linearization must be taken into account, and all existing issues regarding the linear solver, including preconditioner choices, as discussed in the last two chapters, remain active.

Of course, nonlinear PDEs do arise in applications.  They come from physical models with nonlinearities, and we will see examples later in this section.  Discretizing nonlinear PDEs leads to systems like \eqref{eq:nl:equation}, and we will give examples of that.  For now, however, we concentrate on solving \eqref{eq:nl:equation}.

First we recall Newton's method.  If we had determined a step $\bs$ from the current iterate $\bx_k$, where both are vectors in $\RR^n$, we would let $\bx_{k+1} = \bx_k + \bs$ be the next iterate.  By definition, because $\bF$ is differentiable,
\begin{equation}
    \bF(\bx_{k+1}) = \bF(\bx_k) + J_\bF(\bx_k) \bs + o(\|\bs\|)  \label{eq:nl:expandF}
\end{equation}
for some square matrix
\begin{equation}
J_\bF(\bx_k) = \begin{bmatrix}
    \frac{\partial F_0}{\partial x_0} & \dots & \frac{\partial F_0}{\partial x_{N-1}} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial F_{N-1}}{\partial x_0} & \dots & \frac{\partial F_{N-1}}{\partial x_{N-1}}  \end{bmatrix},  \label{eq:nl:jacdefn}
\end{equation}
and where $o(\|\bs\|)$ denotes some quantity that goes to zero as the length of the step $\|\bs\|$ goes to zero.  The matrix $J = J_\bF(\bx_k)$ is called the \emph{Jacobian} of $\bF$ at $\bx_k$, though we also refer to the function $\bx \mapsto J_\bF(\bx)$ as the Jacobian.

An iteration of Newton's method approximately solves nonlinear equation \eqref{eq:nl:equation} by truncating \eqref{eq:nl:expandF} and then seeking $\bs$ so that the updated value $\bF(\bx_{k+1})$ is zero.  That is, each Newton step computes $\bs$ by the equation
\begin{equation}
    0 = \bF(\bx_k) + J_\bF(\bx_k) \bs.
\end{equation}
Writing this equation in our previous style ``$A\bu=\bb$'' for linear systems, at each iteration $k$ we solve a linear system and then do a vector addition:
\begin{align}
    J_\bF(\bx_k) \bs &= - \bF(\bx_k)  \label{eq:nl:newtoneq}  \\
    \bx_{k+1} &= \bx_k + \bs  \label{eq:nl:newtonupdate}
\end{align}
This is Newton's method.  It is simple in theory.

Actual practice is not that much more complicated, especially with \PETSc in hand.  Despite the reputation of Newton iteration as fragile or scary, by using a bit of caution in ``moving'' to the new iterate as in \eqref{eq:nl:newtonupdate}, this will work out just fine on many small nonlinear systems \citep{Kelley2003}.  On big nonlinear systems we will need to pay more attention to the details of the linear solve at each step.\sidenote{It is important to emphasize that the nonlinear problem requires all of the tools for linear systems already considered in Chapters \ref{chap:linearsystem} and \ref{chap:structured}, and more.}  In either case, Newton iteration is the core technology for solving nonlinear problems, including nonlinear PDEs.

A small example gives us a start on exploring the details.

\medskip\noindent\hrulefill
\begin{example}  Nonlinear systems can be visualized as the intersections of curves, surfaces, or hypersurfaces, depending on dimension.  For example, given $b > 1$ this pair of nonlinear equations
    $$y = \frac{1}{b} e^{bx}, \qquad x^2+y^2 = 1,$$
form intersecting curves in the plane.  As shown in Figure \ref{fig:expcirclebasic} for the $b=2$ case, the curves intersect twice, once each in the first and second quadrants.

These equations are put in standard form \eqref{eq:nl:equation} by writing
\begin{equation}
\label{eq:nl:expcircleF}
\bF(\bx) = \begin{bmatrix}
           \frac{1}{b} e^{b x_0} - x_1 \\
           x_0^2 + x_1^2 - 1
           \end{bmatrix}
\end{equation}
for $\bx\in \RR^2$ with components $\bx = [x_0 \, x_1]^\top$.  Thus
\begin{equation}
J_\bF(\bx) = \begin{bmatrix}
    e^{b x_0} & -1 \\
    2 x_0   & 2 x_1 \end{bmatrix}
\end{equation}
Let $b=2$.  If we start the Newton iteration with $\bx_0 = [1 \, 1]^\top$ then the sequence of iterates from \eqref{eq:nl:newtoneq} and \eqref{eq:nl:newtonupdate} is
    $$\twovect{\bx}{0}{1}{1}, \quad \twovect{\bx}{1}{0.619203}{0.880797}, \quad \twovect{\bx}{2}{0.394157}{0.948623}, \quad \dots$$
as also shown in Figure \ref{fig:expcirclebasic}.

\noindent\hrulefill
%  FROM $ for N in 0 1 2; do ./expcircle -snes_fd -snes_max_it $N; done
\end{example}

\begin{marginfigure}
\includegraphics[width=1.2\textwidth]{expcirclebasic}
\caption{Newton iterates approach a solution of $\bF(\bx)=0$ for $\bF$ in \eqref{eq:nl:expcircleF} and $b=2$.}
\label{fig:expcirclebasic}
\end{marginfigure}


\section{\pSNES and call-backs}

We will do this calculation in \PETSc using a nonlinear-solver object of type \pSNES, an acronym which stands for ``scalable nonlinear equation solver.''  This object has the usual \texttt{Create/SetFromOptions/Destroy} sequence.  A \pSNES object also has a method by which we tell it about the function $\bF$.  This is a ``call-back'' in the sense that we supply a function which the \pSNES can call, inputing argument $\bx$ when it needs $\bF(\bx)$ during the Newton iteration.  In further examples we will also provide the \pSNES with a function which computes the derivative of $\bF$, i.e.~we will provide the Jacobian function $J_{\bF}$, but because this derivative can optionally be approximated by finite differences, our first code can avoid a ``call-back'' for the Jacobian.

Figure \ref{code:expcircle} shows our first \pSNES-using code \texttt{expcircle.c}.  It solves problem \eqref{eq:nl:equation} with $\bF$ from \eqref{eq:nl:expcircleF} and $b=2$.  The \texttt{main()} portion is mostly not surprising, but we summarize the contents anyway:  We start by allocating \pVec \texttt{x}, of fixed dimension 2, which will hold both the initial iterate $\bx_0$ and, once the Newton iteration is ended, the converged estimate of the solution to system \eqref{eq:nl:equation}.  Because both components of $\bx_0$ are $1$ in the above example, it is easy to initialize the \pVec with \texttt{VecSet()}.  Next a duplicate \pVec \texttt{r} is created because we need to supply it to the \pSNES as space for the (nonlinear) residual.  Then the \pSNES is created and configured by a \texttt{SNESCreate()} call which is totally uninteresting.  Next the formula \eqref{eq:nl:expcircleF} is supplied by a call to \texttt{SNESSetFunction()}.  This ``call-back'' sets the third argument of \texttt{SNESSetFunction()} to the name of the C function \texttt{FormFunction()}, which also appears in Figure \ref{code:expcircle}.  Next \texttt{SNESSetFromOptions()} is called so that, in particular, we will have run-time control both on how the Jacobian is calculated and on how the length of the step $\bs$ is actually determined; see below.  Then the system is solved by a call to \texttt{SNESSolve()}, which also supplies the initial iterate, and space for the solution, in \texttt{x}.  Finally the new state of \texttt{x}, presumably the converged solution, is printed at the command line using \texttt{VecView()} with a \texttt{STDOUT} viewer.

\vfill
\cinput{expcircle.c}{\CODELOC}{A first \pSNES-using code.  Solves nonlinear system \eqref{eq:nl:equation} with $\bF$ given in \eqref{eq:nl:expcircleF}.}{//START}{//END}{code:expcircle}

In order to match the calling sequence of \texttt{SNESSetFunction()}, our \texttt{FormFunction()} must have a particular ``signature'' as a C function:
\begin{code}
PetscErrorCode (*f)(SNES,Vec,Vec,void*)
\end{code}
That is, \texttt{FormFunction()} must have the \pSNES, the input $\bx$ (as the first \pVec), the output $\bF(\bx)$ (as the second \pVec), and perhaps additional information. such as parameters, in a ``user context;'' we will return to ``user contexts'' below.

Looking inside \texttt{FormFunction()} in Figure \ref{code:expcircle} we see new methods for extracting values from, and setting values in, a \pVec.  Previously we have used \texttt{VecSetValues()} to set values at given indices, but, as here, it is also possible to access the C array underlying the \pVec.  In this case we need to read entries of input \pVec \texttt{x} and then set entries of output \pVec \texttt{F}.  For the former we use the read-only array access method \texttt{VecGetArrayRead()} which supplies us with a read-only pointer \texttt{const PetscReal *ax}; thus \texttt{ax[0]} is the first entry of \pVec \texttt{x}, for example, and the C compiler can stop us from altering \pVec \texttt{x}.  Since we are setting entries of \pVec \texttt{F}, we do nearly the same but using non-\texttt{const} \texttt{PetscReal *aF} and method \texttt{VecGetArray()}.

To avoid conflicts with other methods reading or writing the same memory, these \texttt{VecGetArray()} and \texttt{VecGetArrayRead()} methods are matched by \texttt{VecRestoreArray()} and \texttt{VecRestoreArrayRead()} methods which ``free'' the \pVecs to be read or written by other methods.  In general:
\begin{quote}
\emph{Each \emph{\texttt{VecGetArray()}}-type call should be matched by the corresponding \emph{\texttt{VecRestoreArray()}} call once you are done with that \emph{\pVec}}.
\end{quote}

The actual content of \texttt{FormFunction()} is to implement formulas \eqref{eq:nl:expcircleF}.  Note \texttt{PetscExpReal()} computes the exponential function $e^x$; in fact it is just an alias for \texttt{exp()} from the standard library, but use of such functions means that the \PETSc configuration can link to consistent libraries and give access to them all just by including \texttt{petsc.h}, as we do in Figure \ref{code:expcircle}.

It is time to run this example.  We use option \texttt{-snes\_monitor}, which counts the Newton iterations and shows the residual norm $\|\bF(\bx_k)\|_2$:
\begin{cline}
$ cd c/ch4/
$ make expcircle
...
$ ./expcircle -snes_fd -snes_monitor
  0 SNES Function norm 2.874105323289e+00 
  1 SNES Function norm 8.591393113962e-01 
  2 SNES Function norm 1.609958353862e-01 
  3 SNES Function norm 1.106891696425e-02 
  4 SNES Function norm 6.618141730691e-05 
  5 SNES Function norm 2.420782802130e-09 
Vec Object: 1 MPI processes
  type: seq
0.319632
0.947542
\end{cline}
%$
Thus after 5 iterations the Newton method has reduced the residual norm by a factor of $10^9$ and stopped with solution $x_0=0.319632$ and $x_1=0.947542$.  Compare Figure \ref{fig:expcirclebasic}.

The above run also uses option \texttt{-snes\_fd}, the purpose of which the reader may already see.  Clearly the iteration \eqref{eq:nl:newtoneq} requires the Jacobian, but we have only supplied the \pSNES with an implementation of function $\bF(\bx)$, not with $J_{\bF}(\bx)$.  The entries of the latter matrix are derivatives, however, and we can approximate these by finite differences.  For example, the upper-left entry in $J_{\bF}(\bx)$ is approximately
\begin{equation}
\frac{\partial F_0}{\partial x_0} \approx \frac{F_0(x_0+\delta,x_1) - F_0(x_0,x_1)}{\delta}  \label{eq:nl:examplefdjac}
\end{equation}
if $\delta>0$.  It turns out that choosing $\delta = \sqrt{\eps}$, where $\eps$ is machine precision, gives a reasonably accurate approximation to the derivative if the inputs to $\bF$ are all of order approximately one.\cite{Kelley2003}

Thus with option \texttt{-snes\_fd}, the follow is what is done by \pSNES internally, at least in outline, to implement the Newton method \eqref{eq:nl:newtoneq} and \eqref{eq:nl:newtonupdate} on a system of dimension $N$:
\renewcommand{\labelenumi}{(\emph{\roman{enumi}})}
\begin{enumerate}
\item from the current iterate $\bx_k$, $\bF(\bx_k)$ is evaluated by calling the call-back function set in \texttt{SNESSetFunction()}, i.e.~\texttt{FormFunction()} in this case,
\item then \texttt{FormFunction()} is called $N$ more times to evaluate $\bF(\bx_k+\delta \be_j)$, for $j=0,\dots,N-1$, where $\be_j$ is the $j$th unit vector in $\RR^N$,
\item from formulas like \eqref{eq:nl:examplefdjac}, all entries of the $N\times N$ matrix $J_{\bF}(\bx_k)$ are approximated,
\item $N\times N$ linear system \eqref{eq:nl:newtoneq} is solved,
\item vector update \eqref{eq:nl:newtonupdate} is done,
\item a convergence test is made, and we repeat at (\emph{i}) if not converged.
\end{enumerate}
Note that with item (\emph{i}) and (\emph{ii}) we need to evaluate \texttt{FormFunction()} $N+1$ times per Newton iteration.  While this is no particular problem for a two-dimensional example, as here, it is a worrying amount of work to do if $N$ is large, as it would be for a system of nonlinear equations coming from discretizing a PDE.

If you run without option \texttt{-snes\_fd} then you get an error message about an un-assembled matrix:
\begin{cline}
$ ./expcircle
[0]PETSC ERROR: --------------------- Error Message -------------------------
[0]PETSC ERROR: Object is in wrong state
[0]PETSC ERROR: Matrix must be assembled by calls to MatAssemblyBegin/End();
...
\end{cline}
%$
This message is somewhat opaque unless you are conscious of the need to form the Jacobian $J=J_{\bF}$ at each Newton iteration.

To ask the \pSNES object to try to reduce the residual norm more, or less, we would use option \texttt{-snes\_rtol}.  For example,
\begin{cline}
$ ./expcircle -snes_fd -snes_monitor -snes_rtol 1.0e-14
\end{cline}
%$
asks for much more accuracy than the early run which used the default setting \texttt{-snes\_rtol 1.0e-8}.\marginnote{Recall that \texttt{./expcircle -snes\_fd -help |grep snes\_rtol} gets the default for this parameter.}  Instead of showing the resulting 6 Newton iterations as text output, Figure \ref{fig:newtonconvbasic} shows these residual norm values in a graph with log-scaling on the $y$-axis.  We see that the residual drops fairly abruptly.  FIXME quadratic convergence

\begin{figure}
\includegraphics[width=0.8\textwidth]{newtonconvbasic}
\caption{The characteristic look of the quadratic convergence of the Newton iteration: the residual drops abruptly.}
\label{fig:newtonconvbasic}
\end{figure}


\section{Exact Jacobians}

FIXME: adding exact jacobian AND struct holding $b$ (FIX CODE)

\cinputpart{expcircleJAC.c}{\CODELOC}{FIXME}{I}{//STARTJAC}{//ENDJAC}{code:expcircleJACI}

\cinputpart{expcircleJAC.c}{\CODELOC}{FIXME}{II}{//STARTADDJAC}{//ENDADDJAC}{code:expcircleJACII}


\section{Line search}

There remain two major ideas not covered above, i.e.~beyond the construction of the Newton iteration \eqref{eq:nl:newtoneq} itself, to turn Newton iteration into an effective tool:
\renewcommand{\labelenumi}{\roman{enumi})}
\begin{enumerate}
\item linesearch or trust region needed \citep{Kelley2003}
\item full range of linear tools (e.g.~Chapter \ref{chap:linearsystem}) should be applied to the linear system
\end{enumerate}

Regarding the former FIXME


\section{Example: 1D reaction-diffusion equation}

FIXME

\vfill
\cinputpart{reaction.c}{\CODELOC}{FIXME}{I}{//SETUP}{//ENDSETUP}{code:reactionI}

\cinputpart{reaction.c}{\CODELOC}{FIXME}{II}{//FUNCTION}{//ENDFUNCTION}{code:reactionII}

\cinputpart{reaction.c}{\CODELOC}{FIXME}{III}{//JACOBIAN}{//ENDJACOBIAN}{code:reactionIII}

\cinputpart{reaction.c}{\CODELOC}{FIXME}{IV}{//MAIN}{//ENDMAIN}{code:reactionIV}

\section{Optimization and nonlinear PDEs: $p$-Laplacian equation}

FIXME for $p>1$,
    $$I[u] = \int_\Omega \frac{1}{p} |\grad u|^p - fu$$
the variational equation is the weak form of a PDE:
\begin{align*}
I[u+\eps v] - I[u] &= \int_\Omega \frac{1}{p} |\grad u + \eps \grad v|^p + \frac{1}{p} |\grad u|^p - \eps f v \\
   &= \eps \left(\int_\Omega |\grad u|^{p-2} \grad u \cdot \grad v - f v\right) + O(\eps^2)
\end{align*}
so we want
    $$0 = \int_\Omega |\grad u|^{p-2} \grad u \cdot \grad v - f v$$
for all $v \in W^{1,p}_0(\Omega)$.  this can become a strong form by another integration by parts,
    $$0 = - \Div\left(|\grad u|^{p-2} \grad u\right) - f$$
which is \eqref{poissonsquare} if $p=2$

FIXME introduce $Q^1$ FEM on structured grid

\begin{marginfigure}
\input{q1hat.tikz}
\caption{FIXME}
\label{fig:q1hat}
\end{marginfigure}

FIXME code uses \texttt{SNESSetObjective()} only, though also \texttt{SNESSetFunction()}; no hand-made Jacobian at all

FIXME try NCG

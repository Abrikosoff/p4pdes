
The simplest thing to say about nonlinear equations is that they change change the functional form of the residual, compared to the linear case.  For a linear system the residual $\br$, defined in \eqref{residualdefn}, is a certain function of the unknowns $\bu$, namely
\begin{equation}
\bF(\bu) = \bb - A \bu. \label{eq:nl:linres}
\end{equation}
Now we consider cases in which $\bF$ is a higher-order polynomial, a transcendental function, or some more general function.

In fact, let us simply suppose for now that $\bF : \RR^N \to \RR^N$ is differentiable.  The input $\bx$ and output $\bF(\bx)$ are column vectors,\sidenote{Our name change $\bu\to\bx$, relative to \eqref{eq:nl:linres}, comes from thinking more geometrically about changes in the location of $\bx$.} so in that sense $\bF$ acts like square-matrix multiplication $\bx\mapsto A\bx$.  A linear solver like GMRES (Chapter \ref{chap:ls}) reduces the residual \eqref{eq:nl:linres} to zero by generating a sequence $\bu_k$.  Likewise for nonlinear $\bF$ we want to solve
\begin{equation}
   \bF(\bx) = 0   \label{eq:nl:equation}
\end{equation}
by iteration, generating improved estimates $\bx_k$ so that $\bF(\bx_k)$ goes to zero.

Newton's method linearizes \eqref{eq:nl:equation} around the most recent iterate and then ``moves'' $\bx$ to the location which solves the linear problem.  The new location is, we hope, closer to the solution.  Each iteration therefore solves a linear system, and we already have \PETSc technology for that.  However, the cost of performing the linearization must be taken into account, choosing a smart distance to move will require additional choices, and all existing choices regarding the linear solver---especially preconditioning---remain active.  Solving a nonlinear problem generally requires all the tools for linear systems considered in Chapter \ref{chap:ls}, and more.

Large systems of nonlinear equations arise in applications as there are many PDE-type physical models with nonlinearities.  However, this section is largely about finite-dimensional systems of nonlinear equations \eqref{eq:nl:equation} as problems of their own.  Late in this section we give an example of a discretized one-dimensional nonlinear PDE.  In the next Chapter we continue with another nonlinear PDE problem in two dimensions, though it can also be regarded as an optimization problem.


\section{Newton's method}

Suppose $\bx_k\in\RR^N$ is an approximation, whether good or bad, to the solution of \eqref{eq:nl:equation}.  If we have already determined a \emph{step} $\bs\in\RR^N$ away from this current iterate $\bx_k$ then $\bx_{k+1} = \bx_k + \bs$ will be the next iterate.  Because $\bF$ is differentiable, by definition
\begin{equation}
    \bF(\bx_{k+1}) = \bF(\bx_k) + J_\bF(\bx_k) \bs + o(\|\bs\|)  \label{eq:nl:expandF}
\end{equation}
for some square matrix
\begin{equation}
J_\bF(\bx_k) = \begin{bmatrix}
    \frac{\partial F_0}{\partial x_0} & \dots & \frac{\partial F_0}{\partial x_{N-1}} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial F_{N-1}}{\partial x_0} & \dots & \frac{\partial F_{N-1}}{\partial x_{N-1}}  \end{bmatrix}  \label{eq:nl:jacdefn}
\end{equation}
and some quantity $o(\|\bs\|)$ that goes to zero as the \emph{step length} $\|\bs\|$ goes to zero.  The matrix $J = J_\bF(\bx_k)$ is called the \emph{Jacobian} of $\bF$ at $\bx_k$; we also call the function $\bx \mapsto J_\bF(\bx)$ the Jacobian.

Each iteration of Newton's method approximately solves \eqref{eq:nl:equation} by truncating \eqref{eq:nl:expandF} and seeking $\bs$ so that the updated value $\bF(\bx_{k+1})$ is zero.  That is, each Newton step computes $\bs$ by the linear equation
\begin{equation}
    0 = \bF(\bx_k) + J_\bF(\bx_k) \bs.
\end{equation}
Writing the linear system in form ``$A\bu=\bb$,'' an iteration of Newton's method requires solving a linear system and then doing a vector addition:
\begin{align}
    J_\bF(\bx_k) \bs &= - \bF(\bx_k)  \label{eq:nl:newtoneq}  \\
    \bx_{k+1} &= \bx_k + \bs  \label{eq:nl:newtonupdate}
\end{align}
This is Newton's method.  It is fairly simple in theory.

Actual practice for Newton's method is also not that complicated with \PETSc in hand because it has a mature implementation of this core technology.  Despite the reputation of Newton iteration as fragile or scary, it works just fine on many nonlinear systems once one adds some protections.  For example, a line search may ``move'' a shorter distance to the new iterate than stated in \eqref{eq:nl:newtonupdate}; see page \pageref{sec:linesearch}.

Finite-dimensional nonlinear systems can be visualized as the problem of finding the intersections of curves, surfaces, or hypersurfaces, depending on dimension.  A small example gives us a start on the details.

\clearpage
\noindent\hrulefill
\begin{example}  Given parameter $b > 1$, the nonlinear equations
    $$y = \frac{1}{b} e^{bx}, \qquad x^2+y^2 = 1,$$
form intersecting curves in the plane.  The curves intersect twice, as shown in Figure \ref{fig:expcirclebasic}.  These equations are put in standard form \eqref{eq:nl:equation} by writing
\begin{equation}
\label{eq:nl:expcircleF}
\bF(\bx) = \begin{bmatrix}
           \frac{1}{b} e^{b x_0} - x_1 \\
           x_0^2 + x_1^2 - 1
           \end{bmatrix}
\end{equation}
for $\bx\in \RR^2$ with components $\bx = [x_0 \, x_1]^\top$.  Thus
\begin{equation}
\label{eq:nl:ecjacobian}
J_\bF(\bx) = \begin{bmatrix}
    e^{b x_0} & & -1 \\
    2 x_0   & & 2 x_1 \end{bmatrix}
\end{equation}
As also shown in Figure \ref{fig:expcirclebasic}, for $b=2$, if we start the Newton iteration with $\bx_0 = [1 \,\, 1]^\top$ then the sequence of iterates from \eqref{eq:nl:newtoneq} and \eqref{eq:nl:newtonupdate} is
    $$\twovect{\bx}{0}{1}{1}, \quad \twovect{\bx}{1}{0.619203}{0.880797}, \quad \twovect{\bx}{2}{0.394157}{0.948623}, \quad \dots$$

\begin{marginfigure}
\includegraphics[width=1.2\textwidth]{expcirclebasic}
\caption{Newton iterates approach a solution of $\bF(\bx)=0$ for $\bF$ in \eqref{eq:nl:expcircleF} and $b=2$.}
\label{fig:expcirclebasic}
\end{marginfigure}

\noindent\hrulefill
%  FROM $ for N in 0 1 2; do ./expcircle -snes_fd -snes_max_it $N; done
\end{example}


\section{Using \pSNES with call-backs} \label{sec:usingsnes}

We will compute the Newton iterates in the above example by using a nonlinear solver object of type \pSNES\sidenote{This acronym stands for ``scalable nonlinear equation solver.''} from \PETSc.  Note \pSNES has the usual \texttt{Create/SetFromOptions/Destroy} sequence.  Our code provides a function $\bF$ which is a ``call-back'' in the sense that we supply it to the \pSNES.  The \pSNES can then call it with argument $\bx$ when it needs $\bF(\bx)$ during the Newton iteration.  Later we will also provide the \pSNES with a function which computes the Jacobian.  However, because this derivative can instead be approximated by finite differences applied to $\bF$ itself, so that $J_\bF$ can be approximated by repeated $\bF$ evaluations, our first code avoids such a Jacobian ``call-back.''

The whole of \texttt{expcircle.c}, to solve the above Example, is in Code \ref{code:expcircle}.  The \texttt{main()} portion, which is mostly not surprising, starts by allocating \pVec \texttt{x} of fixed dimension 2.  This will hold both the initial iterate $\bx_0$ and, once the Newton iteration is ended, the converged estimate of the solution.  Because both components of $\bx_0$ are $1$, it is initialized using \texttt{VecSet()}.  Next a duplicate \pVec \texttt{r} is created; the \pSNES needs it as space for the (nonlinear) residual.  Then the \pSNES is created and configured.  Formula \eqref{eq:nl:expcircleF} is in method \texttt{FormFunction()}, which is supplied using \texttt{SNESSetFunction()}.  Then we call \texttt{SNESSetFromOptions()} both to give us run-time control on how the Jacobian is calculated\sidenote{Options \texttt{-snes\_fd} and \texttt{-snes\_mf} are allowed; see Table \ref{tab:snesjacobianoptions} on page \pageref{tab:snesjacobianoptions}.} and on how the length of the step $\bs$ is actually determined.\sidenote{Through \texttt{-snes\_linesearch\_type} and related options; see page \pageref{sec:linesearch}.}  Then \eqref{eq:nl:equation} is solved by a call to \texttt{SNESSolve()}, which also supplies \pVec \texttt{x}.  Finally the new values in \texttt{x}, presumably the converged solution, are printed at the command line using \texttt{VecView()} with a \texttt{STDOUT} viewer.

\vfill
\cinput{expcircle.c}{\CODELOC}{A first \pSNES-using code.  Solves nonlinear system \eqref{eq:nl:equation} with $\bF$ given in \eqref{eq:nl:expcircleF}.}{//START}{//END}{code:expcircle}

In order to match the calling sequence of \texttt{SNESSetFunction()}, \texttt{FormFunction()} must have a particular ``signature'' as a C function:
\begin{code}
PetscErrorCode (*f)(SNES,Vec,Vec,void*)
\end{code}
In particular, \texttt{FormFunction()} takes the input $\bx$ as the first \pVec and it generates output $\bF(\bx)$ as the second \pVec.\sidenote{For the second \pVec argument, note that a \pVec is in reality a \emph{pointer}, so passing a \pVec by value allows it to be modified.}  In other examples there may be additional information, such as parameters, passed to \texttt{FormFunction()} in a ``user context'' which is the fourth pointer argument ``\texttt{void*}.''  We will show how to pass parameters in the next code.

Looking inside \texttt{FormFunction()}, in Code \ref{code:expcircle}, we see a new method for extracting values from, and setting values in, a \pVec.  Previously we have used \texttt{VecSetValues()} to set values at given indices (Chapter \ref{chap:ls}), or we have used a \pDMDA structured grid method of access (Chapter \ref{chap:st}).  Here we access the C array underlying the \pVec.  Because we only need to read entries of input \pVec \texttt{x} we use the read-only array access method \texttt{VecGetArrayRead()} which supplies us with a read-only pointer ``\texttt{const PetscReal *ax},'' and, because of the \texttt{const} qualifier, the C compiler can stop us from altering \texttt{ax[0]}, for example.\sidenote{Try it!}  Since we are setting entries of \pVec \texttt{F}, we do nearly the same but using ``\texttt{PetscReal *aF},'' an unrestricted pointer, and method \texttt{VecGetArray()}.  The actual content of \texttt{FormFunction()} is to implement formulas \eqref{eq:nl:expcircleF}.  \texttt{PetscExpReal()}, which computes the exponential function $e^x$, is just an alias for \texttt{exp()} from the standard library (i.e.~\texttt{math.h}).\sidenote{Use of such \PETSc library functions means that the \PETSc configuration can link to consistent libraries.  We access these just by including \texttt{petsc.h}.}

To avoid conflicts with other methods which might want to read or write to the same memory, \texttt{VecGetArray()} and \texttt{VecGetArrayRead()} are matched by \texttt{VecRestoreArray()} and \texttt{VecRestoreArrayRead()}.  These methods ``free-up''  the \pVecs, but they do not deallocate anything.\sidenote{\texttt{VecDestroy()} does that.}  In general:
\begin{quote}
\emph{Each \emph{\texttt{VecGetArray()}}-type call should be matched by the corresponding \emph{\texttt{VecRestoreArray()}} call once you are done with the array.}
\end{quote}

It is time to run this example.  We use option \texttt{-snes\_monitor} to count the iterations and show the residual norm $\|\bF(\bx_k)\|_2$:
\begin{cline}
$ cd c/ch4/
$ make expcircle
...
$ ./expcircle -snes_fd -snes_monitor
  0 SNES Function norm 2.874105323289e+00 
  1 SNES Function norm 8.591393113962e-01 
  2 SNES Function norm 1.609958353862e-01 
  3 SNES Function norm 1.106891696425e-02 
  4 SNES Function norm 6.618141730691e-05 
  5 SNES Function norm 2.420782802130e-09 
Vec Object: 1 MPI processes
  type: seq
0.319632
0.947542
\end{cline}
%$
Thus after 5 iterations the Newton method has reduced the residual norm by a factor of $10^9$ and stopped with solution $x_0=0.319632$ and $x_1=0.947542$.  Compare Figure \ref{fig:expcirclebasic}.

The above run also uses option \texttt{-snes\_fd}, the purpose of which the reader may already see.  Clearly the Newton iteration \eqref{eq:nl:newtoneq} requires the Jacobian, but we have only supplied the \pSNES with an implementation of function $\bF(\bx)$, not with $J_{\bF}(\bx)$.  The entries of the latter matrix are derivatives, however, and we can approximate these by finite differences.  Specifically, let $\delta\ne 0$ and let $\be_j\in \RR^N$ denote a vector with entry one in the $j$th position and zeros otherwise.  An entry in matrix $J=J_{\bF}(\bx)$ is approximated
\begin{equation}
J_{ij} = \frac{\partial F_i}{\partial x_j} \approx \frac{F_i(\bx+\delta \be_j) - F_i(\bx)}{\delta}.  \label{eq:nl:fdjac}
\end{equation}
When using formula \eqref{eq:nl:fdjac}, \PETSc chooses $\delta$ internally.  For example, $\delta = \sqrt{\eps}$, where $\eps$ is machine precision, gives a reasonably-accurate approximation if the inputs to $\bF$ are all of order approximately one and the function $\bF$ can be accurately evaluated \citep{Kelley2003}.


\section{Inside \pSNES} \label{sec:insidesnes}

In outline, \pSNES does these steps to implement the Newton method:
\begin{quote}
	\renewcommand{\labelenumi}{(\emph{\roman{enumi}})}
	\renewcommand{\labelenumii}{\emph{\alph{enumii}}.}
	\begin{enumerate}
	\item from the current iterate $\bx_k$, $\bF(\bx_k)$ is evaluated using a call-back function set in \texttt{SNESSetFunction()}, e.g.~\texttt{FormFunction()} above,
	\item the Jacobian $J_{\bF}(\bx_k)$ is
	    \begin{enumerate}
	    \item computed and assembled by a call-back to user-supplied code if it is available, e.g.~\texttt{FormJacobian()} in code \texttt{ecjacobian.c} below, \emph{or}
	    \item computed and assembled by evaluating $\bF(\bx_k+\delta \be_j)$ for $j=0,\dots,N-1$, thus  calling \texttt{FormFunction()} $N$ times, and then using formula \eqref{eq:nl:fdjac} as many as $N^2$ times, according to the sparsity pattern of $J$, \emph{or}
	    \item computed and assembled by calling \texttt{FormFunction()} substantially fewer than $N$ times to compute $\bF(\bx_k+\delta \bv)$ for special vectors $\bv$ and using formula \eqref{eq:nl:fdjac} according to the sparsity pattern of $J$, by using a graph-coloring algorithm to construct the vectors $\bv$, \emph{or}
	    \item not assembled, but, in a Krylov iterative method for solving system \eqref{eq:nl:newtoneq}, the action $J_{\bF}(\bx_k) \bv$, of the Jacobian on vectors $\bv$, is computed by finite-differences,
        \end{enumerate}
	\item linear system \eqref{eq:nl:newtoneq} is solved for $\bs$ by some \pKSP object, using whatever additional preconditioning matrix which is chosen,
	\item vector update \eqref{eq:nl:newtonupdate} is done, with possible reduction in the length of $\bs$, \emph{and}
	\item a convergence test is made, and we repeat at (\emph{i}) if not converged.
	\end{enumerate}
\end{quote}

Our single run above of \texttt{expcircle.c} used option (\emph{ii})b for the Jacobian, but our next code will allow option (\emph{ii})a as well.  The graph-coloring technique (\emph{ii})c will be addressed starting on page \pageref{sec:nl:coloring}, and the Jacobian-free method in (\emph{ii})d starting on page \pageref{sec:JFNK}.  Issue (\emph{iv}), about possible step-length reduction, will be addressed starting on page \pageref{sec:linesearch}.

Option (\emph{ii})a is only possible if we supply a call-back function for the Jacobian through \texttt{SNESSetJacobian()} or similar.  For now, however, if you run \texttt{expcircle.c} without option \texttt{-snes\_fd} or \texttt{-snes\_mf} then you get an error message about an un-assembled matrix:
\begin{cline}
$ ./expcircle
[0]PETSC ERROR: --------------------- Error Message -------------------------
[0]PETSC ERROR: Object is in wrong state
[0]PETSC ERROR: Matrix must be assembled by calls to MatAssemblyBegin/End();
...
\end{cline}
%$
This message is somewhat opaque unless you are conscious of the need to form the Jacobian matrix at each Newton iteration.  That is, \emph{something} must supply a Jacobian at step (\emph{ii}).

One possible disadvantage of using  \texttt{-snes\_fd}, option (\emph{ii})b, namely that formula \eqref{eq:nl:fdjac} is only an approximation, but, in most cases using an approximate Jacobian in the Newton step is not problematical \citep{Kelley2003}.  On the other hand, there is a significant performance problem in using \eqref{eq:nl:fdjac} naively for PDE-type applications of Newton's method.  In steps (\emph{i}) and (\emph{ii})b together we evaluate $\bF$ through $N+1$ calls to \texttt{FormFunction()}, per Newton iteration.  This is a worrying amount of work if $N$ is large, as it would be for a system of nonlinear equations coming from discretizing a PDE.  For many PDE discretization schemes, however, the coloring algorithm in (\emph{ii})c makes finite difference Jacobians an efficient option by reducing the number of $\bF$ evaluations to a small constant, independent of $N$, per Newton step.  See the diffusion-reaction PDE example at the end of this Chapter, as well as examples in later Chapters.

There are systems where $\bF$ itself is an expensive function to evaluate, and even if not, evaluating $\bF$ can dominate the work in the Newton iteration.  The work done in solving linear system \eqref{eq:nl:newtoneq} is, of course, the other main concern.  A good \PETSc habit, to start right now, is to use \texttt{-log\_summary} to \emph{see} which kind of work is dominant.

In any case, the benefit of using options (\emph{ii})b, (\emph{ii})c, or (\emph{ii})d above is that we do not need to write any error-prone code based on taking derivatives of our function $\bF$.  Avoiding writing and debugging Jacobian implementations may speed implementation by reducing the time \emph{you} spend on the task.


\section{Residual norm in the Newton iteration}

Option \texttt{-snes\_rtol} specifies by what factor the \pSNES should try to reduce the residual norm.  The default accuracy corresponds to \texttt{-snes\_rtol 1.0e-8}; this default value can be printed by running
\begin{cline}
$ ./expcircle -snes_fd -help | grep snes_rtol
\end{cline}
%$
The example
\begin{cline}
$ ./expcircle -snes_fd -snes_monitor -snes_rtol 1.0e-14
\end{cline}
%$
thus asks for much more accuracy.  It may be a surprise that asking for a further $10^6$ reduction in residual norm requires only one more iteration, namely six iterations this time, but this is typical of the Newton iteration in the best cases.  Instead of showing the Newton iterations again as text output, Figure \ref{fig:newtonconvbasic} shows residual norms in a graph with log-scaling on the $y$-axis.

\begin{figure}
\includegraphics[width=0.8\textwidth]{newtonconvbasic}
\caption{The characteristic look of the quadratic convergence of the Newton iteration: the residual norm drops abruptly.}
\label{fig:newtonconvbasic}
\end{figure}

The residual norm drops abruptly in the Figure, reflecting the hoped-for best-case behavior of Newton iteration.  The per-iteration decrease is substantial in the sense that the error is proportional to the \emph{square} of the error at the last iteration.  This is the main reason the Newton iteration is so powerful.  The next theorem expresses such best-case behavior \citep[Theorems 1.1 and inequalities (1.13)]{Kelley2003}:

\begin{theorem}
Suppose that $\bF:\RR^N\to\RR^N$ is differentiable, $\bx^*$ is a solution of \eqref{eq:nl:equation}, $J_{\bF}$ is Lipschitz near $\bx^*$, and $J_{\bF}(\bx^*)$ is a nonsingular matrix.  Let $\|\cdot\|$ denote a vector norm and its induced matrix norm, and let $\kappa(A)=\|A^{-1}\| \|A\|$ denote the condition number of an invertible matrix $A$.  Let
\begin{equation}
\be_k = \bx_k-\bx^*
\end{equation}
be the \emph{error} at iteration $k$.  If $\bx_0$ is sufficiently close to $\bx^*$ then, in exact arithmetic,
\renewcommand{\labelenumi}{(\roman{enumi})}
\begin{enumerate}
\item there is $K\ge 0$ such that for all $k$ sufficiently large,
\begin{equation}
	\|\be_{k+1}\| \le K \|\be_k\|^2, \label{eq:nl:quadraticconvergence}
\end{equation}
\item and if $\kappa = \kappa\left(J_{\bF}(\bx^*)\right)$ then
\begin{equation}
	\frac{\|\be_k\|}{4 \kappa \|\be_0\|} \le \frac{\|\bF(\bx_k)\|}{\|\bF(\bx_0)\|} \le \frac{4 \kappa \|\be_k\|}{\|\be_0\|}. \label{eq:nl:errorresidualnormequiv}
\end{equation}
\end{enumerate}
\end{theorem}

\medskip
By definition, a sequence $\{\bx_k\}$ in $\RR^N$ \emph{converges quadratically to} $\bx^*$ if the sequence of errors $\{\be_k\}$ satisfies \eqref{eq:nl:quadraticconvergence} for some $K\ge 0$.  Thus the Theorem says that, under strong assumptions about the regularity and nonsingularity of the Jacobian, the iterates converge quadratically to a solution of \eqref{eq:nl:equation}.  Heuristically, once $\|\be_k\|$ gets reasonably small then, from then on, the number of correct digits in $\bx_k$ \emph{doubles} with each additional iteration.

We seem to see quadratic convergence in Figure \ref{fig:newtonconvbasic}, but actually it shows the residual norm $\|\bF(\bx_k)\|_2$ and not the error norm $\|\be_k\|_2$.  However, the second part of the Theorem says that the residual decrease at the $k$th iteration (i.e.~$\|\bF(\bx_k)\|/\|\bF(\bx_0)\|$) is within a factor, determined by the conditioning of the Jacobian at the solution, of the error decrease ($\|\be_k\|/\|\be_0\|$).  This idea is significant because the former quantity is computable.

The Theorem confirms that residual norm decay like that shown in Figure \ref{fig:newtonconvbasic} corresponds to quadratic convergence of $\bx_k$ to a solution $\bx^*$.  If we want to reduce the (generally-unknowable) error $\be_k$ by a given amount then it can suffice to reduce the residual norm by a comparable amount.  The factor $4 \kappa$ by which the two relative norms differ in \eqref{eq:nl:errorresidualnormequiv} is large if the conditioning of the Jacobian at the solution is poor, but, just as in the linear case, a large Jacobian condition number would also mean lost precision in solving \eqref{eq:nl:equation} by \emph{any} numerical means.\sidenote{Recall the numerical facts-of-life in Chapter \ref{chap:ls}.}


\section{\pSNES tolerances}

The amount of residual norm reduction is exactly what the option \texttt{-snes\_rtol} controls, that is, the iteration continues until
    $$\frac{\|\bF(\bx_k)\|_2}{\|\bF(\bx_0)\|_2} \le \text{\texttt{snes\_rtol}}.$$
To give the more complete story, however, note there are \emph{three} \pSNES tolerances, as listed in Table \ref{tab:snestolerances}.  The iteration stops as soon as one of these conditions is satisfied.  Note that $\bs_k$ denotes the solution to linear system \eqref{eq:nl:newtoneq}, the ``step'' at iteration $k$.

\medskip
\begin{table}
\begin{tabular}{lll}
\underline{Option}\hspace{0.2in} & \underline{Name}\hspace{0.2in} & \underline{Condition}\hspace{0.2in} \\
\texttt{-snes\_rtol X} & relative (\texttt{FNORM\_RELATIVE}) & $\|\bF(\bx_k)\|_2 \le \text{\texttt{X}}\, \|\bF(\bx_0)\|_2$ \\
\texttt{-snes\_atol X} & absolute (\texttt{FNORM\_ABS}) & $\|\bF(\bx_k)\|_2 \le \text{\texttt{X}}$ \\
\texttt{-snes\_stol X} & step-length (\texttt{SNORM\_RELATIVE}) & $\|\bs_k\|_2 \le \text{\texttt{X}}\, \|\bx_k\|_2$
\end{tabular}
\caption{The three ways \pSNES can succeed, thereby stopping the Newton iteration.} \label{tab:snestolerances}
\end{table}

\medskip
Option \texttt{-snes\_converged\_reason} reports which termination condition was active, using the name given in Table \ref{tab:snestolerances}.  For example,
\begin{cline}
$ ./expcircle -snes_fd -snes_converged_reason
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 5
...
\end{cline}
%$

One can force the \pSNES to use a subset of the stopping criteria by setting the tolerance to zero (\texttt{X} $=0$) in the unwanted condition(s).  The defaults for the three tolerances in the Table are \texttt{X} $=10^{-8},10^{-50},10^{-8}$, respectively.


\section{Convergence difficulties} \label{sec:divergence}

So far we have portrayed the Newton iteration in optimistic terms, but it is not magic and things can go wrong.  First note a key hypothesis in the above Theorem, namely that $\bx_0$ needs to be sufficiently close to $\bx^*$.  Even on well-behaved nonlinear equations, if $\bx_0$ is far from the solution then the iteration may take many steps before $\|\be_k\|$ becomes small enough so that quadratic convergence \eqref{eq:nl:quadraticconvergence} ``kicks in''.  For example, Figure \ref{fig:newtonconvdelayed} shows what happens if we use initial iterate $\bx_0=[10\,\, 10]^\top$ in the above Example.  About 16 iterations of slow progress\sidenote{From the constant slope shown in the Figure, this is evidently \emph{linear} convergence in which the residual norm is reduced by a constant factor.} is needed before the iterate $\bx_k$ enters the region around $\bx^*$ where the conclusions of the Theorem apply.  This region is sometimes known as the ``ball of quadratic convergence.''

\begin{figure}
\includegraphics[width=0.8\textwidth]{newtonconvdelayed}
\caption{Even in for well-behaved systems $\bF(\bx)=0$, if the initial iterate $\bx_0$ is far from the solution then quadratic convergence can be postponed for many iterations.}
\label{fig:newtonconvdelayed}
\end{figure}

Actual decrease in residual norm, as displayed in Figures \ref{fig:newtonconvbasic} and \ref{fig:newtonconvdelayed}, is also not guaranteed in general.  There is nothing intrinsic about the solution $\bs$ to \eqref{eq:nl:newtoneq} that implies $\|\bF(\bx_{k+1})\| \le \|\bF(\bx_k)\|$.  However, the ``line-search'' methods, described starting on page \pageref{sec:linesearch}, enforce residual norm decrease, or stop if it cannot be achieved.  On some equations the Newton iteration \eqref{eq:nl:newtoneq}, \eqref{eq:nl:newtonupdate} as it stands actually diverges from some initial states.  For an example, see Exercise \ref{chap:nl}.\ref{exer:nl:newtonatan}.  In these cases line-search methods will generally reduce the step length $\|\bs_k\|$ and this may ``globalize'' the convergence \citep{Kelley2003} in the sense of allowing convergence from a larger set of initial states.

Many real-world problems do not have the smoothness needed to apply the above Theorem.  In such cases regularization, continuation, or other procedures may be needed to make the Newton method effective.


\section{Exact Jacobians and passing parameters}

We have yet to exploit two critical possibilities when using a \pSNES, namely passing parameters through the call-back mechanism, so that they can be used inside the residual- and Jacobian-evaluation functions, and providing code for a Jacobian function $J_{\bF}(\bx)$.  The next code \texttt{ecjacobian.c}, in Figures \ref{code:ecjacobianI} and \ref{code:ecjacobianII}, implements both of these capabilities.  It is a ``model use'' of \pSNES.\sidenote{For cases without a structured grid of the \pDMDA type (Chapter \ref{chap:st}), anyway.  Compare \texttt{reaction.c} below.}

The first new idea in the code is the declaration of a C \texttt{struct} called \texttt{AppCtx} (``application context'').  It has just one element, the real parameter $b$ which appears in formulas \eqref{eq:nl:expcircleF} and \eqref{eq:nl:ecjacobian}, so a \texttt{struct} is not necessary here, but in future examples there will be more than one parameter to pass.  Next, the method \texttt{FormFunction()} in Code \ref{code:ecjacobianI} is almost the same as the one in \texttt{expcircle.c} (Code \ref{code:expcircle}), but the value of $b$ comes from the \texttt{struct} instead of being hard-wired as before.  In detail, the argument \texttt{void *ctx} is ``cast'' in the sense of the C language \citep{KernighanRitchie1988} to a pointer of type \texttt{AppCtx*}, and then the parameter is extracted, via the pointer, by ``\texttt{user->b}.''\sidenote{Note ``\texttt{user->b}'' is shorthand for ``\texttt{(*user).b}''.}  This awkward-seeming method of passing parameters allows the signature of \texttt{FormFunction()} to be precisely as before.

\cinputpart{ecjacobian.c}{\CODELOC}{Solves the same nonlinear system as \texttt{expcircle.c}, but with an exact Jacobian and parameter passing.}{I}{//START}{//END}{code:ecjacobianI}

The method \texttt{FormJacobian()} in Code \ref{code:ecjacobianI} is new.  It has similar structure and semantics to \texttt{FormFunction()}, but a new signature for this kind of call-back, namely
\begin{code}
PetscErrorCode (*J)(SNES,Vec,Mat,Mat,void*)
\end{code}
Input \pVec \texttt{x} and pointer \texttt{void *ctx} have the same meaning as in \texttt{FormFunction()}.  Also just as before, when reading \texttt{x} we can use \texttt{VecGetArrayRead()} and \texttt{VecRestoreArrayRead()}.

\cinputpart{ecjacobian.c}{\CODELOC}{This \texttt{main()} method allocates a \pMat to hold the Jacobian.}{II}{//STARTMAIN}{//ENDMAIN}{code:ecjacobianII}

The difference is that we must set a \pMat as output, based on formula \eqref{eq:nl:ecjacobian} in this case.  The roles of \texttt{MatSetValues()}, real array \texttt{v[4]} for the entries themselves, and integer arrays \texttt{row[2]} and \texttt{col[2]} as global indices, are all the same as in Chapter \ref{chap:ls}.  An interesting detail appears here, however.  There are actually \emph{two} output \pMats for \texttt{FormJacobian()} to set.  The first, called \texttt{J} here, corresponds to the Jacobian matrix itself, which, in this simple case, we want to supply.  The second, called \texttt{P} here, is the ``material'' we supply to build a preconditioner.  It might, at least in other cases, be a very poor approximation of the Jacobian.  A method like \texttt{FormJacobian()} sets one of these \pMats according to whether we intend to supply accurate derivatives of $\bF$ or not.  In fact, we assemble the \pMat \texttt{P} and, if the \pMat \texttt{J} is also present and is a different pointer, then we also assemble it.  In particular, these actions allow ``Jacobian-free'' Newton-Krylov methods to work; see page \pageref{sec:JFNK} below.

Looking at \texttt{main()} in Code \ref{code:ecjacobianII}, note that we create and configure a $2\times 2$ \pMat \texttt{J} to hold the Jacobian.  Our use of \texttt{MatCreate(), MatSetSizes(), MatSetFromOptions(),} and \texttt{MatSetUp()} on \texttt{J} mimics what we did for linear systems in Chapter \ref{chap:ls}.  However, this time when we set-up the \pSNES we pass \texttt{J} for two arguments,
\begin{code}
SNESSetJacobian(snes,J,J,FormJacobian,&user);
\end{code}
This means we provide the allocated space in \texttt{J} as both the Jacobian and preconditioner matrices, that is, for both the second and third \pMat arguments.

Now that we have assembled an exact Jacobian, we see that it and its finite-difference approximation produce nearly the same result on this small and well-behaved example:
\begin{cline}
$ make ecjacobian
...
$ ./ecjacobian -snes_monitor
  0 SNES Function norm 2.874105323289e+00 
  1 SNES Function norm 8.591392822370e-01 
  2 SNES Function norm 1.609958166309e-01 
  3 SNES Function norm 1.106891138388e-02 
  4 SNES Function norm 6.618107497046e-05 
  5 SNES Function norm 2.419259135755e-09 
...
$ ./ecjacobian -snes_monitor -snes_fd
  0 SNES Function norm 2.874105323289e+00 
  1 SNES Function norm 8.591393113962e-01 
  2 SNES Function norm 1.609958353862e-01 
  3 SNES Function norm 1.106891696425e-02 
  4 SNES Function norm 6.618141730691e-05 
  5 SNES Function norm 2.420782802130e-09 
...
\end{cline}
%$

\PETSc also helps with debugging exact-Jacobian code.  The finite-difference approximation of the Jacobian $J_{\bF}(\by)$, which needs only \texttt{FormFunction()} evaluations $\bF(\by)$, is compared to the result of \texttt{FormJacobian()} evaluations for a small selection of inputs $\by$.  This action comes from \texttt{-snes\_type test}.   Oddly, it generates an error message even in the case where the comparison shows that the implemented Jacobian is good:
\begin{cline}
$ ./ecjacobian -snes_type test
Testing hand-coded Jacobian, if the ratio is
O(1.e-8), the hand-coded Jacobian is probably correct.
Run with -snes_test_display to show difference
of hand-coded and finite difference Jacobian.
Norm of matrix ratio 1.9182e-08, difference 1.52973e-07 (user-defined state)
Norm of matrix ratio 1.21378e-08, difference 3.64505e-08 (constant state -1.0)
Norm of matrix ratio 1.9182e-08, difference 1.52973e-07 (constant state 1.0)
[0]PETSC ERROR: --------------------- Error Message ------------------------
[0]PETSC ERROR: Object is in wrong state
[0]PETSC ERROR: SNESTest aborts after Jacobian test: it is NORMAL behavior.
...
[0]PETSC ERROR: ----------------End of Error Message -------
...
\end{cline}
%$
See Exercise \ref{chap:nl}.\ref{exer:nl:snestestdisplay} for a bit more on this capability.


\section{Example: a nonlinear diffusion-reaction equation}

To get started on nonlinear PDE problems we look at a simple two-point boundary value problem.  Suppose $u(x)$ is the density of some substance and that $D>0$ is a constant.  An equation of the form
\begin{equation}
- D u'' - R(u) = f(x)  \label{eq:nl:diffusionreaction}
\end{equation}
models a time-independent combination of \emph{diffusion} and \emph{reaction} processes by the terms on the left, respectively.  The right-hand side $f(x)$ models an additional location-dependent \emph{source}.  The condition $R(u)+f(x)>0$ models the production (increase) of $u$, while negative values model destruction (decrease) of $u$.  If $u$ represents temperature, in which case $R(u)$ represents a heat-producing or -absorbing temperature-dependent reaction, according to its sign, \eqref{eq:nl:diffusionreaction} is the model of the equilibrium (steady state) temperature distribution.  These ideas are perhaps clearest if we note \eqref{eq:nl:diffusionreaction} is the steady-state of the time-dependent model
\begin{equation}
u_t = D u_{xx} + R(u) + f(x),  \label{eq:nl:drtimedependent}
\end{equation}
a generalization of the classical one-dimensional time-evolving heat equation.

We should be concerned about the solvability of \eqref{eq:nl:diffusionreaction} in the case where $R$ is an \emph{increasing} function of $u$.  If $R$ is positive and increasing then the ability of the diffusion term to ``damp-out'' maxima may be exceeded by the increasing production from large values of $u$, so that these terms cannot be in balance (as is stated in \eqref{eq:nl:diffusionreaction}).  If $R$ is negative and increasing then the analogous concern applies to minima of $u$.  These concerns are demonstrated by the example $R(u) = \lambda e^u$ with $\lambda>0$ in Exercise 4.\ref{exer:nl:bratu}, in which the problem is at least not numerically-solvable for sufficiently-large $\lambda$.  

On the other hand, it is also fair to say that equation \eqref{eq:nl:diffusionreaction} is a nonlinear elliptic PDE,\sidenote{By mild abuse of the letter ``P''.} but in one-dimension.  Elliptic PDE techniques show the problem is well-posed if $R$ is a \emph{non}-increasing function \citep[pages 93-94]{KinderlehrerStampacchia1980}.  To be concrete, consider Dirichlet boundary conditions
\begin{equation}
u(0)=\alpha \quad \text{and} \quad u(1)=\beta.  \label{eq:nl:drbcs}
\end{equation}
(We have chosen a convenient interval $x\in[0,1]$, but for other intervals we can shift and scale $x$ as needed.)  If $R$ is continuous and non-increasing then the nonlinear operator in \eqref{eq:nl:diffusionreaction} is strictly-monotone \citep{KinderlehrerStampacchia1980}.  Because it is also coercive on the appropriate function space,\sidenote{Namely the Sobolev space $H_0^1[0,1]$, after a linear change of variables to set $\alpha=\beta=0$.} which says intuitively that the highest-order (diffusion) term is effective at damping out large variations in $u$, corresponding to a large norm $\|u'\|_2$, abstract arguments show unique existence of a solution.

Now we consider the example
\begin{equation}
-u'' + \rho \sqrt{u} = 0. \label{eq:nl:drsqrt}
\end{equation}
% R(u) = - rho sqrt(|u|) decreasing so -u''+F(u)=0 with F increasing
This is of form \eqref{eq:nl:diffusionreaction} with $R(u) = - \rho \sqrt{u}$ and $f(x)=0$.  Because $R$ is non-increasing and continuous if $\rho>0$, the corresponding Dirichlet problem is well-posed.  (If desired $R$ can be extended as $R(u)=0$ for $u<0$, thereby becoming non-increasing and continuous for all of $\RR$.)

We are actually using \eqref{eq:nl:drsqrt} as a first example, however, because we want to verify our numerical solution using an exact solution, and its particular form makes this easy.  The solution is found \citep{Ockendonetal2003} by noting that both second-derivative and square-root operations convert certain 4th degree polynomials into quadratic polynomials.  We also get the boundary conditions from the exact solution.  Therefore we substitute $u(x)=M(x+1)^4$ into \eqref{eq:nl:drsqrt} and find $M=(\rho/12)^2$, $\alpha=M$, and $\beta=16 M$.

The actual finite-difference scheme we propose for \eqref{eq:nl:diffusionreaction} is quite obvious.  On an $N$ point grid it is
\begin{equation}
- D \frac{u_{j+1} - 2 u_j + u_{j-1}}{h^2} - R(u_j) = f(x_j)   \label{eq:nl:drfdscheme}
\end{equation}
where $h=1/(N-1)>0$ is the grid spacing, $x_j = j h$ for $j=0,1,\dots,N-1$, and $u_j \approx u(x_j)$.

Code \texttt{reaction.c} shown in Figures \ref{code:reactionI} and \ref{code:reactionII} solves this problem using finite-difference scheme \eqref{eq:nl:drfdscheme}, a \pSNES object for the Newton iteration, and a \pDMDA object for the finite-difference grid.  The first Figure shows three call-back methods.  First we compute the initial iterate---it is a linear function connecting the boundary conditions---and the exact solution in the method \texttt{InitialAndExactLocal()}.  Then we have the residual (\texttt{FormFunctionLocal()}) and Jacobian (\texttt{FormJacobianLocal()}) evaluation functions.

\cinputpart{reaction.c}{\CODELOC}{Call-back functions for \texttt{reaction.c}.}{I}{//CALLBACK}{//ENDCALLBACK}{code:reactionI}

\cinputpart{reaction.c}{\CODELOC}{In \texttt{main()} we create a \pDMDA, then \pVecs, and then a \pSNES.  We hand call-back functions to the \pSNES.  Then we solve the equation, get the numerical error, and clean up.}{II}{//MAIN}{//ENDMAIN}{code:reactionII}

These are ``\texttt{Local}'' methods in the sense that their inputs are C pointers for arrays instead of \pVecs.  Our implementation of \texttt{InitialAndExactLocal()} explains how this works.  As seen in Code \ref{code:reactionII}, before this method is called we use \texttt{DMDAVecGetArray()} on the \pVecs which need reading or setting, namely \texttt{u} and \texttt{uexact}.  This gives pointers of type \texttt{PetscReal*} which are then handed to \texttt{InitialAndExactLocal()}.  The call-back done inside \pSNES calls \texttt{FormFunctionLocal()} the same way.  The \pSNES also treats the input to  \texttt{FormJacobianLocal()} the same way.  Another aspect of these ``\texttt{Local}'' call-back methods is that a \texttt{DMDALocalInfo} struct is passed into the call-back functions.  Thus the local part of the grid (i.e.~\texttt{info.xs} and etc.) and the global grid size (i.e.~\texttt{info.mx}) can be accessed without needing the \pDMDA object itself.\sidenote{Recall Figure \ref{fig:localpartofgrid}.}

In \texttt{main()} (Code \ref{code:reactionII}) we use \texttt{DMDASNESSetFunctionLocal()} instead of \texttt{SNESSetFunction()}\sidenote{Compare \texttt{ecjacobian.c}.} because our local call-back functions have a different signature based on \pDMDA-derived pointers,   The Jacobian is set by a similar method \texttt{DMDASNESSetJacobianLocal()}.  Also, though we implement a Jacobian, we do not allocate a \pMat to hold it because the \pDMDA object has enough information about the grid and stencil so as to pre-allocate a \pMat internally.


\section{Convergence under grid refinement}

Recall that the resolution of a structured grid can be set with either \texttt{-da\_grid\_x N} or \texttt{-da\_refine N}.  For example, on a modestly-refined grid we can compare the number of Newton iterations using analytical and finite-difference Jacobian solutions like this:

\begin{cline}
$ ./reaction -snes_converged_reason -da_refine 6
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 513 point grid:  |u-u_exact|_inf/|u|_inf = 4.62255e-08
$ ./reaction -snes_converged_reason -da_refine 6 -snes_fd
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 513 point grid:  |u-u_exact|_inf/|u|_inf = 4.62255e-08
\end{cline}
The results are identical.  On the other hand, one can see the Newton iterates graphically by
\begin{cline}
$ ./reaction -da_refine 6 -snes_monitor_solution -draw_pause 1
\end{cline}
%$
One sees that the first Newton step moves us close to the solution (not shown).

\begin{figure}
\includegraphics[width=\textwidth]{reaction-conv}
\caption{This convergence evidence suggests \texttt{reaction.c} is correctly-implemented.}
\label{fig:nl:reaction-conv}
\end{figure}

Noting that our finite difference method has local truncation error $O(h^2)$, and because an exact solution allows computation of the numerical error, we can generate convergence data to check the implementation.  The result of this bash loop
\begin{cline}
$ for N in 0 2 4 6 8 10 12 14 16; do
>   ./reaction -da_refine $N -snes_rtol 1.0e-10; done
on 9 point grid:  |u-u_exact|_inf/|u|_inf = 0.000188753
on 33 point grid:  |u-u_exact|_inf/|u|_inf = 1.1825e-05
...
on 131073 point grid:  |u-u_exact|_inf/|u|_inf = 7.05476e-13
on 524289 point grid:  |u-u_exact|_inf/|u|_inf = 6.04273e-12
\end{cline}
is shown in Figure \ref{fig:nl:reaction-conv}.  If we ignore the result on the finest grid then the convergence rate is also $O(h^2)$.\sidenote{Error stagnation always occurs at some level of refinement because of accumulation of round-off error.}  We also see consistent evidence of quadratic convergence by adding \texttt{-snes\_monitor} to the above runs.  Thus we conclude our implementation is correct.


\section{Finite-difference Jacobians by ``coloring''} \label{sec:nl:coloring}

However, when we look closer at results from finite-difference evaluation of Jacobians we see that it is not really working.  This is because, in contrast to the fixed-dimension examples earlier in this Chapter, discretizing a PDE generates an arbitrarily-large number of unknowns.

Consider what happens when the grid has about 8000 points:
\begin{cline}
$ ./reaction -snes_converged_reason -da_refine 10 -snes_fd
Nonlinear solve did not converge due to DIVERGED_FUNCTION_COUNT iterations 1
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 0.0049428
\end{cline}
%$
The problem is that finite-difference Jacobian evaluation \eqref{eq:nl:fdjac} is requiring about 8000 evaluations of $\bF$ per Newton iteration, and this exceeds the default maximum count for the \pSNES.\sidenote{Again, ``\texttt{./reaction -help |grep snes\_}'' gets the default value of 10000.}  If we raise the function-count limit then we do get convergence, but at a huge performance cost relative to the use of an analytical Jacobian:
\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10 -snes_fd \
> -snes_max_funcs 100000
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.75635e-10
real 9.04
\end{cline}
%$
This run is slow because we have evaluated $\bF$ about 25000 times.  By comparison, the analytical Jacobian version evaluates $\bF$ four times and $J_{\bF}$ three times and runs a hundred times faster:
\label{etc:nl:bestreaction}
\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.75603e-10
real 0.04
\end{cline}
%$

Fortunately, given that it is desirable for initial prototyping and saving programmer effort, there is no need to abandon the finite-difference-Jacobian idea.  The additional tool needed to make it work for PDEs is applicable to many numerical schemes, especially on structured grids, which use a small set of grid values---a small stencil---in approximating the PDE at each location.\sidenote{Finite difference, element, and volume schemes for discretizing PDES all normally have this property.  Spectral methods, by contrast, have a large, sometimes global, stencil, and would not benefit.}  The idea is to ``color'' the nodes of the grid with a small set of colors so that each evaluation of $\bF$ allows the computation of as many entries of $J_{\bF}(\bx)$ as possible.  That minimizing the number of function evaluations in this context is equivalent to a graph coloring was first seen by \citet{ColemanMore1983}; see also the  more recent comprehensive review by \citet{Gebremedhinetal2005}.

Consider the seven-point grid, and the view of the corresponding matrix, that comes from this command:
\begin{cline}
$ ./reaction -da_grid_x 7 -mat_view ::ascii_dense
\end{cline}
%$
Figure \ref{fig:fdcolorreaction} first shows this grid and the obvious three-point stencil which comes from finite-difference scheme \eqref{eq:nl:drfdscheme}.  FIXME: flesh-out theory using Figure

\begin{equation}
\bv_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \end{bmatrix}, \qquad 
\bv_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}, \qquad
\bv_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}.
 \label{eq:nl:coloringvecs}
\end{equation}

\begin{figure}
\begin{tikzpicture}[scale=1.0]
\node at (-1.95,9.0) {grid with stencil:};
  \node at (0.5,9.3) {$u_0$};
  \node at (1.5,9.3) {$u_1$};
  \node at (2.5,9.3) {$u_2$};
  \node at (3.5,9.3) {$u_3$};
  \node at (4.5,9.3) {$u_4$};
  \node at (5.5,9.3) {$u_5$};
  \node at (6.5,9.3) {$u_6$};
  \draw[line width=0.7pt] (0.5,9.0) -- (6.5,9.0);
  \filldraw (0.5,9.0) circle (2.0pt);
  \filldraw (1.5,9.0) circle (2.0pt);
  \filldraw (2.5,9.0) circle (2.0pt);
  \filldraw (3.5,9.0) circle (4.0pt);
  \filldraw (4.5,9.0) circle (4.0pt);
  \filldraw (5.5,9.0) circle (4.0pt);
  \filldraw (6.5,9.0) circle (2.0pt);
  \draw[line width=1.5pt] (3.5,9.0) -- (5.5,9.0);
\node at (-1.8,8.0) {colored graph:};
  \node at (0.5,8.4) {$1$};
  \node at (1.5,8.4) {$2$};
  \node at (2.5,8.4) {$3$};
  \node at (3.5,8.4) {$1$};
  \node at (4.5,8.4) {$2$};
  \node at (5.5,8.4) {$3$};
  \node at (6.5,8.4) {$1$};
  \draw (0.5,8.0) circle (4.0pt);
  \draw[line width=0.6pt] (0.65,8.0) -- (1.35,8.0);
  \draw (1.5,8.0) circle (4.0pt);
  \draw[line width=0.6pt] (1.65,8.0) -- (2.35,8.0);
  \draw (2.5,8.0) circle (4.0pt);
  \draw[line width=0.6pt] (2.65,8.0) -- (3.35,8.0);
  \draw (3.5,8.0) circle (4.0pt);
  \draw[line width=0.6pt] (3.65,8.0) -- (4.35,8.0);
  \draw (4.5,8.0) circle (4.0pt);
  \draw[line width=0.6pt] (4.65,8.0) -- (5.35,8.0);
  \draw (5.5,8.0) circle (4.0pt);
  \draw[line width=0.6pt] (5.65,8.0) -- (6.35,8.0);
  \draw (6.5,8.0) circle (4.0pt);
  \draw[line width=0.75pt] (0.6,7.9) .. controls (1.4,7.5) and (1.6,7.5) .. (2.4,7.9);
  \draw[line width=0.75pt] (1.6,7.9) .. controls (2.4,7.5) and (2.6,7.5) .. (3.4,7.9);
  \draw[line width=0.75pt] (2.6,7.9) .. controls (3.4,7.5) and (3.6,7.5) .. (4.4,7.9);
  \draw[line width=0.75pt] (3.6,7.9) .. controls (4.4,7.5) and (4.6,7.5) .. (5.4,7.9);
  \draw[line width=0.75pt] (4.6,7.9) .. controls (5.4,7.5) and (5.6,7.5) .. (6.4,7.9);
\node at (-2.4,6.5) {generate columns of $J$:};
  \draw[xstep=1.0,ystep=1.0,black,thin] (0.0,0.0) grid (7.0,7.0);
  \node at (0.5,6.5) {$1$};
  \node at (0.5,5.5) {$1$};
  \node at (1.5,6.5) {$2$};
  \node at (1.5,5.5) {$2$};
  \node at (1.5,4.5) {$2$};
  \node at (2.5,5.5) {$3$};
  \node at (2.5,4.5) {$3$};
  \node at (2.5,3.5) {$3$};
  \node at (3.5,4.5) {$1$};
  \node at (3.5,3.5) {$1$};
  \node at (3.5,2.5) {$1$};
  \node at (4.5,3.5) {$2$};
  \node at (4.5,2.5) {$2$};
  \node at (4.5,1.5) {$2$};
  \node at (5.5,2.5) {$3$};
  \node at (5.5,1.5) {$3$};
  \node at (5.5,0.5) {$3$};
  \node at (6.5,1.5) {$1$};
  \node at (6.5,0.5) {$1$};
\end{tikzpicture}
\caption{FIXME: coloring in 1D looks like this}
\label{fig:fdcolorreaction}
\end{figure}

FIXME: the result is fast
\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10 -snes_fd -snes_fd_color
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.75633e-10
real 0.06
\end{cline}
%$


\section{Jacobian-free Newton-Krylov (JFNK)} \label{sec:JFNK}

Besides the finite-difference Jacobian approach we have already seen, which uses equation \eqref{eq:nl:fdjac} to compute the entries of an assembled Jacobian matrix, there is a different approach which approximates the Jacobian-times-vector product by a finite difference.  This approach must be used with a Krylov-type method to solve the Newton step equation.  It goes by the name ``Jacobian-free Newton-Krylov'' \citep{KnollKeyes2004} or just ``JFNK''.

To introduce JFNK we recall two things.  First is Newton equation \eqref{eq:nl:newtoneq} for step $\bs$ based on the current iterate $\bx=\bx_k$, namely
\begin{equation}
    J \bs = - \bF(\bx) \label{eq:nl:newtoneqforJFNK}
\end{equation}
where $J = J_\bF(\bx)$.  Second is the Krylov space definition \eqref{eq:li:krylovdefn}, here with matrix $J$ and vector $\br$:
\begin{equation}
    \mathcal{K} = \operatorname{span}\{\br,J\br,J^2\br,\dots,J^{n-1}\br\}.  \label{eq:nl:krylovagain}
\end{equation}

Suppose we use initial estimate $\bs\approx 0$ \citep{KnollKeyes2004} of the solution to \eqref{eq:nl:newtoneqforJFNK}.  The residual is $\br = - \bF(\bx) - J 0 = - \bF(\bx)$.  From this vector we need to compute $J\br$, $J^2\br=J(J\br)$, and so on, to solve \eqref{eq:nl:newtoneqforJFNK} from the Krylov space $\mathcal{K}$.  However, by definition of the derivative of $\bF$, the same idea as equation \eqref{eq:nl:expandF}, if $\bv$ is any vector then
\begin{equation}
J \bv \approx \frac{\bF(\bx+\delta \bv) - \bF(\bx)}{\delta} \label{eq:nl:JFNKbasic}
\end{equation}
if $\delta \ne 0$ is small.  That is, the Jacobian-vector product can be approximated by a finite-difference, thereby avoiding evaluation of the entries of $J$ by finite-difference equation \eqref{eq:nl:fdjac}. In fact we can avoid assembling $J$ as a matrix!

Note $\br = - \bF(\bx)$ comes from evaluating $\bF$.  From the above ideas, each additional vector in the Krylov basis \eqref{eq:nl:krylovagain} can be approximated using one additional evaluation of $\bF$ by the formula
\begin{equation}
J^{\ell} \br \approx \frac{\bF(\bx+\delta J^{\ell-1}\br) - \bF(\bx)}{\delta} \label{eq:nl:JFNKiteration}
\end{equation}
for $\ell=1,\dots,n$.  Equation \eqref{eq:nl:JFNKiteration} is the central idea in JFNK.

Observe that use of the earlier finite difference formula \eqref{eq:nl:fdjac} to compute a generic $N\times N$ Jacobian $J$ would require $N$ evaluations of $\bF$.  Thus the two evaluations of $\bF$ needed for \eqref{eq:nl:JFNKbasic} to compute $J\bv$, or the $n$ evaluations of $\bF$ needed to compute the whole Krylov basis \eqref{eq:nl:krylovagain}, seems very efficient.  However, there are two countervailing points to make: (i) coloring greatly reduces the cost of using \eqref{eq:nl:fdjac} for many PDEs, and (ii) once you have a matrix you can do many things, such as matrix factorizations, other than computing a matrix-vector product or a Krylov basis.  In other words, JFNK is a good strategy to the extent that it actually \emph{works}, which we now pursue in more detail.

JFNK is implemented in \PETSc and invoked by option \texttt{-snes\_mf}:
\begin{cline}
$ ./reaction -snes_converged_reason -snes_mf
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 9 point grid:  |u-u_exact|_inf/|u|_inf = 0.000188753
\end{cline}
%$
Note that ``\texttt{mf}'' in the option stands for ``matrix-free'', but we shall see below that there may be a matrix involved in JFNK anyway.  The method never involves fully-assembling the Jacobian matrix itself, however, and thus the ``Jacobian-free'' label is justified.

Our enthusiasm for JFNK is damped when we try refined grids in the 1D diffusion-reaction PDE example.  Observe we are using an \emph{un}preconditioned Krylov method; that is, option \texttt{-snes\_mf} corresponds to JFNK as stated in \eqref{eq:nl:JFNKiteration}.  To see the bad news in an example, note $J$ is symmetric in the \texttt{reaction.c} case.\sidenote{Well \dots nearly so.  See exercise 4.\ref{exer:nl:symmetrizeJ}.}  Thus we try the CG Krylov method (Chapter \ref{chap:ls}) and ask \PETSc for both the number of Newton and Krylov iterations:
\begin{cline}
$ ./reaction -snes_converged_reason -snes_mf -ksp_converged_reason -ksp_type cg -da_refine 6
  Linear solve converged due to CONVERGED_RTOL iterations 511
  Linear solve converged due to CONVERGED_RTOL iterations 504
  Linear solve converged due to CONVERGED_RTOL iterations 508
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 513 point grid:  |u-u_exact|_inf/|u|_inf = 4.62255e-08
$ ./reaction -snes_converged_reason -snes_mf -ksp_converged_reason -ksp_type cg -da_refine 7
  Linear solve converged due to CONVERGED_RTOL iterations 1023
  Linear solve converged due to CONVERGED_RTOL iterations 1006
  Linear solve converged due to CONVERGED_RTOL iterations 1015
  Linear solve converged due to CONVERGED_RTOL iterations 950
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 4
on 1025 point grid:  |u-u_exact|_inf/|u|_inf = 1.15576e-08
\end{cline}
These large iteration counts, which illustrate the well-known doubling of CG iterations with each doubling of dimension, remind us of the flaw in unpreconditioned Krylov methods addressed in Chapter \ref{chap:ls}.  The default GMRES solver is no better; we simply get a linear solve failure:
\begin{cline}
$ ./reaction -snes_converged_reason -snes_mf -ksp_converged_reason -da_refine 7
  Linear solve did not converge due to DIVERGED_ITS iterations 10000
Nonlinear solve did not converge due to DIVERGED_LINEAR_SOLVE iterations 0
on 1025 point grid:  |u-u_exact|_inf/|u|_inf = 0.217386
\end{cline}
%$

In other words, we see here an excellent reminder to apply the Krylov solver to a \emph{preconditioned} form of the Newton equation \eqref{eq:nl:newtoneqforJFNK}.  Recall there are left- and right-sided versions of preconditioning, equations \eqref{introleftpre} and \eqref{introrightpre} in Chapter \ref{chap:ls}.  For equation \eqref{eq:nl:newtoneqforJFNK}, using an invertible preconditioning matrix $M$, these versions are
\begin{align}
(M^{-1} J) \bs &= - M^{-1} \bF(\bx), \quad \text{and} \label{eq:nl:leftpre} \\
(J M^{-1}) (M \bs) &= - \bF(\bx), \label{eq:nl:rightpre}
\end{align}
respectively.  Left preconditioning \eqref{eq:nl:leftpre} requires no further comment because the action of $M^{-1} J$ on some vector $\bv$ is a straightforward composition of \eqref{eq:nl:JFNKbasic} followed by application of $M^{-1}$.  However, right preconditioning \eqref{eq:nl:rightpre}, in a Krylov iteration, uses a two step process.  The action of $J M^{-1}$ on $\bv$ is computed by first applying $M^{-1}$ to $\bv$, namely by solving $M \by = \bv$, and then using
\begin{equation}
(J M^{-1}) \bv = J \by \approx \frac{\bF(\bx+\delta \by) - \bF(\bx)}{\delta} \label{eq:nl:JFNKwithrightpre}
\end{equation}

Recall that, when preconditioning, matrices $M$ and $M^{-1}$ are usually never assembled.  Even if an assembled preconditioner-material matrix $P$ is present, $M$ may be constructed nontrivially from $P$.  For example, if we supply some assembled matrix $P$ as the preconditioner material, but we ask for ILU($0$) preconditioning,\sidenote{I.e.~the default corresponding to option \texttt{-pc\_type ilu}.} then $M$ is actually the product of the ILU($0$) factors of $P$, and thus it is generally not equal to $P$.


\section{Jacobian cases} \label{sec:jacobiancases}

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (.7,.8) -- (.25,.15) -- cycle;}
\def\bigcheckmark{\tikz\fill[scale=0.6](0,.35) -- (.25,0) -- (.7,.8) -- (.25,.15) -- cycle;}

At this point we risk overwhelming the reader with options, so we pause to review the possibilities before testing them on the \texttt{reaction.c} problem.  Table \ref{tab:snesjacobianoptions} summarizes options relating to residual and Jacobian call-backs in \pSNES-using codes.  If no Jacobian or approximate Jacobian routine is provided in the user-written code then only finite-difference evaluation of an assembled Jacobian matrix (i.e.~\texttt{-snes\_fd}) or finite-difference evaluation of the Jacobian-vector product inside the Krylov method (\texttt{-snes\_mf}, i.e.~JFNK) are available.  If a Jacobian routine \emph{is} provided then the Newton iteration itself occurs when no option is given.  However, the provided Jacobian may instead be used only to precondition the JFNK Jacobian-vector product, which is option \texttt{-snes\_mf\_operator}.  This last option may give quadratic convergence even if the provided Jacobian is inexact, that is, even if $P$ is a somewhat-poor approximation of the Jacobian.

\begin{table}
\begin{tabular}{lllll}
implemented: &\underline{no option}\hspace{0.0in} & \underline{\texttt{-snes\_fd}} & \underline{\texttt{-snes\_mf}} & \underline{\texttt{-snes\_mf\_operator}} \\
only $\bF$      & error           & $\bigcheckmark$ & $\bigcheckmark$ & error \\
$\bF$ and $P$   & $\bigcheckmark$ & $\checkmark$    & $\checkmark$    & $\bigcheckmark$ \\
$\bF$ and $J$   & $\bigcheckmark$ & $\checkmark$    & $\checkmark$    & $\checkmark$
\end{tabular}
\caption{Jacobian options when using \pSNES.  Symbol ``$P$'' denotes an easy-to-invert approximate Jacobian while ``$J$'' denotes the actual Jacobian.  A big check mark shows recommended usage.} \label{tab:snesjacobianoptions}
\end{table}

From the code side we can say that at a minimum, a method for $\bF$ must be implemented in all cases, as there is no other way for \PETSc to know what equations you are solving.  There are two ways of providing $\bF$:  If the problem is based on a structured grid, as in \texttt{reaction.c} above, use \texttt{DMDASNESSetFunctionLocal()}.  In general, use \texttt{SNESSetFunction()}.

If \emph{only} $\bF$ is provided, do not create or preallocate a \pMat for the Jacobian, as this is done internally by the \pSNES for \texttt{-snes\_fd} runs, while no assembled Jacobian \pMat exists for \texttt{-snes\_mf} runs.

If an exact ($J$) or approximate ($P$) Jacobian function is implemented then there are two cases:
\begin{quote}
  \renewcommand{\labelenumi}{(\roman{enumi})}
  \begin{enumerate}
  \item On a structured grid, provide the function (e.g.~``\texttt{FormJacobianLocal()}'') using \texttt{DMDASNESSetJacobianLocal()}.  In this case the \pMat holding the Jacobian is internal, and does not need to be created or preallocated in user code.
  \item In a general nonlinear equation solve, namely one not based on a structured grid, first declare a \pMat variable, say ``\texttt{J}'', and then call \texttt{MatCreate()}, \texttt{MatSetSizes()}, and \texttt{MatSetFromOptions()} on it.  Then call either \texttt{MatSetUp()} or \texttt{MatXAIJPreallocate()} on \texttt{J}, to allocate space for the assembled Jacobian matrix.  Then provide both the call-back code for function $J$ or $P$ (e.g.~``\texttt{FormJacobian()}''), and the \pMat \texttt{J} for both \pMat arguments, through \texttt{SNESSetJacobian()}.
  \end{enumerate}
\end{quote}
In either case (i) or (ii), the Jacobian-implementing code only needs to set values in the second ``preconditioner'' \pMat argument, assuming in (ii) that \texttt{J} is provided for both \pMat arguments of   \texttt{SNESSetJacobian()} as described above.  The code should, however, \emph{assemble} both \pMat arguments.

As practical advice for the debugging stage, you know you have correctly- and fully-implemented the Jacobian, and you have found an adequate initial iterate, if all four options in Table \ref{tab:snesjacobianoptions} work and each gives apparent quadratic convergence when looking at the residual norm decay shown by \texttt{-snes\_monitor}.  As the reader may check, this is the situation for the last two codes \texttt{ecjacobian.c} and \texttt{reaction.c}.


\section{Testing JFNK with preconditioning} \label{sec:testsnesmfoperator}

We now do refined-grid runs of \texttt{reaction.c} to show these alternatives in action.  We have already seen that the \texttt{-snes\_fd\_color} option is effective for reducing evaluations of $\bF$ if a Jacobian is not implemented.  In the same case, the un-preconditioned JFNK method \texttt{-snes\_mf} has serious difficulties as it requires unreasonable numbers of Krylov iterations and function evaluations.  However, what if a Jacobian is available, but it is only approximate?

For example, suppose we modify this line in \texttt{reaction.c}, in Code \ref{code:reactionI},
\begin{code}
    col[1] = i;    v[1] = 2.0 - h*h * dRdu;
\end{code}
to remove ``\texttt{- h*h * dRdu}'', to get
\begin{code}
    col[1] = i;    v[1] = 2.0;
\end{code}
This change, which we call a ``\texttt{J->P}'' below, keeps the tridiagonal sparsity pattern of the Jacobian and it preserves the spectral character of the nonlinear operator $-u''+\rho \sqrt{u}$ in \eqref{eq:nl:drsqrt} by keeping the highest-order term, but it removes the influence of the nonlinear term from the Jacobian.

With this change, convergence is slowed for no option, i.e.~when using the approximate Jacobian as though it were exact.  Specifically, on a coarse grid the number of iterations goes from 4 to 15, and the reported residual norms---here partly suppressed---suggest that the convergence is no longer quadratic:
\begin{cline}
$ ./reaction -da_refine 4 -snes_monitor    # before change
  0 SNES Function norm 1.671129624018e-02 
  1 SNES Function norm 3.609252641302e-04 
  2 SNES Function norm 4.167490508951e-07 
  3 SNES Function norm 4.935230509260e-13 
on 129 point grid:  |u-u_exact|_inf/|u|_inf = 7.39662e-07
$ ./reaction -da_refine 4 -snes_monitor    # with J->P change
  0 SNES Function norm 1.671129624018e-02 
  1 SNES Function norm 3.822032062916e-03 
...
 14 SNES Function norm 3.879363487638e-10 
 15 SNES Function norm 1.119521798815e-10 
on 129 point grid:  |u-u_exact|_inf/|u|_inf = 7.38159e-07
\end{cline}
This loss of performance for the Newton iteration, as caused by a significantly in-exact Jacobian, is expected in theory \citep{Kelley2003}.

However, for this modified Jacobian case, \texttt{-snes\_mf\_operator} is now a fast option for higher-resolution grids, fully competitive with the exact Jacobian and finite-difference-by-coloring cases already seen.  Again on a $\approx 8000$ point grid, using the approximate Jacobian ``as is'' causes too many Newton iterations and less-than-quadratic convergence:
\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10    # with J->P change
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 15
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.36236e-09
real 0.13
\end{cline}
%$
Now we try \texttt{-snes\_mf\_operator}, which is designed for this approximate-Jacobian situation:
\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10 -snes_mf_operator  # with J->P change
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 4
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.80588e-10
real 0.05
\end{cline}
%$
The number of iterations and the time are significantly reduced, and both are comparable to the exact Jacobian case (page \pageref{etc:nl:bestreaction}).

This confirms, in a simple PDE example, the idea that preconditioning the matrix-free Jacobian action by the use of an approximate Jacobian is effective.  As a general rule, for large problems on a structured grid where coloring can be applied, \texttt{-snes\_fd\_color} is often effective if no Jacobian is implemented.  (On an unstructured grid the coloring method requires additional work.)  Implementing an approximate Jacobian and using \texttt{-snes\_mf\_operator} may be a good compromise which saves programmer time by avoiding a full Jacobian implementation.

One might give this advice about \pSNES and the Newton iteration:  Before implementing a Jacobian, try finite-difference evaluation  \texttt{-snes\_fd}, and look at whether coloring applies to your case.  Then consider implementing the exact Jacobian $J$.  If that seems like too much work, or if it is too error-prone, consider implementing a simpler/faster approximate Jacobian $P$; in the PDE case $P$ might only capture the highest-order derivatives in $J$.  To test convergence with $P$, compare ``no option'' runs, where less-than-quadratic convergence is expected, and \texttt{-snes\_mf\_operator} runs where quadratic convergence may be recovered.


\section{Line search Newton methods} \label{sec:linesearch}

As already hinted, once the Newton step $\bs_k$ solving equation \eqref{eq:nl:newtoneq} is computed, the new iterate $\bx_{k+1} = \bx_k + \bs_k$ from \eqref{eq:nl:newtonupdate} may not be what we want.  For example, the residual norm $\|\bF(\bx_{k+1})\|$ may exceed $\|\bF(\bx_{k})\|$, suggesting that no progress is being made in solving \eqref{eq:nl:equation}, that is, $\bF(\bx)=0$.  Improving this situation is the problem of \emph{globalizing} the convergence of the Newton iteration, so that the iterates $\bx_k$ are more likely to head toward the sometimes-small region of quadratic convergence around the solution $\bx^*$ to \eqref{eq:nl:equation}.

The \emph{line search} globalization technique \citep{DennisSchnabel1983} is to replace \eqref{eq:nl:newtonupdate} with
\begin{equation}
\bx_{k+1} = \bx_k + \lambda_k \bs_k.  \label{eq:nl:linesearchupdate}
\end{equation}
That is, we accept $\bs_k$ as a search direction for solving \eqref{eq:nl:newtoneq}, but use an actual step of less ($\lambda_k < 1$), or occasionally more ($\lambda_k > 1$), than in \eqref{eq:nl:newtonupdate}.  The question, of course, is how to choose $\lambda_k$.  As we are solving a system of nonlinear equations \eqref{eq:nl:equation}, there is no pre-existing sense of ``better'' or ``worse'' approximations, which would exist in an optimization context.  But we may introduce a particular \emph{merit function} \citep{NocedalWright2006} to provide such a sense, namely
\begin{equation}
\phi(\bx) = \frac{1}{2} \|\bF(\bx)\|_2^2.  \label{eq:nl:linesearchmeritfunction}
\end{equation}
This merit function is used in all \pSNES line searches, unless a different objective function is provided (next Chapter).

To determine $\lambda_k$ one could perhaps solve a one-dimensional optimization problem such as
\begin{equation}
\min_{0<\lambda} \phi(\bx_k + \lambda_k \bs_k),  \label{eq:nl:linesearchperhapsopt}
\end{equation}
thereby making the residual 2-norm as small as possible along the search direction.  However, solving this optimization problem accurately is rarely-justified because we are merely using the solution to compute the next Newton iterate; we hope the Newton iteration itself generates rapid convergence.

A practical line-search tries candidate values of $\lambda=\lambda_k$, testing them for whether they reduce $\phi(\bx_k + \lambda \bs_k)$.  The initial try is usually $\lambda=1$.  A \emph{sufficient  decrease} condition with parameter $\alpha\in(0,1)$ is to require
\begin{equation}
\phi(\bx_k + \lambda \bs_k) \le \phi(\bx_k) + \alpha \lambda \bs_k^\top \grad \phi(\bx_k).  \label{eq:nl:linesearchsuffdecrease}
\end{equation}
The \PETSc default for $\alpha$ is $10^{-4}$, corresponding to an easy-to-satisfy reduction criterion.  \PETSc line searches will stop the Newton iteration with an error if such a decrease cannot be found.

Note that for merit function \eqref{eq:nl:linesearchmeritfunction}, and by using \eqref{eq:nl:newtoneq}, we have
\begin{align}
\bs_k^\top \grad \phi(\bx_k) &= \bs_k^\top J_{\bF}(\bx_k)^\top \bF(\bx_k) = \left(J_{\bF}(\bx_k) \bs_k\right)^\top \bF(\bx_k) \label{eq:nl:linesearchinitslope} \\
  &= - \|\bF(\bx_k)\|_2^2 \notag
\end{align}
on the right side of \eqref{eq:nl:linesearchsuffdecrease}.  The quantity in \eqref{eq:nl:linesearchinitslope} is an initial slope of $\phi(\bx)$ as we move in the search direction, i.e.~a directional derivative in the direction $\bs_k$, and it is always negative.  Thus $\bs_k$ is at least a descent direction if we use \eqref{eq:nl:linesearchmeritfunction} as our merit function.  Also observe that the right side of \eqref{eq:nl:linesearchsuffdecrease} can be evaluated given only  the already-known value $\bF(\bx_k)$.

Merit function \eqref{eq:nl:linesearchmeritfunction} has another nice property.  Though $\phi(\bx)$ may have local minima other than solutions to \eqref{eq:nl:equation}, the Jacobian must also be singular at such non-solution locations.  In fact, $\bF(\bx')\ne 0$ at such locations, but
\begin{equation}
0 = \grad \phi (\bx') = J_{\bF}(\bx')^\top \bF(\bx'),
\end{equation}
so $J_{\bF}(\bx')^\top$ has a nonzero null-space.  Thus, for example, no such local minima arise in a region where there is a finite bound on the condition number of the Jacobian.

The line search options available in \PETSc are summarized in Table \ref{tab:nl:linesearchoptions}.  The Table has only minimal information on the line search details.  For more see \citep[Chapter 6]{DennisSchnabel1983} and the \PETSc source code itself.

\begin{table}
\begin{tabular}{ll}
\underline{Name} & \underline{Summary} \vspace{0.05in} \\ \vspace{0.1in}
\texttt{basic} & No line search.  Use full Newton step $\lambda_k=1$ in \eqref{eq:nl:linesearchupdate}. \\ \vspace{0.1in}
\texttt{bt} [\emph{default}] & \begin{minipage}[t]{0.8\textwidth}
Polynomial-fit back-tracking \citep[section 6.3.2]{DennisSchnabel1983}.  A quadratic or cubic [\emph{default}] polynomial is built up as a model for \mbox{$\phi(\bx_k+\lambda \bs_k)$}, though initially the model is always quadratic.  Successive values $\lambda$ are required to decrease.
\end{minipage} \\ \vspace{0.1in}
\texttt{cp} & \begin{minipage}[t]{0.83\textwidth}
Assumes $\bF$ is the gradient of an unknown objective function, i.e.~\mbox{$\bF=\grad g$}.  A critical point of $g$ is then found along the search direction by the secant method.
\end{minipage} \\ \vspace{0.1in}
\texttt{l2} & \begin{minipage}[t]{0.83\textwidth}
Secant-line minimization along the search direction, starting with testing \mbox{$\lambda=1/2$} and \mbox{$\lambda=1$}. Repeated a fixed number of times.
\end{minipage}
\end{tabular}
\caption{Line search methods (types) for \pSNES.  Use \texttt{-snes\_linesearch\_type X} to choose one.  Only types \texttt{bt} and \texttt{l2} use an objective function, supposing it is provided by a call to \texttt{SNESSetObjective()}.  Sufficient decrease \eqref{eq:nl:linesearchsuffdecrease} is only tested in type \texttt{bt}.} \label{tab:nl:linesearchoptions}
\end{table}

Note one chooses line search type \texttt{X} by option \texttt{-snes\_linesearch\_type X}, and use \texttt{-snes\_linesearch\_monitor} for runtime information.  Parameter $\alpha$ in sufficient decrease condition \eqref{eq:nl:linesearchsuffdecrease} can be set, in the rare case where this is needed, by \texttt{-snes\_linesearch\_alpha}.  The initial value for $\lambda_k$ can be set by \texttt{-snes\_linesearch\_damping}, with default of $1$.  Option \texttt{-snes\_linesearch\_max\_it} sets the maximum number of line search tries for type \texttt{bt}, but for types \texttt{l2} and \texttt{cp} it sets the fixed number of repetitions of the secant line calculation.  As usual, find all such options by
\begin{cline}
$ ./reaction -help | grep snes_linesearch
\end{cline}
%$
for example.

We have actually only been describing one type of \pSNES solver in the last section, namely the line-search \pSNES type chosen by \texttt{-snes\_type newtonls}.  There are also trust region methods \citep{NocedalWright2006} chosen by \texttt{-snes\_type newtontr}, as well as variations on nonlinear preconditioning.  See the \PETSc User's Manual \citep{petsc-user-ref}.


\section{Exercises}

\renewcommand{\labelenumi}{\arabic{chapter}.\arabic{enumi}\quad}
\renewcommand{\labelenumii}{(\alph{enumii})}
\begin{enumerate}
\item One needs to \emph{see} quadratic convergence to believe it.  Observe that for $x_k$ in both parts (b) and (c) below, the number of correct digits in $x_k$ \emph{doubles} at each iteration.
    \begin{enumerate}
    \item The sequence $x_k = 1-2^{-n}$ converges to $x^*=1$, but not quadratically.\sidenote{It converges \emph{linearly}.}  Find $k$ so that $|e_k| < 10^{-16}$.
    \item The sequence $x_k = 1-2^{(-2^n)}$ converges quadratically to $x^*=1$.  Find $k$ so that $|e_k| < 10^{-16}$.  Find the smallest $K$ so that $|e_{k+1}| \le K |e_k|^2$ for all $k$.
    \item Let $F(x) = \cos(x-1) - \exp(1-x)$ and $x_0=0.5$.  Using any quick-and-dirty numerical tool,\sidenote{Extra credit for doing it in \PETSc.} compute Newton iterates $x_k$ for $k=1,\dots,6$.  Estimate $K$ so that $|e_{k+1}| \le K |e_k|^2$ for large $k$.
    \end{enumerate}

\item Make a tiny modification to \texttt{ecjacobian.c} to set the initial vector to $\bx_0 = [10\,\, 10]^\top$.  Rerun it with option \texttt{-snes\_monitor} and note it does not converge to a solution.  Why does the \pSNES stop?  Add one runtime option so that it converges, thereby producing the data shown in Figure \ref{fig:newtonconvdelayed}.

\item \label{exer:nl:snestestdisplay}  Run and interpret the result:
\begin{cline}
$ ./ecjacobian -snes_type test -snes_test_display
\end{cline}
%$

\item  \label{exer:nl:newtonatan}
This example shows a famous case where the original Newton iteration can diverge or enter a limit cycle, according to the initial iterate $x_0$, while using linesearch gives essentially global convergence.
    \begin{enumerate}
    \item By straightforward modifications of \texttt{expcircle.c}, write a code \texttt{atan.c} which solves the (one-dimensional) nonlinear equation $F(x)=\arctan(x)=0$ using initial iterate $x_0=2.0$.  (Use ``\texttt{atan}'' for the $\arctan$ function.)  By running it \texttt{./atan -snes\_fd -snes\_linesearch\_type basic}, show that the Newton iteration without a linesearch diverges, while using the default choice (i.e.~\texttt{-snes\_linesearch\_type bt}) gives convergence.
    \item Returning to a paper calculation, require the Newton iteration \eqref{eq:nl:newtoneq}, \eqref{eq:nl:newtonupdate} to enter a sign-flipping limit cycle, i.e.~so that $x_{k+1} = - x_k$ for all $k$.  Thereby approximate the positive value $x_0$ so that the Newton iteration both makes no progress, and remains bounded, for all $k$.\sidenote{One may solve this problem with Newton's method.}  Sketch the situation.
    \item By modifying $x_0$ in \texttt{atan.c} and using linesearch type \texttt{basic}, confirm that one can make the Newton iteration ``stick'' in this actually-unstable limit cycle for quite a while.  Then confirm that linesearch types \texttt{bt,l2,cp} all get unstuck immediately.
% to solve:  - x = x - arctan(x) (1+x^2)
% octave:
%   G = @(x) 2*x - atan(x) * (1+x^2);
%   J = @(x) 1 - atan(x) * (2*x);
%   x = 1.4;
%   format long g
%   for j = 1:4, x = x - G(x) / J(x), end
% get:
%  x = 1.39174520027073
    \end{enumerate}

\item Option \texttt{-log\_summary} can show whether function evaluations or linear solves are dominating the cost of Newton's method.  By looking at \texttt{-log\_summary} output, compare where the work is done in these runs: \texttt{./reaction}, \texttt{./reaction -snes\_fd}, \texttt{./reaction -snes\_mf}.  Does the comparison change if you add \texttt{-da\_refine 5}?  Explain as much as you can.  (\emph{Hints}: Look at \texttt{SNESSolve} events for the total time spent in the Newton method, which is $\approx$100\% for these cases.  Within that event, event \texttt{SNESLineSearch} is where function and Jacobian evaluations happen, while the linear solve is the \texttt{KSPSolve} event.  Within the \texttt{SNESLineSearch} event, look at \texttt{SNESFunctionEval} and \texttt{SNESJacobianEval} events.)

\item \label{exer:nl:symmetrizeJ}  In Chapter \ref{chap:st} we gave some advice which we have not followed in the current Chapter, namely that one should implement Dirichlet conditions in a manner that preserves the symmetry of the matrix in question, which is the Jacobian here.  Modify \texttt{reaction.c} to preserve symmetry, noting that this requires modifying both the $\bF$ and $J$ evaluation routines.  Does this modification improve performance when using the CG iteration?  Does it change our conclusions about un-preconditioned JFNK, which were based on the CG iteration?

\item Most \PETSc programs add more runtime options than shown in our examples so far; ours are deliberately de-cluttered.  The PDE problem in \texttt{reaction.c} has parameter $\rho$ which can be adjusted via \texttt{PetscOptionsReal()}.  Modify \texttt{reaction.c} to ``\texttt{optreact.c}'' and add an option like this at the appropriate point:
\begin{code}
  ierr = PetscOptionsBegin(PETSC_COMM_WORLD,
                           "optr_","options for optreact",""); CHKERRQ(ierr);
  ierr = PetscOptionsReal("-rho","coefficient of nonlinear zeroth-order term",
                          NULL,user.rho,&(user.rho),NULL); CHKERRQ(ierr);
  ierr = PetscOptionsEnd(); CHKERRQ(ierr);
\end{code}
By sampling, for what range of positive \texttt{X}$=\rho$ does the run
\begin{cline}
$ ./optreact -snes_monitor -da_refine 4 -optr_rho X
\end{cline}
%$
converge?  Explain why this case, with this way of choosing the boundary conditions and initial iterate, and in contrast to the next exercise, is insensitive to the value of the major parameter $\rho$.
% because this option is used in bratu.c below, for simplicity of maintenance, the result optreact.c is NOT saved in c/ch4/solns/

\item \label{exer:nl:bratu} Modify \texttt{reaction.c} to create ``\texttt{bratu.c}'' which solves
\begin{equation}
    - u'' - \lambda e^u = 0 \label{eq:nl:bratuoned}
\end{equation}
% R(u) = lambda e^u increasing
with Dirichlet boundary conditions $u(0)=u(1)=0$.  Make $\lambda$ an option-adjustable real parameter---see the previous exercise---and use the simplest function which satisfies the boundary conditions as an initial iterate.  Confirm by using a fine grid, and option \texttt{-snes\_converged\_reason} plus other options as needed, that around $\lambda=3.513$ there is a transition to non-convergence of the Newton iteration.  This \emph{critical value} is intrinsic to the nonlinear PDE problem and not a result of a Newton failure \citep{Doedeletal1991}.

\item To the author's knowledge an exact solution of \eqref{eq:nl:bratuoned} cannot be written in elementary functions for $\lambda>0$.  Because we started from \texttt{reaction.c}, writing the code in the last exercise is doable.  However, one should not walk over high wires too often without a net, and having an exact solution to check correctness is such a ``net.''  Fortunately, we can ``manufacture'' \citep{Wesseling2001} an exact solution to a generalized form of \eqref{eq:nl:bratuoned}, namely to equation \eqref{eq:nl:diffusionreaction} in the case where $D=1$ and $R(u)=-\lambda e^u$.  For example we can choose $u(x) = \sin(\pi x)$ as the exact solution of \eqref{eq:nl:diffusionreaction}, and then determine $f(x)$ from the equation itself.  Here $u(0)=u(1)=0$ are the boundary conditions, because they are satisfied by the exact solution.

Thus we get to the exercise itself: Add options and a manufactured exact solution to the code \texttt{bratu.c} in the previous exercise.  The new code will both solve the previous exercise and be verified using the new exact solution.  One might add a user-option-settable boolean member \texttt{manufactured} to \texttt{AppCtx}: when it is false then $f(x)=0$ and the code solves the previous exercise, but if it is true then we report a numerical error based on the manufactured solution.  Observe that having $f(x)\ne 0$ does not change the Jacobian.  Confirm that in the manufactured-solution case we have both quadratic convergence of the Newton iteration \emph{and} $O(h^2)$ convergence of the numerical error under grid refinement.

\item As long as the number of MPI processes does not exceed the number of grid points, \texttt{reaction.c} will solve correctly, though perhaps not efficiently, in parallel.  Confirm this by running
\begin{cline}
$ mpiexec -n N ./reaction -snes_converged_reason -da_refine M
\end{cline}
%$
with a few values of \texttt{N} and \texttt{M}.

Code \texttt{ecjacobian.c} does not run correctly in parallel even on two MPI processes!  Modify so that it does.

\end{enumerate}

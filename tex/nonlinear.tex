\renewcommand{\CODELOC}{c/ch4/}

\chapter{Nonlinear equations}
\label{chap:nonlinear}

How should nonlinear equations first appear in a code which uses \PETSc?  The answer is that they merely change the functional form of the residual.  For a linear system the residual is this function of the unknowns,
\begin{equation}
\br = \bF(\bu) = \bb - A \bu. \label{eq:nl:linres}
\end{equation}
Now we consider cases in which $\bF$ is a higher-order polynomial, a transcendental function, or some more general function.  In fact, let us suppose for now that $\bF : \RR^N \to \RR^N$ is differentiable.

The input $\bx$ and output $\bF(\bx)$ are column vectors,\sidenote{The name change of the unknown $\bu\to\bx$, relative to \eqref{eq:nl:linres}, comes from thinking more geometrically about changes in the location of $\bx$.} so in that sense $\bF$ acts like multiplying by a square matrix $\bx\mapsto A\bx$.  Just as we would reduce the residual to zero in an iterative linear algebra method, for nonlinear $\bF$ we want to solve
\begin{equation}
   \bF(\bx) = 0   \label{eq:nl:equation}
\end{equation}
by iteration.

Newton's method linearizes \eqref{eq:nl:equation} around the most recent iterate and then ``moves'' $\bx$ to the location which solves the linear problem.  (This is, we hope, closer to the solution.)  Each iteration therefore solves a linear system; we already have \PETSc technology for that!  However, the cost of performing the linearization must be taken into account, choosing a ``smart'' distance to move will require additional choices, and all existing issues regarding the linear solver---especially preconditioning as discussed in Chapters \ref{chap:linearsystem} and \ref{chap:structured}---remain active.

Large systems of nonlinear equations certainly arise in applications as there are many PDE-type physical models with nonlinearities.  However, this section is mostly about finite-dimensional systems of nonlinear equations \eqref{eq:nl:equation} as problems of their own.  Later in this section we give an example of a discretized one-dimensional nonlinear PDE, and in the next Chapter we continue with another nonlinear PDE problem in two dimensions.


\section{Newton's method}

Suppose $\bx_k\in\RR^N$ is an approximation, whether good or bad, to the solution of \eqref{eq:nl:equation}.  If we have already determined a \emph{step} $\bs\in\RR^N$ away from the current iterate $\bx_k$ then $\bx_{k+1} = \bx_k + \bs$ is the next iterate.  By definition, because $\bF$ is differentiable,
\begin{equation}
    \bF(\bx_{k+1}) = \bF(\bx_k) + J_\bF(\bx_k) \bs + o(\|\bs\|)  \label{eq:nl:expandF}
\end{equation}
for some square matrix
\begin{equation}
J_\bF(\bx_k) = \begin{bmatrix}
    \frac{\partial F_0}{\partial x_0} & \dots & \frac{\partial F_0}{\partial x_{N-1}} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial F_{N-1}}{\partial x_0} & \dots & \frac{\partial F_{N-1}}{\partial x_{N-1}}  \end{bmatrix},  \label{eq:nl:jacdefn}
\end{equation}
and some quantity $o(\|\bs\|)$ that goes to zero as the \emph{step length} $\|\bs\|$ goes to zero.  The matrix $J = J_\bF(\bx_k)$ is called the \emph{Jacobian} of $\bF$ at $\bx_k$; we also call the function $\bx \mapsto J_\bF(\bx)$ by the same name.

An iteration of Newton's method approximately solves \eqref{eq:nl:equation} by truncating \eqref{eq:nl:expandF} and seeking $\bs$ so that the updated value $\bF(\bx_{k+1})$ is zero.  That is, each Newton step computes $\bs$ by the linear equation
\begin{equation}
    0 = \bF(\bx_k) + J_\bF(\bx_k) \bs.
\end{equation}
Writing this equation as ``$A\bu=\bb$,'' at each iteration $k$ we solve a linear system and then do a vector addition:
\begin{align}
    J_\bF(\bx_k) \bs &= - \bF(\bx_k)  \label{eq:nl:newtoneq}  \\
    \bx_{k+1} &= \bx_k + \bs  \label{eq:nl:newtonupdate}
\end{align}
This is Newton's method.  It is fairly simple in theory.

Actual practice for Newton's method is not that much more complicated with \PETSc in hand.  It has a mature implementation of this core technology for solving nonlinear PDEs.  Despite the reputation of Newton iteration as fragile or scary, it works out just fine on many nonlinear systems, once one ``moves'' a possibly shorter distance to the new iterate compared to that than stated in \eqref{eq:nl:newtonupdate} \citep{Kelley2003}; see page \pageref{sec:linesearch} below.  On big nonlinear systems we will also need to pay more attention to the details of the linear solve at each step.\sidenote{A nonlinear PDE problem generally requires all of the tools for linear systems already considered in Chapters \ref{chap:linearsystem} and \ref{chap:structured}, and more.}

Finite-dimensional nonlinear systems can be visualized as the problem of finding the intersections of curves, surfaces, or hypersurfaces, depending on dimension.  A small example gives us a start on the details.

\medskip\noindent\hrulefill
\begin{example}  Given $b > 1$ this pair of nonlinear equations
    $$y = \frac{1}{b} e^{bx}, \qquad x^2+y^2 = 1,$$
form intersecting curves in the plane.  The curves intersect twice, as shown in Figure \ref{fig:expcirclebasic} for the $b=2$ case.  These equations are put in standard form \eqref{eq:nl:equation} by writing
\begin{equation}
\label{eq:nl:expcircleF}
\bF(\bx) = \begin{bmatrix}
           \frac{1}{b} e^{b x_0} - x_1 \\
           x_0^2 + x_1^2 - 1
           \end{bmatrix}
\end{equation}
for $\bx\in \RR^2$ with components $\bx = [x_0 \, x_1]^\top$.  Thus
\begin{equation}
\label{eq:nl:ecjacobian}
J_\bF(\bx) = \begin{bmatrix}
    e^{b x_0} & & -1 \\
    2 x_0   & & 2 x_1 \end{bmatrix}
\end{equation}
Let $b=2$.  If we start the Newton iteration with $\bx_0 = [1 \,\, 1]^\top$ then the sequence of iterates from \eqref{eq:nl:newtoneq} and \eqref{eq:nl:newtonupdate} is
    $$\twovect{\bx}{0}{1}{1}, \quad \twovect{\bx}{1}{0.619203}{0.880797}, \quad \twovect{\bx}{2}{0.394157}{0.948623}, \quad \dots$$
as also shown in Figure \ref{fig:expcirclebasic}.

\begin{marginfigure}
\includegraphics[width=1.2\textwidth]{expcirclebasic}
\caption{Newton iterates approach a solution of $\bF(\bx)=0$ for $\bF$ in \eqref{eq:nl:expcircleF} and $b=2$.}
\label{fig:expcirclebasic}
\end{marginfigure}

\noindent\hrulefill
%  FROM $ for N in 0 1 2; do ./expcircle -snes_fd -snes_max_it $N; done
\end{example}


\section{Using \pSNES; call-backs} \label{sec:usingsnes}

We will do the above example in \PETSc by using a nonlinear solver object of type \pSNES.\sidenote{This acronym stands for ``scalable nonlinear equation solver.''}  It has the usual \texttt{Create/SetFromOptions/Destroy} sequence, but it also has a method by which we provide the function $\bF$.  This is a ``call-back'' in the sense that we supply a function, here named \texttt{FormFunction()}, which the \pSNES can then call with argument $\bx$ when it needs $\bF(\bx)$ during the Newton iteration.  Later we will also provide the \pSNES with a function which computes the Jacobian.  However, because this derivative can instead be approximated by finite differences applied to $\bF$ itself, and thus because $J_\bF$ can be approximated by repeatedly calling \texttt{FormFunction()}, our first code avoids such a Jacobian ``call-back.''

Figure \ref{code:expcircle} shows our entire first \pSNES-using code \texttt{expcircle.c}, which solves problem \eqref{eq:nl:equation} for the above Example.

\vfill
\cinput{expcircle.c}{\CODELOC}{A first \pSNES-using code.  Solves nonlinear system \eqref{eq:nl:equation} with $\bF$ given in \eqref{eq:nl:expcircleF}.}{//START}{//END}{code:expcircle}

The \texttt{main()} portion is mostly not surprising.  In brief:  We start by allocating \pVec \texttt{x} of fixed dimension 2.  This will hold both the initial iterate $\bx_0$ and, once the Newton iteration is ended, the converged estimate of the solution.  Because both components of $\bx_0$ are $1$ in the above Example, it is initialized using \texttt{VecSet()}.  Next a duplicate \pVec \texttt{r} is created; the \pSNES needs it as space for the (nonlinear) residual.  Then the \pSNES is created and configured, and, in particular, formula \eqref{eq:nl:expcircleF} is supplied by a call to \texttt{SNESSetFunction()}.  This call-back sets the third argument of \texttt{SNESSetFunction()} to the name of the C function \texttt{FormFunction()}.  Then we call \texttt{SNESSetFromOptions()} because it gives us run-time control both on how the Jacobian is calculated\sidenote{Options \texttt{-snes\_fd} and \texttt{-snes\_mf} are allowed; see Table \ref{tab:snesjacobianoptions} on page \pageref{tab:snesjacobianoptions}.} and on how the length of the step $\bs$ is actually determined.\sidenote{Through \texttt{-snes\_linesearch\_type} and related options; see page \pageref{sec:linesearch}.}  Then system \eqref{eq:nl:equation} is solved by a call to \texttt{SNESSolve()}, which also supplies \pVec \texttt{x}.  Finally the new state of \texttt{x}, presumably the converged solution, is printed at the command line using \texttt{VecView()} with a \texttt{STDOUT} viewer.

In order to match the calling sequence of \texttt{SNESSetFunction()}, \texttt{FormFunction()} must have a particular ``signature'' as a C function:
\begin{code}
PetscErrorCode (*f)(SNES,Vec,Vec,void*)
\end{code}
In particular, \texttt{FormFunction()} takes the input $\bx$ as the first \pVec and it generates output $\bF(\bx)$ as the second \pVec.  (For the secon \pVec argument note that a \pVec is real a \emph{pointer}, so passing a \pVec by value does allow it to be modified.)

In other examples there may be additional information, such as parameters, passed to \texttt{FormFunction()} in a ``user context'' which is the fourth pointer argument ``\texttt{void*}.''  We will show how to pass parameters in later codes in this Chapter.

Looking inside \texttt{FormFunction()}, in Figure \ref{code:expcircle}, we see a new method for extracting values from, and setting values in, a \pVec.  Previously we have used \texttt{VecSetValues()} to set values at given indices (Chapter \ref{chap:linearsystem}), or we have used a \pDMDA structured grid method of access (Chapter \ref{chap:structured}), but here we access the C array underlying the \pVec.  Because we only need to read entries of input \pVec \texttt{x} we use the read-only array access method \texttt{VecGetArrayRead()} which supplies us with a read-only pointer \texttt{const PetscReal *ax}.\sidenote{Because of the \texttt{const} qualifier, the C compiler can stop us from altering \texttt{ax[0]}, for example.  Try it!}  Since we are setting entries of \pVec \texttt{F}, we do nearly the same but using \texttt{PetscReal *aF}, an unrestricted pointer, and method \texttt{VecGetArray()}.

To avoid conflicts with other methods reading or writing the same memory, \texttt{VecGetArray()} and \texttt{VecGetArrayRead()} are matched by \texttt{VecRestoreArray()} and \texttt{VecRestoreArrayRead()}.  These methods ``free-up''  the \pVecs, so that they can be read or written by other methods.\sidenote{Note that these \texttt{Restore} methods do not deallocate anything.  \texttt{VecDestroy()} does that.}  In general:
\begin{quote}
\emph{Each \emph{\texttt{VecGetArray()}}-type call should be matched by the corresponding \emph{\texttt{VecRestoreArray()}} call once you are done with that \emph{\pVec}}.
\end{quote}

The actual content of \texttt{FormFunction()} is to implement formulas \eqref{eq:nl:expcircleF}.  \texttt{PetscExpReal()}, which computes the exponential function $e^x$, is just an alias for \texttt{exp()} from the standard library (i.e.~\texttt{math.h}).  Use of such \PETSc library functions means that the \PETSc configuration can link to consistent libraries which we access just by including \texttt{petsc.h}.

It is time to run this example.  We use option \texttt{-snes\_monitor} to count the iterations and show the residual norm $\|\bF(\bx_k)\|_2$:
\begin{cline}
$ cd c/ch4/
$ make expcircle
...
$ ./expcircle -snes_fd -snes_monitor
  0 SNES Function norm 2.874105323289e+00 
  1 SNES Function norm 8.591393113962e-01 
  2 SNES Function norm 1.609958353862e-01 
  3 SNES Function norm 1.106891696425e-02 
  4 SNES Function norm 6.618141730691e-05 
  5 SNES Function norm 2.420782802130e-09 
Vec Object: 1 MPI processes
  type: seq
0.319632
0.947542
\end{cline}
%$
Thus after 5 iterations the Newton method has reduced the residual norm by a factor of $10^9$ and stopped with solution $x_0=0.319632$ and $x_1=0.947542$.  Compare Figure \ref{fig:expcirclebasic}.

The above run also uses option \texttt{-snes\_fd}, the purpose of which the reader may already see.  Clearly the Newton iteration \eqref{eq:nl:newtoneq} requires the Jacobian, but we have only supplied the \pSNES with an implementation of function $\bF(\bx)$, not with $J_{\bF}(\bx)$.  The entries of the latter matrix are derivatives, however, and we can approximate these by finite differences.  Specifically, an entry in matrix $J=J_{\bF}(\bx)$ is approximated
\begin{equation}
J_{ij} = \frac{\partial F_i}{\partial x_j} \approx \frac{F_i(\bx+\delta \be_j) - F_i(\bx)}{\delta}  \label{eq:nl:examplefdjac}
\end{equation}
if $\delta>0$ and $\be_j\in \RR^N$ denotes a vector with entry one in the $j$th position (and zero otherwise).  It turns out that choosing $\delta = \sqrt{\eps}$, where $\eps$ is machine precision, gives a reasonably accurate approximation if the inputs to $\bF$ are all of order approximately one \citep{Kelley2003}.


\section{Inside \pSNES} \label{sec:insidesnes}

In outline, \pSNES does these steps to implement the Newton method:
\begin{quote}
	\renewcommand{\labelenumi}{(\emph{\roman{enumi}})}
	\renewcommand{\labelenumii}{\emph{\alph{enumii}}.}
	\begin{enumerate}
	\item from the current iterate $\bx_k$, $\bF(\bx_k)$ is evaluated using a call-back function set in \texttt{SNESSetFunction()}, like \texttt{FormFunction()} above,
	\item the Jacobian $J_{\bF}(\bx_k)$ is
	    \begin{enumerate}
	    \item computed and assembled by a call-back to user-supplied code, e.g.~\texttt{FormJacobian()} in in Figure \ref{code:ecjacobianI} below, \emph{or}
	    \item computed and assembled by using formula \eqref{eq:nl:examplefdjac} $N^2$ times, which requires calling \texttt{FormFunction()} $N$ times to evaluate $\bF(\bx_k+\delta \be_j)$ for $j=0,\dots,N-1$, \emph{or}
	    \item not formed, but, in a Krylov iterative method for solving system \eqref{eq:nl:newtoneq}, the action of $J_{\bF}(\bx_k)$ on vectors is computed by finite-differences, possibly using an additional preconditioning matrix,
        \end{enumerate}
	\item linear system \eqref{eq:nl:newtoneq} is solved for $\bs$, and vector update \eqref{eq:nl:newtonupdate} is done, with possible reduction in the length of $\bs$, \emph{and}
	\item a convergence test is made, and we repeat at (\emph{i}) if not converged.
	\end{enumerate}
\end{quote}

The Jacobian-free method mentioned in (\emph{ii})c will be addressed starting on page \pageref{sec:JFNK} below.  Issue (\emph{iii}) about possible step-length reduction will be addressed on page \pageref{sec:linesearch}.

Option (\emph{ii})a is taken if we supply a call-back function for the Jacobian through \texttt{SNESSetJacobian()} or similar; we give an example below in the next section.  For now, however, if you run without option \texttt{-snes\_fd} or \texttt{-snes\_mf} then you get an error message about an un-assembled matrix:
\begin{cline}
$ ./expcircle
[0]PETSC ERROR: --------------------- Error Message -------------------------
[0]PETSC ERROR: Object is in wrong state
[0]PETSC ERROR: Matrix must be assembled by calls to MatAssemblyBegin/End();
...
\end{cline}
%$
This message is somewhat opaque unless you are conscious of the need to form the Jacobian matrix at each Newton iteration.  That is, \emph{something} must happen at step (\emph{ii}), and actions (\emph{ii})b and (\emph{ii})c usually require runtime options.

Option (\emph{ii})b is what happens if we run with option \texttt{-snes\_fd}.  One possible disadvantage of this option is that formula \eqref{eq:nl:examplefdjac} is only an approximation, but in many cases using an approximate Jacobian in the Newton step is not problematic \citep{Kelley2003}.

However, there is a significant performance problem with finite-difference evaluation of the Jacobian when it comes to PDE-related applications of Newton's method.  In steps (\emph{i}) and (\emph{ii})b together we evaluate $\bF$ through $N+1$ calls to \texttt{FormFunction()} per Newton iteration.  While this is no particular problem in dimension $N=2$, as here, it is a worrying amount of work if $N$ is large, as it would be for a system of nonlinear equations coming from discretizing a PDE.  For many PDE discretization schemes, however, a graph-theoretic ``coloring'' algorithm makes \texttt{-snes\_fd} an efficient option by reducing the number of evaluations of \texttt{FormFunction()} to a small constant per Newton step.  See the diffusion-reaction PDE example at the end of this Chapter, as well as examples in later Chapters.

There are systems where $\bF$ itself is an expensive function to evaluate.  In any case, evaluating $\bF$ many times can be the dominant work in the Newton iteration.  The work done in solving the linear system \eqref{eq:nl:newtoneq} is the other main concern.\sidenote{A good \PETSc habit, to start right now, is using \texttt{-log\_summary} to know which kind of work is dominant!  \label{sidenote:logsummary}}

The benefit of using options \texttt{-snes\_fd} or \texttt{-snes\_mf} is that we do not need to write any error-prone code based on taking derivatives of our function $\bF$.  Avoiding writing and debugging Jacobian implementations can speed implementation by reducing \emph{your} time.


\section{Residual norm in the Newton iteration}

Option \texttt{-snes\_rtol} specifies by what factor the \pSNES should try to reduce the residual norm.  The default accuracy corresponds to \texttt{-snes\_rtol 1.0e-8}; this default value can be printed by running
\begin{cline}
$ ./expcircle -snes_fd -help | grep snes_rtol
\end{cline}
%$
The example
\begin{cline}
$ ./expcircle -snes_fd -snes_monitor -snes_rtol 1.0e-14
\end{cline}
%$
thus asks for much more accuracy than the earlier run which used the default.  It may be a surprise that this tight-tolerance run, asking for a further $10^6$ reduction in residual norm, requires only one more iteration, namely six iterations this time, but this is typical of the Newton iteration in the best cases.  Instead of showing the Newton iterations again as text output, Figure \ref{fig:newtonconvbasic} shows these residual norm values in a graph with log-scaling on the $y$-axis.

\begin{figure}
\includegraphics[width=0.8\textwidth]{newtonconvbasic}
\caption{The characteristic look of the quadratic convergence of the Newton iteration: the residual norm drops abruptly.}
\label{fig:newtonconvbasic}
\end{figure}

The residual drops very abruptly in the Figure, reflecting the hoped-for best-case behavior of Newton iteration.  The residual norm, and the error in the solution also, decreases substantially at each iteration in the sense that the error is proportional to the \emph{square} of the error at the last iteration.  The next theorem expresses such best-case behavior \citep[Theorems 1.1 and inequalities (1.13)]{Kelley2003}:

\begin{theorem}
Suppose that $\bF:\RR^N\to\RR^N$ is differentiable, $\bx^*$ is a solution of \eqref{eq:nl:equation}, $J_{\bF}$ is Lipschitz near $\bx^*$, and $J_{\bF}(\bx^*)$ is a nonsingular matrix.  Let $\be_k=\bx_k-\bx^*$, let $\|\cdot\|$ denote a vector norm and its induced matrix norm, and let $\kappa(A)=\|A^{-1}\| \|A\|$ denote the condition number of an invertible matrix $A$.  If $\bx_0$ is sufficiently close to $\bx^*$ then, in exact arithmetic,
\renewcommand{\labelenumi}{(\roman{enumi})}
\begin{enumerate}
\item there is $K\ge 0$ such that for all $k$ sufficiently large,
\begin{equation}
	\|\be_{k+1}\| \le K \|\be_k\|^2, \label{eq:nl:quadraticconvergence}
\end{equation}
\item and if $\kappa = \kappa\left(J_{\bF}(\bx^*)\right)$, so that $\kappa\ge 1$, then
\begin{equation}
	\frac{\|\be_k\|}{4 \kappa \|\be_0\|} \le \frac{\|\bF(\bx_k)\|}{\|\bF(\bx_0)\|} \le \frac{4 \kappa \|\be_k\|}{\|\be_0\|}. \label{eq:nl:errorresidualnormequiv}
\end{equation}
\end{enumerate}
\end{theorem}

\medskip
By definition, a sequence $\{\bx_k\}$ in $\RR^N$ \emph{converges quadratically to} $\bx^*$ if the sequence $\be_k=\bx_k-\bx^*$ satisfies \eqref{eq:nl:quadraticconvergence} for some $K\ge 0$.  Thus the first idea in the Theorem is that, under strong assumptions about the regularity and nonsingularity of the Jacobian, the error decays very rapidly in the sense that the iterates converge quadratically to a solution of \eqref{eq:nl:equation}.  This remarkable fact explains why Newton iteration is such a powerful tool.  Heuristically, once the error norm $\|\be_k\|$ is a small number (e.g.~$\|\be_k\| \ll 1$), the number of correct digits in $\bx_k$ \emph{doubles} with each additional iteration.

We seem to see quadratic converge in Figure \ref{fig:newtonconvbasic}, but this Figure shows the residual norm $\|\bF(\bx_k)\|_2$ and not the error norm $\|\be_k\|_2$.  The second part of the Theorem says that the error decrease at the $k$th iteration (i.e.~$\|\be_k\|/\|\be_0\|$) is within a factor, determined by the conditioning of the Jacobian at the solution, of the residual decrease at the $k$th iteration (i.e.~$\|\bF(\bx_k)\|/\|\bF(\bx_0)\|$).  This idea is significant because the latter quantity, the residual norm reduction ratio, is \emph{computable}.

The Theorem confirms that residual norm decay like that shown in Figure \ref{fig:newtonconvbasic} corresponds to quadratic convergence of $\bx_k$ to a solution $\bx^*$.  If we want to reduce the (generally-unknowable) numerical error $\be_k=\bx_k-\bx^*$ by a given amount then it can suffice to reduce the residual norm by a comparable amount.  The factor $4 \kappa$ by which the two relative norms differ in \eqref{eq:nl:errorresidualnormequiv} is large only if the conditioning of the Jacobian at the solution is poor.  Just as in the linear case, however, a large condition number $\kappa=\kappa\left(J_{\bF}(\bx^*)\right)$ would also mean lost precision in solving \eqref{eq:nl:equation} by \emph{any} numerical means.\sidenote{Recall the numerical facts-of-life in Chapter \ref{chap:linearsystem}.}

The amount of residual norm reduction is exactly what the option \texttt{-snes\_rtol} controls, that is, the iteration continues until
    $$\frac{\|\bF(\bx_k)\|_2}{\|\bF(\bx_0)\|_2} \le \text{\texttt{snes\_rtol}}.$$
Actually, to give the more complete story, there are \emph{three} \pSNES tolerances, listed in Table \ref{tab:snestolerances}.  The iteration stops as soon as one of these conditions is satisfied.  Note that $\bs_k$ denotes the solution to linear system \eqref{eq:nl:newtoneq}, the ``step'' at iteration $k$.  The defaults for the three tolerances are \texttt{X}$=10^{-8},10^{-50},10^{-8}$, respectively.

\medskip
\begin{table}
\begin{tabular}{lll}
\underline{Option}\hspace{0.2in} & \underline{Name}\hspace{0.2in} & \underline{Condition}\hspace{0.2in} \\
\texttt{-snes\_rtol X} & relative (\texttt{FNORM\_RELATIVE}) & $\|\bF(\bx_k)\|_2 \le \text{\texttt{X}}\, \|\bF(\bx_0)\|_2$ \\
\texttt{-snes\_atol X} & absolute (\texttt{FNORM\_ABS}) & $\|\bF(\bx_k)\|_2 \le \text{\texttt{X}}$ \\
\texttt{-snes\_stol X} & step-length (\texttt{SNORM\_RELATIVE}) & $\|\bs_k\|_2 \le \text{\texttt{X}}\, \|\bx_k\|_2$
\end{tabular}
\caption{The three ways \pSNES can succeed, thereby stopping the Newton iteration.} \label{tab:snestolerances}
\end{table}

\medskip
Option \texttt{-snes\_converged\_reason} reports which termination condition was active, using the parenthetical name given in Table \ref{tab:snestolerances}.  For example,
\begin{cline}
$ ./expcircle -snes_fd -snes_converged_reason
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 5
...
\end{cline}
%$
\vspace{-4mm}
One can force the \pSNES to use a subset of the stopping criteria by setting the tolerance to zero in the unwanted conditions(s).


\section{Convergence difficulties} \label{sec:divergence}

So far we have portrayed the Newton iteration in optimistic terms, but it is not magic and things can go wrong.

\begin{figure}
\includegraphics[width=0.8\textwidth]{newtonconvdelayed}
\caption{Even in for well-behaved systems $\bF(\bx)=0$, if the initial iterate $\bx_0$ is far from the solution then quadratic convergence can be postponed for many iterations.}
\label{fig:newtonconvdelayed}
\end{figure}

First note a key hypothesis in the above Theorem, namely that $\bx_0$ is sufficiently close to $\bx^*$.  Even on well-behaved nonlinear equations, if $\bx_0$ is far from the solution then the iteration may take many steps before $\|\be_k\|$ becomes small enough so that quadratic convergence \eqref{eq:nl:quadraticconvergence} ``kicks in''.  For example, Figure \ref{fig:newtonconvdelayed} shows what happens if we use initial iterate $\bx_0=[10\,\, 10]^\top$ in the above Example.  A long period of slower convergence\sidenote{In this particular case, evidently it is \emph{linear} convergence in which the residual norm is being reduced by a constant factor.} is needed before the iterate enters the region where $\bx_0$ is sufficiently close so that the conclusions of the Theorem apply.  This region is sometimes known as the ``ball of quadratic convergence.''

Actual decrease in residual norm, as displayed in Figures \ref{fig:newtonconvbasic} and \ref{fig:newtonconvdelayed}, is also not guaranteed in general.  There is nothing intrinsic about the solution $\bs$ to \eqref{eq:nl:newtoneq} that implies $\|\bF(\bx_{k+1})\| \le \|\bF(\bx_k)\|$.  Again, however, ``line-search'' methods, described starting on page \pageref{sec:linesearch}, can enforce residual norm decrease.  Such methods generate an error if searching along the direction $\bs$ does not find a step-length which generates a (sufficient) residual norm decrease.

On some equations the Newton iteration \eqref{eq:nl:newtoneq}, \eqref{eq:nl:newtonupdate} as it stands actually diverges from some initial states.  For an example, see Exercise \ref{chap:nonlinear}.\ref{exer:newtonatan}.  In those cases line-search methods will generally reduce the step length $\|\bs_k\|$ and this may ``globalize'' the convergence \citep{Kelley2003} in the sense of allowing convergence from a larger set of initial states.

Finally, many practical problems do not have the smoothness needed to apply the above Theorem.  In such cases the problem may need regularization, continuation, or other procedures to make it manageable.


\section{Exact Jacobians and passing parameters}

We have yet to exploit two critical possibilities when using a \pSNES, namely the ability to provide a method which computes the Jacobian function $J_{\bF}(\bx)$, and the ability to pass parameters through the call-back mechanism so that they can be used inside the residual- and Jacobian-evaluation functions.  The next code \texttt{ecjacobian.c}, in Figures \ref{code:ecjacobianI} and \ref{code:ecjacobianII}, uses these abilities; this example is a ``model use'' of \pSNES.\sidenote{For cases without a structured grid of the \pDMDA type (Chapter \ref{chap:structured}), anyway.  Compare \texttt{reaction.c} below.}

The first new idea (Figure \ref{code:ecjacobianI}) is the declaration of a C \texttt{struct} called \texttt{AppCtx} (``application context'').  It has just one element, the real parameter $b$ which appears in formulas \eqref{eq:nl:expcircleF} and \eqref{eq:nl:ecjacobian}.  A \texttt{struct} is not really necessary here, but in future examples there will be more than one parameter to pass.

Next, the method \texttt{FormFunction()} in Figure \ref{code:ecjacobianI} is almost the same as the one in \texttt{expcircle.c} (Figure \ref{code:expcircle}), but the value of $b$ comes from the \texttt{struct} instead of being hard-wired as before.  In detail, the argument \texttt{void *ctx} is ``cast'' in the sense of the C language \citep{KernighanRitchie1988} to a pointer of type \texttt{AppCtx*}, and then the parameter is extracted, via the pointer, by ``\texttt{user->b}'' which is shorthand for ``\texttt{(*user).b}''.  This awkward-seeming method of passing parameters allows the signature of \texttt{FormFunction()} to be precisely as before.

\cinputpart{ecjacobian.c}{\CODELOC}{Solves the same nonlinear system as \texttt{expcircle.c}.  This version has an exact Jacobian.  It also passes a parameter into the call-back functions.}{I}{//START}{//END}{code:ecjacobianI}

The method \texttt{FormJacobian()} in Figure \ref{code:ecjacobianI} is new.  It has similar structure and semantics to \texttt{FormFunction()}, but a new (required) signature so that it can be used in call-back, namely
\begin{code}
PetscErrorCode (*J)(SNES,Vec,Mat,Mat,void*)
\end{code}
Input \pVec \texttt{x} and parameter-passing pointer \texttt{void *ctx} have the same meaning as in \texttt{FormFunction()}.  Also just as before, when reading \texttt{x} we can use \texttt{VecGetArrayRead()} and \texttt{VecRestoreArrayRead()}.

\cinputpart{ecjacobian.c}{\CODELOC}{This \texttt{main()} method adds code, relative to  \texttt{expcircle.c}, to allocate a \pMat to hold the Jacobian.}{II}{//STARTMAIN}{//ENDMAIN}{code:ecjacobianII}

The difference is that we must set a \pMat as output, based on formula \eqref{eq:nl:ecjacobian} in this case.  The roles of \texttt{MatSetValues()}, real array \texttt{v[4]} for the entries themselves, and integer arrays \texttt{row[2]} and \texttt{col[2]} as global indices, are all the same as in Chapter \ref{chap:linearsystem}.  An interesting detail appears here, however, and it requires some explanation.  There are actually \emph{two} output \pMats for \texttt{FormJacobian()} to set.  The first, called \texttt{J} here, corresponds to the Jacobian matrix itself, which, in this simple case, we want to supply.  The second, called \texttt{P} here, is the ``material'' we supply to build a preconditioner.  It might, at least in other cases, be a very poor approximation of the Jacobian.  A method like \texttt{FormJacobian()} sets one of these \pMats according to whether we intend to supply accurate derivatives of $\bF$ or not.  In fact, we assemble the \pMat \texttt{P} and, if the \pMat \texttt{J} is also present and is a different pointer, then we also assemble it.  These actions allow ``Jacobian-free'' Newton-Krylov methods to work (below).
%FIXME: I haven't really explained anything!

Looking at \texttt{main()} in Figure \ref{code:ecjacobianII}, note that we create and configure a $2\times 2$ \pMat \texttt{J} to hold the Jacobian.  Our use of \texttt{MatCreate(), MatSetSizes(), MatSetFromOptions(),} and \texttt{MatSetUp()} on \texttt{J} mimics what we did for finite-dimensional systems in Chapter \ref{chap:linearsystem}.  However, this time when we set-up the \pSNES we pass \texttt{J} for two arguments,
\begin{code}
SNESSetJacobian(snes,J,J,FormJacobian,&user);
\end{code}
This means we provide the allocated space in \texttt{J} as both the Jacobian and preconditioner matrices, that is, for both the second and third \pMat arguments.

Now that we have assembled an exact Jacobian, we see that it and its finite-difference approximation produce nearly the same result on this small and well-behaved example:
\begin{cline}
$ make ecjacobian
...
$ ./ecjacobian -snes_monitor
  0 SNES Function norm 2.874105323289e+00 
  1 SNES Function norm 8.591392822370e-01 
  2 SNES Function norm 1.609958166309e-01 
  3 SNES Function norm 1.106891138388e-02 
  4 SNES Function norm 6.618107497046e-05 
  5 SNES Function norm 2.419259135755e-09 
...
$ ./ecjacobian -snes_monitor -snes_fd
  0 SNES Function norm 2.874105323289e+00 
  1 SNES Function norm 8.591393113962e-01 
  2 SNES Function norm 1.609958353862e-01 
  3 SNES Function norm 1.106891696425e-02 
  4 SNES Function norm 6.618141730691e-05 
  5 SNES Function norm 2.420782802130e-09 
...
\end{cline}
%$


\section{Example: a nonlinear diffusion-reaction equation}

To get started on nonlinear PDE problems we look at a simple two-point boundary value problem.  Suppose $u(x)$ is the density of some substance and that $D>0$ is a constant.  An equation of the form
\begin{equation}
- D u'' - R(u) = f(x)  \label{eq:nl:diffusionreaction}
\end{equation}
models a time-independent combination of \emph{diffusion} and \emph{reaction} processes by the terms on the left, respectively.  The right-hand side $f(x)$ models an additional location-dependent \emph{source}.

The condition $R(u)+f(x)>0$ models the production of $u$, while the other sign is for the destruction of $u$.  Our sign convention is clearest in the time-dependent model
\begin{equation}
u_t = D u_{xx} + R(u) + f(x),  \label{eq:nl:drtimedependent}
\end{equation}
a typical generalization of the classical one-dimensional time-evolving heat equation.  In such a case, where $u$ represents temperature and $R(u)$ represents a heat-producing or -absorbing temperature-dependent reaction, according to its sign, \eqref{eq:nl:diffusionreaction} is the model of the equilibrium (steady state) temperature distribution.

We should be concerned about the solvability of \eqref{eq:nl:diffusionreaction} in the case where $R$ is an \emph{increasing} function of $u$.  If $R$ is positive and increasing then the ability of the $D u''$ term to ``damp-out'' maxima may be exceeded by the increasing production from large values of $u$, so that these terms cannot be in balance (as is stated in \eqref{eq:nl:diffusionreaction}).  If $R$ is negative and increasing then the analogous concern applies to minima of $u$.  These concerns are demonstrated by the example $R(u) = \lambda e^u$ with $\lambda>0$ in Exercise 4.\ref{exer:nl:bratu}, in which the problem is demonstrably not numerically-solvable for sufficiently-large $\lambda$.  

On the other hand, it is also fair to say that equation \eqref{eq:nl:diffusionreaction} is a nonlinear elliptic PDE,\sidenote{By mild abuse of the letter ``P''.} but in one-dimension.  From this point of view we can address the converse of our concern.  Namely, if $R$ is a non-increasing function then \eqref{eq:nl:diffusionreaction} is a well-posed problem.  To be more concrete, consider the Dirichlet boundary conditions
\begin{equation}
u(0)=\alpha \quad \text{and} \quad u(1)=\beta,  \label{eq:nl:drbcs}
\end{equation}
wherein we have chosen a convenient interval $x\in[0,1]$.  (For other intervals we can shift and scale $x$ as needed.)  In the case where $R$ is continuous and \emph{non}-increasing, the nonlinear operator in \eqref{eq:nl:diffusionreaction} is strictly-monotone \citep{KinderlehrerStampacchia1980}.  Because it is also coercive on the appropriate function space,\sidenote{Namely the Sobolev space $H_0^1[0,1]$, after a linear change of variables to set $\alpha=\beta=0$.} which says intuitively that the highest-order $D u''$ term is effective at damping out variations which generate a large norm of the first derivative $u'$, abstract arguments show unique existence of a solution \citep[pages 93-94]{KinderlehrerStampacchia1980}.
% FIXME in 1D perhaps recall first integral?:
%   - u' u'' - R(u) u' = 0 ==> (u')^2 + 2 S(u) = C  where S'(u) = R(u)

For example,
\begin{equation}
-u'' + \rho \sqrt{u} = 0 \label{eq:nl:drsqrt}
\end{equation}
% R(u) = - rho sqrt(|u|) decreasing so -u''+F(u)=0 with F increasing
is of form \eqref{eq:nl:diffusionreaction} with $R(u) = - \rho \sqrt{u}$ and $f(x)=0$.  Because $R$ is non-increasing and continuous if $\rho>0$, the corresponding Dirichlet problem is well-posed.  (If desired $R$ can be extended as $R(u)=0$ for $u<0$, thereby becoming non-increasing and continuous for all of $\RR$.)

\cinputpart{reaction.c}{\CODELOC}{Call-back functions for \texttt{reaction.c}.}{I}{//CALLBACK}{//ENDCALLBACK}{code:reactionI}

We are actually using \eqref{eq:nl:drsqrt} as a first example, however, because we want to verify our numerical solution using an exact solution, and its particular form makes this easy.  The solution is found by noting that both second-derivative and square-root operations convert certain 4th degree polynomials into quadratic polynomials \citep{Ockendonetal2003}.  We also get the boundary conditions from the exact solution.  Therefore we substitute
\begin{equation}
u(x)=M(x+1)^4 \label{eq:nl:exactform}
\end{equation}
into \eqref{eq:nl:drsqrt} and find $M=(\rho/12)^2$, $\alpha=M$, and $\beta=16 M$.  Thus \eqref{eq:nl:exactform} is a nontrivial exact solution to the boundary value problem.

Code \texttt{reaction.c} shown in Figures \eqref{code:reactionI} and \eqref{code:reactionII} solves this problem using a \pSNES object for the Newton iteration and a \pDMDA object for the finite-difference grid.  The first Figure shows three call-back methods.  First we compute the initial iterate---it is a linear function connecting the boundary conditions---and exact solution  \eqref{eq:nl:exactform} in the method \texttt{InitialAndExactLocal()}.  Then we have the residual (\texttt{FormFunctionLocal()}) and Jacobian (\texttt{FormJacobianLocal()}) evaluation functions.

These are ``\texttt{Local}'' methods in the sense that their inputs are C pointers for arrays instead of \pVecs.  Our implementation of \texttt{InitialAndExactLocal()} explains how this works.  As seen in Figure \ref{code:reactionII}, before this method is called we use \texttt{DMDAVecGetArray()} on the \pVecs which need reading or setting, namely \texttt{u} and \texttt{uexact}.  This gives pointers of type \texttt{PetscReal*} which are then handed to \texttt{InitialAndExactLocal()}.  The call-back done inside \pSNES calls \texttt{FormFunctionLocal()} the same way.  The \pSNES also treats the input to  \texttt{FormJacobianLocal()} the same way.  Another aspect of these ``\texttt{Local}'' call-back methods is a \texttt{DMDALocalInfo} struct is passed into the call-back functions.  Thus the local part of the grid (i.e.~\texttt{info.xs} and etc.) and the global grid size (i.e.~\texttt{info.mx})---recall Figure \ref{fig:localpartofgrid}---can be accessed without needing the \pDMDA object itself.

Observe that in \texttt{main()} in Figure \ref{code:reactionII}, because our local call-back functions have a different signature that uses \pDMDA-specific pointers, we use \texttt{DMDASNESSetFunctionLocal()} instead of \texttt{SNESSetFunction()} as in \texttt{ecjacobian.c} earlier.  The Jacobian is set by a similar method \texttt{DMDASNESSetJacobianLocal()}.  Also, though we implement a Jacobian, we do not allocate a \pMat to hold it.  In fact the \pDMDA object has enough information about the grid and stencil so as to pre-allocate a \pMat internally; this is the \pMat that is passed to \texttt{FormJacobianLocal()} for filling-in.

\cinputpart{reaction.c}{\CODELOC}{In \texttt{main()} we create a \pDMDA, then \pVecs, and then a \pSNES.  We hand call-back functions to the \pSNES.  Then we solve the equation, get the numerical error, and clean up.}{II}{//MAIN}{//ENDMAIN}{code:reactionII}


\section{Running and demonstrating convergence}

Recall that the resolution of a structured grid can be set with either \texttt{-da\_grid\_x N} or with \texttt{-da\_refine N}.  For example, on a modestly-refined grid we can compare the number of Newton iterations using analytical and finite-difference Jacobian solutions like this:
\begin{cline}
$ ./reaction -snes_converged_reason -da_refine 6
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 513 point grid:  |u-u_exact|_inf/|u|_inf = 4.62255e-08
$ ./reaction -snes_converged_reason -da_refine 6 -snes_fd
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 513 point grid:  |u-u_exact|_inf/|u|_inf = 4.62255e-08
\end{cline}

Recall our finite difference method has local truncation error $O(h^2)$.  We can generate convergence data, with the result shown in Figure \ref{fig:nl:reaction-conv}, like this:
\begin{cline}
$ for N in 0 2 4 6 8 10 12 14 16; do
>   ./reaction -da_refine $N -snes_rtol 1.0e-10; done
on 9 point grid:  |u-u_exact|_inf/|u|_inf = 0.000188753
on 33 point grid:  |u-u_exact|_inf/|u|_inf = 1.1825e-05
...
on 131073 point grid:  |u-u_exact|_inf/|u|_inf = 7.05476e-13
on 524289 point grid:  |u-u_exact|_inf/|u|_inf = 6.04273e-12
\end{cline}
If we ignore the result on the finest grid, the convergence rate we see is also $O(h^2)$; error stagnation arises at some level of refinement because of accumulation of round-off error.  We also see consistent evidence of quadratic convergence by adding \texttt{-snes\_monitor} to the above runs.  Thus we conclude our implementation is correct.

\begin{figure}
\includegraphics[width=\textwidth]{reaction-conv}
\caption{This convergence evidence suggests \texttt{reaction.c} is correctly-implemented.}
\label{fig:nl:reaction-conv}
\end{figure}


\section{Finite-difference Jacobians by ``coloring''}

However, when we look closer at the finite-difference evaluation of Jacobians we see that it is not really working.  In contrast to the fixed-dimension examples earlier in this Chapter, discretizing a PDE generates an arbitrarily-large number of unknowns.  Consider what happens when the dimension is about 8000:
\begin{cline}
$ ./reaction -snes_converged_reason -da_refine 10 -snes_fd
Nonlinear solve did not converge due to DIVERGED_FUNCTION_COUNT iterations 1
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 0.0049428
\end{cline}
%$
The problem is that this finite-difference Jacobian evaluation is requiring about 8000 evaluations of $\bF$ per Newton iteration, and this exceeds the default maximum count for the \pSNES.\sidenote{Again, ``\texttt{./reaction -help |grep snes\_}'' gets the default value of 10000.}  If we raise the limit by a factor of ten then we do get convergence:
\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10 -snes_fd \
> -snes_max_funcs 100000
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.75635e-10
real 9.04
\end{cline}
%$
This run, however, is very slow because we have evaluated $\bF$ about 25000 times.  The analytical-Jacobian alternative evaluates $J_{\bF}$ just three times, and is fast:
\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.75603e-10
real 0.04
\end{cline}
%$

FIXME: compare fd and fd-with-coloring

\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10 -snes_fd -snes_max_funcs 100000
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.75633e-10
real 14.27
$ timer ./reaction -snes_converged_reason -da_refine 10 -snes_fd -snes_fd_color
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 3
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.75633e-10
real 0.06
\end{cline}

FIXME: show \texttt{-snes\_monitor\_solution}



\section{Jacobian-free Newton-Krylov} \label{sec:JFNK}

FIXME: theory of matrix-free JK \citep{KnollKeyes2004}


\section{Exploring Jacobian cases} \label{sec:jacobiancases}

\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (.7,.8) -- (.25,.15) -- cycle;}
\def\bigcheckmark{\tikz\fill[scale=0.6](0,.35) -- (.25,0) -- (.7,.8) -- (.25,.15) -- cycle;}

The paradigm for residual and Jacobian call-backs in \pSNES-using codes is summarized in Table \ref{tab:snesjacobianoptions}.  If a Jacobian routine is \emph{not} provided in the user-written code then only finite-difference evaluation of the assembled Jacobian matrix (i.e.~\texttt{-snes\_fd}) and finite-difference evaluation of the Jacobian-vector product inside the Krylov method (\texttt{-snes\_mf}) are available for the Newton iteration.  If a Jacobian routine is provided then additionally the Newton iteration can be used with no option---this is the obvious way to implement the Newton method---but also an additional option exists.  The provided Jacobian may be used only to \emph{precondition} the finite-difference Jacobian-vector product (\texttt{-snes\_mf\_operator}).
\begin{table}
\begin{tabular}{rllll}
 &\underline{no option}\hspace{0.0in} & \underline{\texttt{-snes\_fd}} & \underline{\texttt{-snes\_mf}} & \underline{\texttt{-snes\_mf\_operator}} \\
only $\bF$:      & error           & $\bigcheckmark$ & $\bigcheckmark$ & error \\
$\bF$ and $P$:   & $\bigcheckmark$ & $\checkmark$    & $\checkmark$    & $\bigcheckmark$ \\
$\bF$ and $J$:   & $\bigcheckmark$ & $\checkmark$    & $\checkmark$    & $\checkmark$
\end{tabular}
\caption{Jacobian options when using \pSNES.  Symbol ``$P$'' denotes a routine which computes an easy-to-invert approximate Jacobian while ``$J$'' denotes an exact one.  A big check mark is recommended usage.} \label{tab:snesjacobianoptions}
\end{table}

From the code side we can state the cases this way:
\begin{quote}
\renewcommand{\labelenumi}{(\roman{enumi})}
\begin{enumerate}
\item[($0$)] At a minimum, a method for $\bF$ must be implemented in all cases, as there is no other way for \PETSc to know what equations you are solving!  There are two ways of providing $\bF$:
  \renewcommand{\labelenumii}{\alph{enumii}.}
   \begin{enumerate}
   \item If the problem is based on a structured grid, as in \texttt{reaction.c} above, use \texttt{DMDASNESSetFunctionLocal()}.
   \item In general, use \texttt{SNESSetFunction()}.
   \end{enumerate}
\item If \emph{only} $\bF$ is provided, do not create or preallocate a \pMat for the Jacobian, as this is done internally by the \pSNES for \texttt{-snes\_fd} runs, while no assembled Jacobian \pMat exists for \texttt{-snes\_mf} runs.
\item If an exact or approximate Jacobian is implemented then there are two cases:
   \renewcommand{\labelenumii}{\alph{enumii}.}
   \begin{enumerate}
   \item On a structured grid, provide the exact or approximate Jacobian through call-back using \texttt{DMDASNESSetJacobianLocal()}.  In this case the \pMat holding the Jacobian is internal, and does not need to be created or preallocated in user code.
   \item In a general nonlinear equation solve, namely one not based on a structured grid, first declare a \pMat variable, say ``\texttt{J}'', and then call \texttt{MatCreate()}, \texttt{MatSetSizes()}, and \texttt{MatSetFromOptions()} on it.  Then call either \texttt{MatSetUp()} or \texttt{MatXAIJPreallocate()} on \texttt{J}, to allocate space for the assembled Jacobian matrix.  Then provide both the call-back code (e.g.~``\texttt{FormJacobian()}'') and \texttt{J}, for both \pMat arguments, through \texttt{SNESSetJacobian()}.
   \end{enumerate}
\end{enumerate}
\end{quote}

In either case (ii)a or (ii)b, the Jacobian-implementing code, e.g.~\texttt{FormJacobian()} or \texttt{FormJacobianLocal()} in the above examples, only needs to set values in the second ``preconditioner'' \pMat argument.\sidenote{This assumes \texttt{J} is provided for both \pMat arguments of   \texttt{SNESSetJacobian()} as described in (ii)a above.}  It should, however, \emph{assemble} both \pMat arguments.

As practical advice for the debugging stage, you know you have correctly- and fully-implemented the Jacobian, and you have found an adequate initial iterate, if all four options in Table \ref{tab:snesjacobianoptions} work and give apparent quadratic convergence.\sidenote{I.e.~from looking at the residual norm decay shown by \texttt{-snes\_monitor}.}  As the reader may check, this is the situation for the last two codes \texttt{ecjacobian.c} and \texttt{reaction.c}.

We use the \texttt{reaction.c} example to show these alternatives in action.  First, for high-resolution grids, the Jacobian-free method \texttt{-snes\_mf}, which like \texttt{-snes\_fd} does not evaluate the Jacobian through the call-back mechanism at all, has serious difficulties.  Again using the ``\texttt{-da\_refine 10}'' grid, the \texttt{-snes\_mf} method no longer converges at all:
\begin{cline}
$ ./reaction -snes_converged_reason -da_refine 10 -snes_mf
Nonlinear solve did not converge due to DIVERGED_LINEAR_SOLVE iterations 0
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 0.217386
\end{cline}
%$

However, what if a Jacobian is available, but it is only approximate?  For example, suppose we modify this line in \texttt{reaction.c},
\begin{code}
    col[1] = i;    v[1] = 2.0 + h*h * dRdu;
\end{code}
to remove ``\texttt{+ h*h * dRdu}'', to get
\begin{code}
    col[1] = i;    v[1] = 2.0;
\end{code}
This change, which we call a ``\texttt{J->P}'' in the next paragraphs below, keeps the tridiagonal sparsity pattern of the Jacobian, but it removes the influence of the nonlinear term ``$+\rho \sqrt{u}$''---see \eqref{eq:nl:diffusionreaction}---from the Jacobian.

With this change, convergence is slowed for no option, i.e.~when using the approximate Jacobian as though it were exact.  Specifically, on a coarse grid the number of iterations goes from 4 to 15, and the reported residual norms---here partly suppressed---suggest that the convergence is no longer quadratic:
\begin{cline}
$ ./reaction -da_refine 4 -snes_monitor    # before change
  0 SNES Function norm 1.671129624018e-02 
  1 SNES Function norm 3.609252641302e-04 
  2 SNES Function norm 4.167490508951e-07 
  3 SNES Function norm 4.935230509260e-13 
on 129 point grid:  |u-u_exact|_inf/|u|_inf = 7.39662e-07
$ ./reaction -da_refine 4 -snes_monitor    # with J->P change
  0 SNES Function norm 1.671129624018e-02 
  1 SNES Function norm 3.822032062916e-03 
...
 14 SNES Function norm 3.879363487638e-10 
 15 SNES Function norm 1.119521798815e-10 
on 129 point grid:  |u-u_exact|_inf/|u|_inf = 7.38159e-07
\end{cline}
This loss of performance for the Newton iteration, as caused by an in-exact Jacobian, is expected in theory \citep{Kelley2003}.

For this modified Jacobian case, \texttt{-snes\_mf\_operator} is now the fastest option for higher-resolution grids.  Again on the 8000 or so point grid that comes from option \texttt{-da\_refine 10}, using the approximate Jacobian ``as is'' causes many Newton iterations:
\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10    # with J->P change
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 15
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.36236e-09
real 0.13
\end{cline}
%$
Now we try \texttt{-snes\_mf\_operator}, which is designed for this approximate-Jacobian situation:
\begin{cline}
$ timer ./reaction -snes_converged_reason -da_refine 10 -snes_mf_operator  # with J->P change
Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 4
on 8193 point grid:  |u-u_exact|_inf/|u|_inf = 1.80588e-10
real 0.05
\end{cline}
%$
The number of iterations and the time are significantly reduced.

This confirms, in one overly-simple example, the idea that preconditioning the matrix-free Jacobian action by the use of a preconditioner is effective.  As a general rule, for large problems where \texttt{-snes\_fd} is seen to be too costly, one might give this advice:
\begin{quote}
First consider implementing the exact Jacobian.  If that seems like too much work, or if it is too error-prone, consider implementing an approximate Jacobian which is easier to assemble and apply.  (In the PDE case it might only capture the highest-order derivatives.)  To test convergence with such an approximate Jacobian, compare ``no option'' runs, where less-than-quadratic convergence is expected, and \texttt{-snes\_mf\_operator} runs where quadratic convergence may be recovered.
\end{quote}

FIXME: \texttt{-snes\_test}

\section{Line search} \label{sec:linesearch}

FIXME linesearch or trust region sometimes needed to ``globalize'' convergence \citep{Kelley2003}


\section{Exercises}

\renewcommand{\labelenumi}{\arabic{chapter}.\arabic{enumi}\quad}
\renewcommand{\labelenumii}{(\alph{enumii})}
\begin{enumerate}
\item One needs to \emph{see} quadratic convergence to believe it.  Observe that for $x_k$ in both parts (b) and (c) below, the number of correct digits in $x_k$ \emph{doubles} at each iteration.
    \begin{enumerate}
    \item The sequence $x_k = 1-2^{-n}$ converges to $x^*=1$, but not quadratically.\sidenote{It converges \emph{linearly}.}  Find $k$ so that $|e_k| < 10^{-16}$.
    \item The sequence $x_k = 1-2^{(-2^n)}$ converges quadratically to $x^*=1$.  Find $k$ so that $|e_k| < 10^{-16}$.  Find the smallest $K$ so that $|e_{k+1}| \le K |e_k|^2$ for all $k$.
    \item Let $F(x) = \cos(x-1) - \exp(1-x)$ and $x_0=0.5$.  Using any quick-and-dirty numerical tool,\sidenote{Extra credit for doing it in \PETSc.} compute Newton iterates $x_k$ for $k=1,\dots,6$.  Estimate $K$ so that $|e_{k+1}| \le K |e_k|^2$ for large $k$.
    \end{enumerate}

\item Make a tiny modification to \texttt{ecjacobian.c} to set the initial vector to $\bx_0 = [10\,\, 10]^\top$.  Rerun it with option \texttt{-snes\_monitor} and note it does not converge to a solution.  Why does the \pSNES stop?  Add one runtime option so that it  converges, thereby producing the data shown in Figure \ref{fig:newtonconvdelayed}.

\item  \label{exer:newtonatan}
This example shows a famous case where the original Newton iteration can diverge or enter a limit cycle, according to the initial iterate $x_0$, while using linesearch gives essentially global convergence.
    \begin{enumerate}
    \item By straightforward modifications of \texttt{expcircle.c}, write a code \texttt{atan.c} which solves the (one-dimensional) nonlinear equation $F(x)=\arctan(x)=0$ using initial iterate $x_0=2.0$.  (Use ``\texttt{atan}'' for the $\arctan$ function.)  By running it \texttt{./atan -snes\_fd -snes\_linesearch\_type basic}, show that the Newton iteration without a linesearch diverges, while using the default choice (i.e.~\texttt{-snes\_linesearch\_type bt}) gives convergence.
    \item Returning to a paper calculation, require the Newton iteration \eqref{eq:nl:newtoneq}, \eqref{eq:nl:newtonupdate} to enter a sign-flipping limit cycle, i.e.~so that $x_{k+1} = - x_k$ for all $k$.  Thereby approximate the positive value $x_0$ so that the Newton iteration both makes no progress, and remains bounded, for all $k$.\sidenote{One may solve this problem with Newton's method.}  Sketch the situation.
    \item By modifying $x_0$ in \texttt{atan.c} and using linesearch type \texttt{basic}, confirm that one can make the Newton iteration ``stick'' in this actually-unstable limit cycle for quite a while.  Then confirm that linesearch types \texttt{bt,l2,cp} all get unstuck immediately.
% to solve:  - x = x - arctan(x) (1+x^2)
% octave:
%   G = @(x) 2*x - atan(x) * (1+x^2);
%   J = @(x) 1 - atan(x) * (2*x);
%   x = 1.4;
%   format long g
%   for j = 1:4, x = x - G(x) / J(x), end
% get:
%  x = 1.39174520027073
    \end{enumerate}

\item As noted on page \pageref{sidenote:logsummary}, \PETSc option \texttt{-log\_summary} can show whether function evaluations or linear solves are dominating the cost of Newton's method.  By looking at \texttt{-log\_summary} output, compare where the work is done in these runs: \texttt{./reaction}, \texttt{./reaction -snes\_fd}, \texttt{./reaction -snes\_mf}.  Does the comparison change if you add \texttt{-da\_refine 5}?  Explain as much as you can.  (\emph{Hints}: Look at \texttt{SNESSolve} events for the total time spent in the Newton method, which is $\approx$100\% for these cases.  Within that event, event \texttt{SNESLineSearch} is where function and Jacobian evaluations happen, while the linear solve is the \texttt{KSPSolve} event.  Within the \texttt{SNESLineSearch} event, look at \texttt{SNESFunctionEval} and \texttt{SNESJacobianEval} events.)

\item Most \PETSc programs add more runtime options than shown in our examples so far; ours are deliberately de-cluttered.  The PDE problem in \texttt{reaction.c} has parameter $\rho$ which can be adjusted via \texttt{PetscOptionsReal()}.  Modify \texttt{reaction.c} to ``\texttt{optreact.c}'' and add an option like this at the appropriate point:
\begin{code}
  ierr = PetscOptionsBegin(PETSC_COMM_WORLD,
                           "optr_","options for optreact",""); CHKERRQ(ierr);
  ierr = PetscOptionsReal("-rho","coefficient of nonlinear zeroth-order term",
                          NULL,user.rho,&(user.rho),NULL); CHKERRQ(ierr);
  ierr = PetscOptionsEnd(); CHKERRQ(ierr);
\end{code}
By sampling, for what range of positive \texttt{X}$=\rho$ does the run
\begin{cline}
./optreact -snes_monitor -da_refine 4 -optr_rho X
\end{cline}
converge?  Explain why this case, with this way of choosing the boundary conditions and initial iterate, and in contrast to the next exercise, is insensitive to the value of the major parameter $\rho$.
% because this option is used in bratu.c below, for simplicity of maintenance, the result optreact.c is NOT saved in c/ch4/solns/

\item \label{exer:nl:bratu} Modify \texttt{reaction.c} to create ``\texttt{bratu.c}'' which solves
\begin{equation}
    - u'' - \lambda e^u = 0 \label{eq:nl:bratuoned}
\end{equation}
% R(u) = lambda e^u increasing
with Dirichlet boundary conditions $u(0)=u(1)=0$.  Make $\lambda$ an option-adjustable real parameter---see the previous exercise---and use the simplest function which satisfies the boundary conditions as an initial iterate.  Confirm by using a fine grid, and option \texttt{-snes\_converged\_reason} plus other options as needed, that around $\lambda=3.513$ there is a transition to non-convergence of the Newton iteration.  This \emph{critical value} is intrinsic to the nonlinear PDE problem and not a result of a Newton failure \citep{Doedeletal1991}.

\item To the author's knowledge an exact solution of \eqref{eq:nl:bratuoned} cannot be written in elementary functions for $\lambda>0$.  On the other hand, while writing the code in the last exercise is doable because of the simplicity of the problem and the availability of a ``nearby'' correct example code,\sidenote{I.e.~we started from \texttt{reaction.c}.} one should not walk over high wires too often without a net.  Here ``having a net'' means having an exact solution computed independently of the numerical approximation and its implementation.

Fortunately, we can ``manufacture'' \citep{Wesseling2001} an exact solution to a generalized form of \eqref{eq:nl:bratuoned}, namely to equation \eqref{eq:nl:diffusionreaction} in the case where $R(u)=-\lambda e^u$.  For example we can choose $u(x) = \sin(\pi x)$ as the exact solution of \eqref{eq:nl:diffusionreaction}, and then determining $f(x)$ from the equation itself and have $u(0)=u(1)=0$ as boundary conditions that are satisfied by the chosen exact solution.

Thus: Add options and a manufactured exact solution to the code \texttt{bratu.c} in the previous exercise so that the new code can both solve the previous exercise and be verified using the new exact solution.  For instance, one might add a user-option-settable boolean member \texttt{manufactured} to \texttt{AppCtx}.  When it is false then $f(x)=0$ and the code solves the previous exercise.  If it is true then we use the appropriate $f(x)$ in computing the residual, and we report a numerical error.  Observe that having $f(x)\ne 0$ does not change the Jacobian.  Confirm that in the manufactured-solution case we have both quadratic convergence of the Newton iteration \emph{and} $O(h^2)$ convergence of the numerical error.

\end{enumerate}